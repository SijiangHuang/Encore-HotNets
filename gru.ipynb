{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 512\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153255"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'final'\n",
    "gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2.2759292125701904 2.3169453811645506 31.125028133392334\n",
      "2000 2.3109793663024902 2.3088170325756074 61.787867069244385\n",
      "3000 2.3519763946533203 2.3060654497146604 91.97442746162415\n",
      "4000 2.2883689403533936 2.30165091753006 122.27227854728699\n",
      "5000 2.260915756225586 2.2969442417621613 151.6994230747223\n",
      "6000 2.2441365718841553 2.2777613394260405 182.12320590019226\n",
      "7000 2.0051376819610596 2.1663767577409745 212.7797360420227\n",
      "8000 1.5103310346603394 1.8023248428106309 243.37592148780823\n",
      "9000 1.1003156900405884 1.2265918893814087 274.0526418685913\n",
      "10000 0.7858275175094604 0.8324305825829506 304.76387119293213\n",
      "11000 0.6151956915855408 0.6527332273721695 335.41023087501526\n",
      "12000 0.5491046905517578 0.5704638618826866 366.1033401489258\n",
      "13000 0.4974129796028137 0.5269281603991985 394.66207337379456\n",
      "14000 0.46185436844825745 0.4947409906685352 423.4757807254791\n",
      "15000 0.4703219532966614 0.47008905437588694 452.8376100063324\n",
      "16000 0.4694601595401764 0.45385423603653907 482.4670796394348\n",
      "17000 0.46322670578956604 0.43473734325170516 513.083108663559\n",
      "18000 0.4045514762401581 0.4214194384813309 543.5258133411407\n",
      "19000 0.4226604104042053 0.4112964318692684 574.1998660564423\n",
      "20000 0.40938088297843933 0.398438597291708 604.8883244991302\n",
      "21000 0.4002359211444855 0.3908386701643467 635.4832301139832\n",
      "22000 0.3871961534023285 0.3826142008304596 665.9604682922363\n",
      "23000 0.3644039034843445 0.3748159418404102 696.3999116420746\n",
      "24000 0.3347936272621155 0.36918250924348833 726.9320785999298\n",
      "25000 0.37428420782089233 0.3617855954170227 757.5401813983917\n",
      "26000 0.33609145879745483 0.3562024667859077 788.1282122135162\n",
      "27000 0.3375524580478668 0.3509119377732277 818.7884168624878\n",
      "28000 0.3414066433906555 0.3470116049051285 849.5834193229675\n",
      "29000 0.3260968327522278 0.34170410895347597 878.0394480228424\n",
      "30000 0.3263484239578247 0.3390120740234852 906.051952123642\n",
      "31000 0.3112577497959137 0.3340391043126583 934.0507216453552\n",
      "32000 0.3614495098590851 0.32969500783085826 962.1320357322693\n",
      "33000 0.3305221199989319 0.32687797489762305 992.3460621833801\n",
      "34000 0.36210769414901733 0.3237004371583462 1022.6123356819153\n",
      "35000 0.3493674099445343 0.32035573506355286 1053.3555257320404\n",
      "36000 0.3062998354434967 0.31839329704642294 1083.9561984539032\n",
      "37000 0.3358410894870758 0.3149193440079689 1114.5521214008331\n",
      "38000 0.31103527545928955 0.3130129843056202 1145.9265084266663\n",
      "39000 0.318111389875412 0.31082022535800935 1174.3174250125885\n",
      "40000 0.2909507751464844 0.3073021240234375 1203.045310974121\n",
      "41000 0.2913607060909271 0.3059943197667599 1231.3394258022308\n",
      "42000 0.293438196182251 0.30342277652025224 1261.6000273227692\n",
      "43000 0.27705246210098267 0.3008513354361057 1290.9314715862274\n",
      "44000 0.3359927833080292 0.2992694155275822 1320.6877791881561\n",
      "45000 0.2759692966938019 0.2980906843394041 1350.9542763233185\n",
      "46000 0.2731247544288635 0.2959681681394577 1379.5909662246704\n",
      "47000 0.3041028678417206 0.2945009421110153 1408.0253188610077\n",
      "48000 0.302658349275589 0.292488767310977 1438.156980752945\n",
      "49000 0.2923319637775421 0.290319080427289 1467.983862876892\n",
      "50000 0.3112962543964386 0.2887251338809729 1496.6730346679688\n",
      "51000 0.28506460785865784 0.2879629796594381 1526.8749306201935\n",
      "52000 0.2826344966888428 0.28619124104082583 1556.9605450630188\n",
      "53000 0.27120083570480347 0.2847255907654762 1585.9115130901337\n",
      "54000 0.2865735590457916 0.28379136955738066 1615.9761078357697\n",
      "55000 0.27344945073127747 0.2816676676422358 1646.3549523353577\n",
      "56000 0.2538892924785614 0.2799328304678202 1676.2406821250916\n",
      "57000 0.2705709934234619 0.27942426228523254 1706.7234282493591\n",
      "58000 0.27114564180374146 0.2778294638544321 1737.3950815200806\n",
      "59000 0.275476336479187 0.2781978847235441 1768.1729679107666\n",
      "60000 0.28673216700553894 0.27479881423711777 1798.8226010799408\n",
      "61000 0.2703011929988861 0.2744722785204649 1829.3882822990417\n",
      "62000 0.27550849318504333 0.27540306764841077 1859.9197928905487\n",
      "63000 0.2755732238292694 0.27235436721146106 1890.3753736019135\n",
      "64000 0.2790822982788086 0.27187428990006446 1920.8961534500122\n",
      "65000 0.256345272064209 0.2703196015059948 1951.3400592803955\n",
      "66000 0.2874932289123535 0.27030620220303536 1980.6605153083801\n",
      "67000 0.27491724491119385 0.26881360729038717 2011.1174402236938\n",
      "68000 0.2824264168739319 0.26708360786736013 2041.5669832229614\n",
      "69000 0.25501903891563416 0.26680216746032237 2072.136381626129\n",
      "70000 0.2553938031196594 0.26603086398541925 2102.416052341461\n",
      "71000 0.2796827554702759 0.265696648016572 2132.4478855133057\n",
      "72000 0.2757175862789154 0.26433989618718623 2162.1612253189087\n",
      "73000 0.2592073082923889 0.2634869977980852 2192.4227254390717\n",
      "74000 0.25787127017974854 0.264004332318902 2220.984646320343\n",
      "75000 0.26809585094451904 0.2615439901202917 2251.132638692856\n",
      "76000 0.25157198309898376 0.2614874527156353 2281.067004919052\n",
      "77000 0.29119035601615906 0.26087058608233926 2311.3797664642334\n",
      "78000 0.2588428556919098 0.260193295866251 2340.9085636138916\n",
      "79000 0.24963977932929993 0.2590470648854971 2369.80796790123\n",
      "80000 0.2787313759326935 0.25954965014755726 2400.2041738033295\n",
      "81000 0.27193957567214966 0.25795739014446734 2429.4647159576416\n",
      "82000 0.23936299979686737 0.2578690896183252 2459.2670137882233\n",
      "83000 0.27847298979759216 0.2565836057513952 2488.6108009815216\n",
      "84000 0.26153501868247986 0.2561312435865402 2519.035527944565\n",
      "85000 0.25853490829467773 0.2560028490871191 2550.6442081928253\n",
      "86000 0.2536197900772095 0.2555426376610994 2581.9053606987\n",
      "87000 0.24612444639205933 0.254423466950655 2612.2755105495453\n",
      "88000 0.25574150681495667 0.2541193261742592 2642.7629776000977\n",
      "89000 0.24860766530036926 0.25420073281228545 2672.731488466263\n",
      "90000 0.2605496942996979 0.25371522599458696 2702.915610551834\n",
      "91000 0.2502409517765045 0.252130403354764 2730.899249315262\n",
      "92000 0.24214231967926025 0.25243754824995995 2759.8522696495056\n",
      "93000 0.25707972049713135 0.25215800768136976 2789.389823913574\n",
      "94000 0.26804426312446594 0.25209678927063944 2819.443188905716\n",
      "95000 0.2674601674079895 0.25154583278298376 2847.942016839981\n",
      "96000 0.24389620125293732 0.2505744300186634 2877.3681225776672\n",
      "97000 0.2402881383895874 0.25085058525204657 2906.5119228363037\n",
      "98000 0.2283792942762375 0.24957060562074185 2936.531519174576\n",
      "99000 0.24709132313728333 0.24929424710571765 2966.5153799057007\n",
      "100000 0.2453305572271347 0.24959840443730355 2995.6452009677887\n",
      "101000 0.2644094228744507 0.2488083056360483 3025.871664047241\n",
      "102000 0.25649216771125793 0.24793597929179667 3054.6016330718994\n",
      "103000 0.23234209418296814 0.24775482274591923 3082.554572582245\n",
      "104000 0.2683124542236328 0.24839979961514472 3110.514291524887\n",
      "105000 0.2374735027551651 0.24763981354236603 3138.484696626663\n",
      "106000 0.23578234016895294 0.24695653995871544 3168.3481652736664\n",
      "107000 0.23817509412765503 0.24695855264365674 3198.440213918686\n",
      "108000 0.2546321153640747 0.24600520564615727 3227.1855611801147\n",
      "109000 0.25274646282196045 0.24600502528250218 3256.6348469257355\n",
      "110000 0.23936481773853302 0.24616387523710728 3286.6322281360626\n",
      "111000 0.24175646901130676 0.24561454801261426 3316.720440864563\n",
      "112000 0.2469342201948166 0.24533612155914306 3346.9563941955566\n",
      "113000 0.23685520887374878 0.24501708871126174 3377.106684923172\n",
      "114000 0.24349194765090942 0.2447687222212553 3407.012134075165\n",
      "115000 0.23017685115337372 0.24507464495301245 3437.1600363254547\n",
      "116000 0.24388305842876434 0.24343017913401127 3467.330352783203\n",
      "117000 0.23877179622650146 0.24401549704372882 3495.5177853107452\n",
      "118000 0.23978371918201447 0.2435690843462944 3525.708328962326\n",
      "119000 0.23137164115905762 0.24325395153462886 3555.9220485687256\n",
      "120000 0.24984028935432434 0.2432701556533575 3586.1812195777893\n",
      "121000 0.2575876712799072 0.24276956802606584 3616.4361896514893\n",
      "122000 0.2306046038866043 0.24262551936507226 3646.5511178970337\n",
      "123000 0.2583208680152893 0.24245676030218602 3676.543688774109\n",
      "124000 0.23752963542938232 0.24237763918936253 3706.8160672187805\n",
      "125000 0.2418513149023056 0.24193843872845172 3737.079034090042\n",
      "126000 0.22788728773593903 0.2422029882669449 3767.3321602344513\n",
      "127000 0.22396220266819 0.24162881423532961 3797.2972407341003\n",
      "128000 0.22457264363765717 0.2409326524436474 3827.8159143924713\n",
      "129000 0.23248349130153656 0.24125718350708486 3858.4360253810883\n",
      "130000 0.2512674033641815 0.24039357018470764 3889.0233924388885\n",
      "131000 0.2330663651227951 0.24118284782767296 3919.2374913692474\n",
      "132000 0.2408047765493393 0.24071244537830352 3947.9913232326508\n",
      "133000 0.24821974337100983 0.24025654374063016 3976.298024892807\n",
      "134000 0.2235545516014099 0.24007718883454798 4004.891405105591\n",
      "135000 0.22995978593826294 0.23963035899400711 4033.091253042221\n",
      "136000 0.2579043507575989 0.2397585286051035 4062.840167284012\n",
      "137000 0.24665467441082 0.24050335775315762 4093.307974100113\n",
      "138000 0.23283211886882782 0.2388939547240734 4123.70818066597\n",
      "139000 0.2369173914194107 0.23986901204288005 4154.563589811325\n",
      "140000 0.22935879230499268 0.23827616892755032 4185.082980155945\n",
      "141000 0.2211093157529831 0.23902651292085647 4215.437129735947\n",
      "142000 0.22333596646785736 0.23880403162539005 4245.919832229614\n",
      "143000 0.2512984573841095 0.23983712843060492 4276.083345413208\n",
      "144000 0.21192702651023865 0.2383317755460739 4306.415750741959\n",
      "145000 0.22087599337100983 0.2378234192878008 4336.939977884293\n",
      "146000 0.22825977206230164 0.2380053421407938 4367.252099514008\n",
      "147000 0.22570499777793884 0.23821524530649185 4398.115417957306\n",
      "148000 0.22998175024986267 0.23719127361476422 4428.277593135834\n",
      "149000 0.25110897421836853 0.23813484893739223 4455.959750413895\n",
      "150000 0.22627943754196167 0.23753510177135467 4485.204822540283\n",
      "151000 0.23192834854125977 0.23711614310741425 4515.39283823967\n",
      "152000 0.24365729093551636 0.23721680879592896 4544.754549026489\n",
      "153000 0.2473306655883789 0.23781766299903392 4574.668442964554\n",
      "154000 0.23348897695541382 0.23630512982606888 4604.708023548126\n",
      "155000 0.236303448677063 0.2372009356915951 4634.950038433075\n",
      "156000 0.24675917625427246 0.2362493970990181 4665.15537238121\n",
      "157000 0.22260184586048126 0.23676232768595218 4695.3242909908295\n",
      "158000 0.225072979927063 0.23601357473433018 4725.2575562000275\n",
      "159000 0.22367289662361145 0.23622905507683753 4755.430467367172\n",
      "160000 0.22182510793209076 0.23531022295355797 4785.384302616119\n",
      "161000 0.22550702095031738 0.23617051289975644 4815.455609798431\n",
      "162000 0.23059199750423431 0.2362387469112873 4845.575517654419\n",
      "163000 0.24254527688026428 0.23549911168217658 4875.881363868713\n",
      "164000 0.24877028167247772 0.23528399874269962 4904.083610296249\n",
      "165000 0.2443302422761917 0.23593093536794185 4932.526504039764\n",
      "166000 0.2133680284023285 0.23569068051874636 4962.116744995117\n",
      "167000 0.22815440595149994 0.23452005459368228 4992.567678928375\n",
      "168000 0.23720382153987885 0.23532689848542213 5022.988168716431\n",
      "169000 0.2068365216255188 0.23437790274620057 5053.472477197647\n",
      "170000 0.23982983827590942 0.23475309489667415 5081.596130132675\n",
      "171000 0.23035027086734772 0.23455802173912524 5109.587361812592\n",
      "172000 0.24224628508090973 0.23487307018041612 5137.596433401108\n",
      "173000 0.21509264409542084 0.23523686394095422 5166.788567781448\n",
      "174000 0.2089042216539383 0.23407525527477263 5197.411420345306\n",
      "175000 0.2259100079536438 0.234635940015316 5227.794408798218\n",
      "176000 0.232173353433609 0.23393711890280247 5258.2881553173065\n",
      "177000 0.23036153614521027 0.2344998672157526 5288.739483833313\n",
      "178000 0.21800456941127777 0.23326740385591985 5319.127671957016\n",
      "179000 0.2231772243976593 0.23434098656475544 5349.639293909073\n",
      "180000 0.2368144541978836 0.2334015383720398 5380.100349903107\n",
      "181000 0.22275181114673615 0.23371218670904637 5410.53947520256\n",
      "182000 0.2493569254875183 0.2331934306770563 5441.019236326218\n",
      "183000 0.24084198474884033 0.23358962441980838 5471.478799343109\n",
      "184000 0.23422536253929138 0.2339140574336052 5500.325337409973\n",
      "185000 0.24310533702373505 0.23394181253015994 5530.8207993507385\n",
      "186000 0.2203344702720642 0.23299406723678112 5561.448781967163\n",
      "187000 0.22645656764507294 0.23371578203141688 5591.033526420593\n",
      "188000 0.23096945881843567 0.23299472165107726 5620.951400279999\n",
      "189000 0.23522157967090607 0.23281278964877128 5651.560861349106\n",
      "190000 0.24907775223255157 0.2334582680463791 5682.0360271930695\n",
      "191000 0.2476193755865097 0.23260203875601293 5712.373738765717\n",
      "192000 0.22697581350803375 0.2323460332751274 5742.9319896698\n",
      "193000 0.25225213170051575 0.23288272202014923 5773.42884349823\n",
      "194000 0.22878211736679077 0.2320215252637863 5803.701447248459\n",
      "195000 0.23844636976718903 0.23230061967670917 5833.035008430481\n",
      "196000 0.23780354857444763 0.23246223206818104 5862.044216632843\n",
      "197000 0.2244737297296524 0.2329846620261669 5892.50476026535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()(output[i], target_tensor[i])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sum_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/autograd/grad_mode.py:26\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m     27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/autograd/grad_mode.py:135\u001b[0m, in \u001b[0;36mno_grad.__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mis_grad_enabled()\n\u001b[1;32m    133\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc_type: Any, exc_value: Any, traceback: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprev)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 4, 13, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8, 8, 8, 11] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4] False\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6] False\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 7, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6] False\n",
      "[8, 8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] False\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 4, 11, 2, 4, 8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11] True\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 11, 8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8] False\n",
      "[8, 8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13] True\n",
      "[8, 8, 4, 8, 11, 8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] False\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 8, 4, 6, 8, 6] False\n",
      "[8, 4, 6, 8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6] False\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 9, 13, 4, 4, 7, 7, 8, 4, 7] False\n",
      "[8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15] True\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 6, 6, 4, 4, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4] False\n",
      "[8, 4, 7, 8, 13, 8, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6] False\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 7, 13, 8, 4, 8, 11, 8, 13, 4, 9, 4, 10, 8, 7, 8] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15] True\n",
      "[8, 7, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13] False\n",
      "[8, 8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1] True\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.003996946393828166 0.02364114508306244 0.06808682802283574\n",
      "1 0.002977102104653019 0.017401626486596405 0.09587070479026971\n",
      "2 0.003939623418306078 0.015782461782687114 0.09041966129037701\n",
      "3 0.004404837112221716 0.020557794429726994 0.08037825775341918\n",
      "4 0.0031900069157271655 0.01886497924964737 0.11541280624370863\n",
      "5 0.0037147055114340115 0.01684717794497215 0.04362384327836768\n",
      "6 0.0033308027250655613 0.024953412225489274 0.12806281186266769\n",
      "7 0.0035115864566774508 0.027628100812761684 0.11718434610348075\n",
      "8 0.001658318024199237 0.014815724394961781 0.13076504713529877\n",
      "9 0.002100930475839527 0.016949727058718406 0.09984615243879238\n",
      "10 0.002851023841056501 0.017158522416967113 0.08451325312891833\n",
      "11 0.0026050766174175125 0.02571662783180828 0.12444833584924142\n",
      "12 0.003048943453730398 0.01847533876066091 0.11170162928225534\n",
      "13 0.003470737597571001 0.019615590506424037 0.1098784278877494\n",
      "14 0.0020833827238962085 0.01949318446634337 0.10770806975887792\n",
      "15 0.00418367744090894 0.01783402475706437 0.10702417609416995\n",
      "16 0.0038772125827659445 0.01453659339831109 0.08211342620477911\n",
      "17 0.0023307728115712287 0.02113583410243277 0.1338076640505814\n",
      "18 0.004625937393137909 0.026088173531667647 0.09505802848246517\n",
      "19 0.004574595209460519 0.018660644079883763 0.10725497113284456\n",
      "20 0.002474938430010194 0.012955979141866261 0.11493331334406842\n",
      "21 0.0028057100468189245 0.01926728324837545 0.10996115827061187\n",
      "22 0.004225392888023857 0.019412100830531038 0.11219160432682712\n",
      "23 0.0029490251355461902 0.01284416691016171 0.12260792078861099\n",
      "24 0.0025381675885030788 0.017469588664662784 0.10043666212992552\n",
      "25 0.0020271922420267655 0.02040743243234247 0.12676180410838037\n",
      "26 0.0031382858565866113 0.019561451437341913 0.09891848859700221\n",
      "27 0.0021988400173971936 0.007516892387489743 0.0730846074721768\n",
      "28 0.0025826165036856533 0.01625229921158575 0.1104124405284788\n",
      "29 0.002647440929067331 0.01188294829049683 0.13541102398212612\n",
      "30 0.00463857459320528 0.02115337268408289 0.08280698660367523\n",
      "31 0.002032722590784646 0.009545366412535512 0.04868861446856464\n",
      "32 0.0025123824304392533 0.020349341101754057 0.11417219516820398\n",
      "33 0.0027323061551900794 0.02099809615407739 0.09203515388689569\n",
      "34 0.004347375491001887 0.019604831631797798 0.1139012105484031\n",
      "35 0.0027590192566113313 0.018689416584558693 0.09665539912796595\n",
      "36 0.004356862772862074 0.014521654361621212 0.11453779848575152\n",
      "37 0.004120354935735156 0.019455337452147198 0.08568096205518708\n",
      "38 0.0032969654832933114 0.018472908378184758 0.07490635155715515\n",
      "39 0.003991174053301414 0.026757558106897065 0.11519057774560024\n",
      "40 0.002485091452236153 0.015319351732596877 0.10995695858419105\n",
      "41 0.004703474696338494 0.023389694849551054 0.09332418849834113\n",
      "42 0.0026451006641089536 0.01343421734924398 0.08173421960100616\n",
      "43 0.003530123575757479 0.017252694633628236 0.09144748171023842\n",
      "44 0.0027995927835797866 0.023620114485588702 0.10673038793264576\n",
      "45 0.0036572472052029716 0.02121690822411229 0.1045416717245708\n",
      "46 0.0034284104486598885 0.01887083828658917 0.10131811258050497\n",
      "47 0.0031310441496748467 0.02133492781615492 0.09352312731515616\n",
      "48 0.004192264027726651 0.015261105710963244 0.11032567787826482\n",
      "49 0.003581308924490158 0.017811298778466614 0.08237362303622925\n",
      "50 0.0025719926961740155 0.020339717641446858 0.09591794513505242\n",
      "51 0.0029186423804896207 0.016218108577235494 0.12283278093789532\n",
      "52 0.004263397713677028 0.025316958748494257 0.10298109460560273\n",
      "53 0.0030463485235292177 0.012089701405085786 0.08920788958351944\n",
      "54 0.0021362542528822514 0.016753774110151404 0.11725059928091289\n",
      "55 0.0039317689050599285 0.020995971166278334 0.11864348112353379\n",
      "56 0.0022592697342570414 0.01974442292635502 0.10315916506533149\n",
      "57 0.0029109331735803633 0.016325813812858466 0.08492055734442056\n",
      "58 0.003941662134127221 0.021460498833449547 0.13174019180173754\n",
      "59 0.0023877795105303914 0.014246342938570883 0.0829196744059342\n",
      "60 0.004517624525055757 0.01705151850460052 0.1306220140729357\n",
      "61 0.003261859347083996 0.011621868591197893 0.07898722130023553\n",
      "62 0.002040601313370154 0.011874221452991256 0.09239453235357423\n",
      "63 0.004139649866054857 0.023505957527898372 0.1055014573275081\n",
      "64 0.003294402093144608 0.013118280022878789 0.05288173229587856\n",
      "65 0.003539811535716199 0.021192749558625094 0.08960023315334897\n",
      "66 0.00439947968558327 0.024763751140754582 0.09539199109774618\n",
      "67 0.0029406756257414966 0.012780869901704187 0.10114932300440768\n",
      "68 0.0024401659866481666 0.02198910094686057 0.11256503973884295\n",
      "69 0.0029001078590876 0.02270807739179735 0.09782239060365072\n",
      "70 0.002922726398970751 0.011488519591266032 0.09645210707340438\n",
      "71 0.0024472734020593995 0.016396893981942412 0.09208198734110978\n",
      "72 0.0020164803976276723 0.01215999485635224 0.09612596438433921\n",
      "73 0.0026563649441506522 0.015773568029735008 0.11424021784363969\n",
      "74 0.003595627630471564 0.02706613998128398 0.11966501316647711\n",
      "75 0.002559478046028161 0.019860939362866602 0.129072299761036\n",
      "76 0.0031405003150613694 0.023346033152399238 0.08164650808916038\n",
      "77 0.002333963289241565 0.024916152951272853 0.10565417187332546\n",
      "78 0.004360824353960226 0.033061351132720115 0.10872402078336675\n",
      "79 0.003318206053119441 0.018907990891704037 0.1110879621573403\n",
      "80 0.003981180205730474 0.03225725462132099 0.1301729265588225\n",
      "81 0.003941939073726343 0.022155895410548004 0.10634195342627252\n",
      "82 0.003806894817477455 0.02015393151536994 0.1239971912601549\n",
      "83 0.003372501838986501 0.023549950781383056 0.0837604983927544\n",
      "84 0.00178856116264589 0.018256176606495193 0.0967919793390512\n",
      "85 0.001949824820554875 0.013343692390523323 0.08823484783360139\n",
      "86 0.004331105745979795 0.02210155099208728 0.1175586511420501\n",
      "87 0.0036024620420059606 0.017641997304768918 0.1056548716485837\n",
      "88 0.0022353568530635243 0.01515800178738859 0.09942935378279542\n",
      "89 0.0027477113205841286 0.018906581551711557 0.1108539252846088\n",
      "90 0.0027468758005081495 0.021466805918645415 0.08669679087887328\n",
      "91 0.002458879008108837 0.018542485742789667 0.1022206765504856\n",
      "92 0.002408546264838012 0.011902458850276791 0.06611252741223159\n",
      "93 0.004477844710584678 0.01903333080302073 0.08674519970533176\n",
      "94 0.003351196205044371 0.015344085945951744 0.08863416448218087\n",
      "95 0.003400158236401226 0.021454206015963526 0.1165412214199228\n",
      "96 0.004890810765180801 0.022458490731376377 0.08983328362148077\n",
      "97 0.003971046329113916 0.018290749834395298 0.10891586298682547\n",
      "98 0.003919667824212842 0.022995704446795044 0.11332896683555777\n",
      "99 0.0022492347918348942 0.013503904251241403 0.10379974340790225\n",
      "100 0.002858255103797493 0.022011881191204855 0.13692131367277433\n",
      "101 0.002849172884070245 0.017889214382446568 0.10413461725153723\n",
      "102 0.0024725262345541756 0.017214801809924393 0.08852600013635087\n",
      "103 0.004578511218269958 0.0297456858027417 0.1192559680100595\n",
      "104 0.0018331442775105476 0.016239376219528867 0.10170668247132991\n",
      "105 0.004851393708383138 0.023079167426951028 0.08884725333651065\n",
      "106 0.0031120049889393573 0.015864554046280035 0.11110819876482851\n",
      "107 0.0015906196520867996 0.019400719233882376 0.12065236187576686\n",
      "108 0.004043498339000072 0.029293601975031065 0.10326605029160557\n",
      "109 0.0042895925908838055 0.023704181354974513 0.10615126756144289\n",
      "110 0.004735706798703684 0.03552785893127261 0.10128003927610213\n",
      "111 0.0028229602990080963 0.013196402002108445 0.07256352029682542\n",
      "112 0.0039107881117697 0.024661802541649867 0.1293282320905316\n",
      "113 0.0013694416519354157 0.01650617354680326 0.12638875080914658\n",
      "114 0.004452883871326919 0.03467733435471665 0.13460038879236425\n",
      "115 0.004375135302711463 0.022831159683258703 0.10652496228845723\n",
      "116 0.0036597342883530415 0.02495029953369796 0.11258895932137199\n",
      "117 0.0013661620682794012 0.010547962711559302 0.09995493399080008\n",
      "118 0.002291276410402882 0.009866185674585136 0.0794892518535974\n",
      "119 0.0039019090004214813 0.02073027941098704 0.138250433309481\n",
      "120 0.003139339373797777 0.021124061104620596 0.11502864659880893\n",
      "121 0.0031613785058961313 0.013542627198720434 0.09329764008179678\n",
      "122 0.0031814035760940867 0.013168490212650055 0.07073966493734425\n",
      "123 0.001690751434860336 0.01822934213819568 0.07782755384879206\n",
      "124 0.004762126438974141 0.014764888396372673 0.07503233168473025\n",
      "125 0.00392007050608018 0.01580974059326951 0.10392707744431326\n",
      "126 0.0027710453567719893 0.023114084852095537 0.1163588196417264\n",
      "127 0.002151276533576007 0.012768181020806513 0.08360067206775501\n",
      "128 0.0021019646859085214 0.024305340530209652 0.12537942197044644\n",
      "129 0.0022654655763835983 0.014174469634623533 0.07801377441543818\n",
      "130 0.002645870992178506 0.017108037958370487 0.10037983821453339\n",
      "131 0.0016058013339018808 0.01670772375032218 0.12054625282329907\n",
      "132 0.0018657912925056112 0.014061785200322002 0.09297148849780391\n",
      "133 0.0037449645774035715 0.022557708338955793 0.10515749732217408\n",
      "134 0.0012407360452757624 0.01573690132824128 0.10716850891620353\n",
      "135 0.002741114860779343 0.014815963555158337 0.0918415288049867\n",
      "136 0.002191894367192282 0.017299252032667092 0.12431621740871861\n",
      "137 0.003546959473062603 0.019845032432215814 0.10889703979626311\n",
      "138 0.002165643528958737 0.020444962778069554 0.10434777736504153\n",
      "139 0.003932945463744419 0.020410415584627972 0.09763832990793875\n",
      "140 0.003976535759518307 0.024039951677880478 0.10255466969416974\n",
      "141 0.002270632414199821 0.013684337564091648 0.11259642168878181\n",
      "142 0.00186191100466476 0.01505062481717095 0.09193579208817079\n",
      "143 0.002894298707030578 0.019648477664460578 0.12254235741535972\n",
      "144 0.001975521322181183 0.015031740560865432 0.09869780050820959\n",
      "145 0.0021994808219194598 0.01176926035433224 0.11541129152639154\n",
      "146 0.001865003585561017 0.012784878985682322 0.10540991090134584\n",
      "147 0.002637270179525834 0.017768713070948378 0.13323951883987312\n",
      "148 0.0009891641019842334 0.009310584151197591 0.0914952724087226\n",
      "149 0.0029967202314645784 0.021133141263604927 0.0822411000210086\n",
      "150 0.0030870462488837625 0.018564600895177988 0.09900538210233237\n",
      "151 0.0025641835503242743 0.018266219138467387 0.09353343984330284\n",
      "152 0.0022103754038660895 0.01837186935535883 0.11500151086600185\n",
      "153 0.0013242420437473363 0.010862855072409246 0.11219027926767523\n",
      "154 0.0044295836768171205 0.019129336036247037 0.11623202066715323\n",
      "155 0.0038457883117519395 0.014782990689401667 0.08157529893210924\n",
      "156 0.0024783198491568656 0.01635367597118519 0.10734611857972207\n",
      "157 0.0029380398949504028 0.021014276186471392 0.10634811159356042\n",
      "158 0.0031099650386747575 0.01752598355020712 0.08620933821924906\n",
      "159 0.0021475914417251516 0.011351283825809182 0.10967146187427737\n",
      "160 0.0017175574765986345 0.009118460684654647 0.11145145246753675\n",
      "161 0.002137055546548492 0.02260966009191291 0.0913321459666403\n",
      "162 0.0013171013946050797 0.029298702057193822 0.10938622962885272\n",
      "163 0.0046250284103888095 0.01954842050560105 0.1440204289680475\n",
      "164 0.003607334669698953 0.026041614789079896 0.12467210253729477\n",
      "165 0.0016528131823373257 0.008888687747592156 0.14427100591157815\n",
      "166 0.0028079669556685845 0.01942837817829371 0.09392474762174316\n",
      "167 0.002631120018052494 0.021687085190172907 0.13077286677526326\n",
      "168 0.0028778163386382967 0.01640031616293313 0.09132100516134187\n",
      "169 0.0018921051223085275 0.015077468878389429 0.09208460480028274\n",
      "170 0.003995178797597455 0.03130101612742324 0.11461953957463303\n",
      "171 0.0020199526373560848 0.016774464772867075 0.0897149320723227\n",
      "172 0.004430040938852842 0.023240679311382247 0.0992667521589993\n",
      "173 0.002655733539455379 0.011807191870982314 0.1093718026187037\n",
      "174 0.002769531123641731 0.014855506970535705 0.11053039788971583\n",
      "175 0.004126696066284985 0.028095965039843875 0.06976376488801109\n",
      "176 0.0020304016421403815 0.011544923840709109 0.07054170276594847\n",
      "177 0.004383798766698754 0.021559028816770617 0.08820582594460868\n",
      "178 0.0035967855512415523 0.02470295537715845 0.1120705454007988\n",
      "179 0.004812079458411765 0.02609505730181402 0.1080118015119227\n",
      "180 0.0014900932756247189 0.01063732473009793 0.09521186218857755\n",
      "181 0.00373491791211594 0.0161498525592713 0.10944759510605569\n",
      "182 0.001533756064446672 0.015601489443750696 0.08533517821657335\n",
      "183 0.0022386383375836037 0.01704319960529868 0.1127836210992387\n",
      "184 0.004179517736122943 0.014318630309814943 0.09022935214861721\n",
      "185 0.0015926909806969255 0.015837447357236595 0.10865305935371966\n",
      "186 0.002895439674424837 0.01724069607580683 0.10143916904734847\n",
      "187 0.0016294291796474267 0.011342270800707101 0.10462669284669618\n",
      "188 0.0036884332199443597 0.018888493005750226 0.11303927553630455\n",
      "189 0.0038906869582523867 0.018161424089404375 0.07968903372599809\n",
      "190 0.002863996824652021 0.028347219555445786 0.11695209486798021\n",
      "191 0.0018826628059084496 0.013997956481420354 0.0899953611319127\n",
      "192 0.0021759796560237507 0.024292397511337886 0.10359719357423562\n",
      "193 0.002122383210231541 0.016907111007472927 0.10875106523139749\n",
      "194 0.004701532982776399 0.02430520029776917 0.12399922186241244\n",
      "195 0.0021447450465525847 0.019231199670307023 0.10649152646989012\n",
      "196 0.004567235410955299 0.025093087922073855 0.11608650708681037\n",
      "197 0.004879694642561968 0.029098982652910194 0.08968739191306596\n",
      "198 0.0023385752732535785 0.023615047888394088 0.11878424647599163\n",
      "199 0.002381667348671974 0.015755730239664072 0.10121332713942374\n",
      "200 0.0027193762084953745 0.018729279056672814 0.12367583698126107\n",
      "201 0.004320735869212553 0.025897896937904577 0.09467294757995162\n",
      "202 0.002045451213825506 0.011253275740951172 0.1010967865078123\n",
      "203 0.0026568615579675723 0.017727808805500685 0.08921113159783448\n",
      "204 0.002619751143197163 0.012907578710077396 0.11280835998862546\n",
      "205 0.0019523120824377176 0.013403562861860303 0.11107055979952722\n",
      "206 0.0033036170532990917 0.028263093428214585 0.13238117227232046\n",
      "207 0.002475745625732651 0.014050013828334523 0.09998269234763446\n",
      "208 0.0028891945081588486 0.014216941204460355 0.11435944006259378\n",
      "209 0.0019572891174547092 0.013729133325833306 0.10446839936076702\n",
      "210 0.0022981629412558912 0.016337588739406717 0.12544362373967938\n",
      "211 0.0014317456922329028 0.022220910036232416 0.11274196769023648\n",
      "212 0.0014140762947229383 0.017766686705104694 0.12281199556711783\n",
      "213 0.003543809233083915 0.015639552812381604 0.10624420725688354\n",
      "214 0.003287547902490448 0.02104791792239336 0.12589516276219342\n",
      "215 0.002092244609510196 0.013442590521661947 0.13836969896731957\n",
      "216 0.0021826206219823403 0.012324032391579011 0.11361079161635287\n",
      "217 0.0012523617178638088 0.01435536337611597 0.09998809311885062\n",
      "218 0.004200967826235495 0.03184469513368432 0.13390407879601338\n",
      "219 0.004391820995346078 0.034718031150524846 0.12267791169516525\n",
      "220 0.004507730880004212 0.022131380490665944 0.09895392217448609\n",
      "221 0.0018487564210533415 0.01236750621379815 0.10640683235965775\n",
      "222 0.002891550289450834 0.012801069506591004 0.10627207389423671\n",
      "223 0.002619919010683841 0.014278554255934777 0.08945603428006685\n",
      "224 0.0031833646793074802 0.019916954793157294 0.1259548317234055\n",
      "225 0.0027051767154546956 0.016422836032963878 0.12008233799634718\n",
      "226 0.002242403468042965 0.02238950878657576 0.12862860627704564\n",
      "227 0.002007988223778338 0.011545854924125633 0.08739933028265784\n",
      "228 0.0047741830070298676 0.024948425278190627 0.1074607599637005\n",
      "229 0.00355253396156264 0.024579671898827152 0.12074665604755051\n",
      "230 0.004102786849305679 0.033820605504045795 0.09502743041254137\n",
      "231 0.003572939390000421 0.02402461290697336 0.1100406056566126\n",
      "232 0.00469840580123149 0.017077170785385676 0.06371096436360343\n",
      "233 0.003784551920660217 0.022378559478403726 0.09784718333604112\n",
      "234 0.00294321146650175 0.01761171657576032 0.07664730343177653\n",
      "235 0.004100890540880817 0.02035321647019049 0.09828369278998737\n",
      "236 0.0010778611682303405 0.012599232255641916 0.1274398264630219\n",
      "237 0.002284405286620021 0.02181983068724299 0.1112194297036622\n",
      "238 0.0036902856255196952 0.023129135678685055 0.13184994220593665\n",
      "239 0.002384693334965033 0.01852320725063778 0.06549787907242777\n",
      "240 0.004194916251208459 0.02004570039560507 0.07349260550374623\n",
      "241 0.0025463368114316608 0.013877184777371846 0.08724867129597366\n",
      "242 0.0026969213525403885 0.01528175731762298 0.11167167413143889\n",
      "243 0.00230866961659239 0.012988035220409876 0.09631207971556355\n",
      "244 0.004823550033509783 0.02707422116560343 0.1387953048045815\n",
      "245 0.003897753006516478 0.015047037381772729 0.10903212132059367\n",
      "246 0.0037407285279760787 0.024383486330495245 0.09456617890302374\n",
      "247 0.0032712550840534108 0.020890055620362197 0.09462625217493123\n",
      "248 0.003112148379388798 0.02143659523224494 0.10256951011243054\n",
      "249 0.0038274471370377348 0.0229261417789509 0.12512254926603747\n",
      "250 0.0016849401255298814 0.011846203194605082 0.08795897324555735\n",
      "251 0.00283503088762532 0.018478827945519066 0.09352315539773254\n",
      "252 0.004848308000790587 0.024670803599954758 0.09090873274665354\n",
      "253 0.0028354057521324 0.01679238725901541 0.12274072617518009\n",
      "254 0.004072786619247747 0.02154582461708407 0.09763578251149092\n",
      "255 0.0013559178377048025 0.010273911328993588 0.08524788952982967\n",
      "256 0.0035004785042443056 0.018939711529803484 0.11072588310758426\n",
      "257 0.0025521523133611545 0.023164548804826445 0.1003323008937009\n",
      "258 0.0016254707830365717 0.01392707537414251 0.10509251387409\n",
      "259 0.004834357006268806 0.022670557461241733 0.07691303424205301\n",
      "260 0.003237191682586839 0.014619459511011087 0.09108740258915114\n",
      "261 0.0029155277746615985 0.020872440477751608 0.10745140759019495\n",
      "262 0.0039866802486131035 0.02108395607997665 0.11764798934996903\n",
      "263 0.003303394284543066 0.021340952954026875 0.08271304463510257\n",
      "264 0.001915489963332073 0.014109965268191575 0.09222857944314894\n",
      "265 0.0030518854117240536 0.014316100319188554 0.10329143206573264\n",
      "266 0.004177418980567914 0.01860704579551916 0.11540122807636301\n",
      "267 0.00218421893880481 0.024558591445544505 0.11507348685413302\n",
      "268 0.0016611661382477717 0.026692339739047145 0.12117776391769042\n",
      "269 0.0047207978218108005 0.019990169960978594 0.09542130064176199\n",
      "270 0.0034558394066963333 0.018981958780515885 0.12690448473814386\n",
      "271 0.003250290443565325 0.022180311821886362 0.10088684964069353\n",
      "272 0.004768607860318812 0.02005151143769179 0.1089261638961837\n",
      "273 0.002280284771012204 0.018272327820624615 0.10829117481273301\n",
      "274 0.003983531919962648 0.02239724162410881 0.11654364943341772\n",
      "275 0.00445468231359405 0.02839369742545657 0.14242968612552628\n",
      "276 0.0017260697179615398 0.01154765207874703 0.08622998062302269\n",
      "277 0.0015737186703811321 0.023003213836768534 0.12034578659082912\n",
      "278 0.0024004697086724607 0.01547833618385687 0.09764734167751767\n",
      "279 0.0027082014223763495 0.025709223654016213 0.10182230060337846\n",
      "280 0.002216746627586782 0.013195794328403758 0.06709648055331469\n",
      "281 0.0019188464708910994 0.013265295111682908 0.09662446900181479\n",
      "282 0.0015025937683085027 0.008691537133459867 0.07806396636244352\n",
      "283 0.0017754758348005003 0.016980444668687332 0.1394120361031364\n",
      "284 0.003235803979084414 0.026779408569737085 0.12168406666096307\n",
      "285 0.004290176702447396 0.024377054918218247 0.09811081316973778\n",
      "286 0.002281340735467311 0.02686102391481332 0.11626471840295458\n",
      "287 0.0020670680724372025 0.014790275511911439 0.094140735102795\n",
      "288 0.0015919397315028992 0.01608626993122861 0.10111169362392305\n",
      "289 0.0032742891258133594 0.014677010412263916 0.10919202339480627\n",
      "290 0.001998259969066301 0.01606694505847423 0.11177797845128236\n",
      "291 0.001922034285620218 0.015689940854261453 0.08498675612554116\n",
      "292 0.004544936607568905 0.01681818944038793 0.10178029982010695\n",
      "293 0.0027315569949844403 0.019022959909853154 0.13102789283458643\n",
      "294 0.0020325300768864784 0.011932314601784594 0.12088085512173324\n",
      "295 0.0026928351292717194 0.014872199137250516 0.07984582092451412\n",
      "296 0.003876662841426486 0.019688987851542716 0.10648201583182101\n",
      "297 0.0026684573050255052 0.01847974307874403 0.10609384870795012\n",
      "298 0.002192690054258358 0.02692289903416105 0.11368192613963335\n",
      "299 0.0022225388870713387 0.013486285617929684 0.09697672794981922\n",
      "300 0.003706915358908488 0.020326296037631877 0.09231436512453052\n",
      "301 0.004651884056431827 0.022920275042619967 0.13246504561010486\n",
      "302 0.003431476886317588 0.01783202300091718 0.12498732577704316\n",
      "303 0.004339295730099088 0.018210739046863038 0.1008838933965999\n",
      "304 0.0040288582863524415 0.029481787151088682 0.13238764947680393\n",
      "305 0.0022522752920670647 0.015351373450424918 0.09012742277807681\n",
      "306 0.003899743081545111 0.013058358381578896 0.0683508570279329\n",
      "307 0.0017813468756909241 0.01580229854659706 0.1436357003664342\n",
      "308 0.0018033699846223446 0.012405457388722512 0.09576265028290376\n",
      "309 0.0034474400673257443 0.01993185104127839 0.11015514030459167\n",
      "310 0.003789263926906066 0.02131210468976772 0.1049541110679303\n",
      "311 0.003705907659517955 0.020778093124845163 0.08955957586364444\n",
      "312 0.0030344582507898458 0.023004462192812526 0.10955659359844727\n",
      "313 0.0035598996702823535 0.023334931767029506 0.12132492066986807\n",
      "314 0.0033524024468560582 0.01807115163473127 0.07148901873030042\n",
      "315 0.0038862288387842663 0.01196747998070043 0.06927549919376928\n",
      "316 0.0031592465763565785 0.015554089872232932 0.12351848630774752\n",
      "317 0.002253650948773063 0.013338026010588425 0.0877212047374474\n",
      "318 0.0024384723642249923 0.015970036380198793 0.11246494426944557\n",
      "319 0.0027848609323724966 0.022185687822202357 0.10855176717383548\n",
      "320 0.001980862077923187 0.017636064635295323 0.13093308663705333\n",
      "321 0.003118295805470404 0.020147573682280043 0.12422431344518942\n",
      "322 0.002852976503902498 0.03956134452105167 0.13135990129504913\n",
      "323 0.003525330222889581 0.015380676267384805 0.08537868512984256\n",
      "324 0.002568618539869283 0.0163490187676449 0.0981362084624664\n",
      "325 0.002836853132528144 0.017977069322013373 0.11676154579567163\n",
      "326 0.002854381989237279 0.024428546279978425 0.08140185639989239\n",
      "327 0.004060191351643849 0.02448855064530206 0.10662404061483288\n",
      "328 0.004032419231588635 0.01700940357144612 0.12180183551598683\n",
      "329 0.0019510763648118985 0.02094044223055068 0.1050079049419437\n",
      "330 0.002218876191931669 0.016575546511257144 0.11990135887943743\n",
      "331 0.002393948876215856 0.008595721091437476 0.055482626761304915\n",
      "332 0.0016314617987051824 0.012342940680553485 0.0942073085627522\n",
      "333 0.0042974199501338425 0.02500499489364563 0.11638820911204915\n",
      "334 0.002033406428662604 0.01532855967623167 0.10623527524751736\n",
      "335 0.0041126645613455085 0.020179895520191556 0.07282936070676962\n",
      "336 0.0036284238922836545 0.018631265347431716 0.08791239230762879\n",
      "337 0.0022336956496962074 0.013234230510083617 0.09308216116097355\n",
      "338 0.0042660199133068375 0.03616051633002592 0.10701007560302356\n",
      "339 0.0034534001742667583 0.02113993780421066 0.09815231091976596\n",
      "340 0.002583586851821846 0.01563891694516733 0.09631185758038044\n",
      "341 0.004358012372727582 0.019631043325593067 0.08789429520457998\n",
      "342 0.004264927092370023 0.016133129335119133 0.12288751464683088\n",
      "343 0.0034722337603127307 0.019024097077167207 0.09577675463089452\n",
      "344 0.002120450672088517 0.009904477193870463 0.0957896992397349\n",
      "345 0.0035403858850587937 0.015960433895454252 0.09659245342178277\n",
      "346 0.0048704283710592836 0.02325226043520314 0.10829266534861523\n",
      "347 0.0038667530951995363 0.01858782333353643 0.11781755900054405\n",
      "348 0.003150293981990993 0.014299930045296286 0.0930499511200905\n",
      "349 0.0028322410359477353 0.015788601378657126 0.12393171569215927\n",
      "350 0.0038505742945675646 0.0238289709146411 0.11487287280920652\n",
      "351 0.004394575498994571 0.016115932126614968 0.08514872746120557\n",
      "352 0.003277644093767281 0.03171863372454436 0.10538973090368084\n",
      "353 0.0020934877189500603 0.01927140600836518 0.11315943552581233\n",
      "354 0.0037973843759646583 0.023944066839698283 0.11218399299601248\n",
      "355 0.004657218944843403 0.020533532697220244 0.1105175274612468\n",
      "356 0.0038642491978214274 0.02290179803650568 0.0864928457560483\n",
      "357 0.0014835634158057633 0.01572139718930994 0.11359947492243086\n",
      "358 0.003519083926544234 0.014962016444849083 0.09382705186151381\n",
      "359 0.0031326492772041697 0.01632391159150873 0.09604230822277007\n",
      "360 0.002608348189580183 0.01163732068934835 0.09099978678452462\n",
      "361 0.004656413176580585 0.028787296356658634 0.11645696932181726\n",
      "362 0.004427600441371475 0.021668817599037975 0.0924395197501859\n",
      "363 0.0018240963632891148 0.018247284090507337 0.11038688400970083\n",
      "364 0.0031586962518449554 0.019440074190993282 0.09573139353546475\n",
      "365 0.004384894578171903 0.02353463632739345 0.1189588465303374\n",
      "366 0.00471550329105488 0.02658409388662294 0.12623251379342443\n",
      "367 0.0026698676512035726 0.012580117706379592 0.10675163173075378\n",
      "368 0.004680232990015614 0.01732976313736604 0.09203336905367354\n",
      "369 0.0028154517617572893 0.021520444159237845 0.12196096750250893\n",
      "370 0.004743560302033042 0.022678592375331666 0.08245034638991587\n",
      "371 0.00303253308372273 0.017793744475641256 0.11408949976605831\n",
      "372 0.004088293650627671 0.01926403083823225 0.09875804974503241\n",
      "373 0.002494512779695288 0.011770533463256745 0.09180950386409993\n",
      "374 0.004195909235996524 0.02013357160360339 0.11244043953553376\n",
      "375 0.0012120488747332115 0.011858934189898107 0.11753398948949063\n",
      "376 0.003221707328739328 0.016274907273181306 0.12127890984545356\n",
      "377 0.0019820484908781555 0.016143226105881668 0.09180697578342964\n",
      "378 0.002262103871814008 0.015360382213799852 0.07164541161289965\n",
      "379 0.0020607115030118655 0.008771745812608697 0.09073845632684223\n",
      "380 0.003010374325290651 0.019046733521674145 0.08398576623574866\n",
      "381 0.0022461091266679986 0.01311918176812293 0.09650755786746851\n",
      "382 0.0018911104575928986 0.013805073691621424 0.10307188042432194\n",
      "383 0.004138657246282081 0.020711314296080162 0.12881905245794129\n",
      "384 0.00293196334719263 0.02247673770215589 0.08245726288025274\n",
      "385 0.004094986412607893 0.013350685316305951 0.0883759249478242\n",
      "386 0.0018059161517734955 0.013685135033594947 0.09563813635304469\n",
      "387 0.001928635825405813 0.016186591431603826 0.11694138494078324\n",
      "388 0.0022406098756750092 0.016268695348411775 0.1226540514181972\n",
      "389 0.00367401486020627 0.02896287585112175 0.11720797538492803\n",
      "390 0.0020342746793081054 0.011788810159980765 0.11486738308178639\n",
      "391 0.0017892493043053011 0.016060621600953045 0.10738746122491602\n",
      "392 0.001476893000520182 0.012787172012761576 0.11727832665924803\n",
      "393 0.002662001740431065 0.02092795737870589 0.11899718846692772\n",
      "394 0.004426940888477544 0.01998627386811659 0.10900686954658913\n",
      "395 0.004868456984479848 0.026696307231520267 0.12597474604645836\n",
      "396 0.004470303952219904 0.017097700674884104 0.09264200543499615\n",
      "397 0.003393175033936897 0.01933718571705397 0.09889747913387296\n",
      "398 0.002236151222914928 0.01808124983978073 0.1031660597077276\n",
      "399 0.0024451341103486762 0.015107725380514258 0.07906148870627144\n",
      "400 0.0039920226947492985 0.02585120061572187 0.12141713927562083\n",
      "401 0.0042019998879142114 0.027105181240844115 0.10556963453568899\n",
      "402 0.0024969655833113652 0.021551197932332553 0.10942974800248664\n",
      "403 0.0017268414838433788 0.01075510539855013 0.07196674046576933\n",
      "404 0.004029012821006339 0.02055101161112707 0.11865109507095514\n",
      "405 0.004485967057350715 0.029668438110173362 0.10085423887660933\n",
      "406 0.0023550060445054143 0.010573667970393635 0.08403252818670859\n",
      "407 0.0013154153526271125 0.018089006918029758 0.09702128676475552\n",
      "408 0.003709808671985345 0.02341508183578095 0.10839770134019858\n",
      "409 0.0014355676456605355 0.01761027641765576 0.1338309981030626\n",
      "410 0.0046076309374483175 0.01840496896413424 0.09837304249413426\n",
      "411 0.004153870950197821 0.021182039159015113 0.10120236897641188\n",
      "412 0.0030179604043688944 0.020945039583741963 0.07844704607409204\n",
      "413 0.0023657471588854288 0.01219154822096808 0.08993054041610987\n",
      "414 0.004353440209648597 0.023412010333619518 0.09130344376206903\n",
      "415 0.002656066929669237 0.017570228561584307 0.11374526414511728\n",
      "416 0.002242474245437395 0.018174316699466387 0.0989329679328873\n",
      "417 0.002848124188478676 0.010554240648454163 0.0925559479322939\n",
      "418 0.0024879348927552437 0.015736238138233764 0.12552226073226064\n",
      "419 0.0034373417891777024 0.022206443759030144 0.127946894554112\n",
      "420 0.0015515500616772396 0.016246109369707114 0.12208991426721436\n",
      "421 0.003209236154395853 0.017964983777449584 0.09565245341121861\n",
      "422 0.002357318069539511 0.0100996052891348 0.09010834477374144\n",
      "423 0.004411605533655906 0.02241292756812671 0.0874861673780597\n",
      "424 0.0044133181585406 0.032721749528060284 0.11607049753539225\n",
      "425 0.0017551377219650335 0.012391634028981552 0.08741003013825152\n",
      "426 0.001807293614926304 0.01439068503819938 0.1286228458355525\n",
      "427 0.00286832532558346 0.019356854405584954 0.11959050650002613\n",
      "428 0.002469489174218714 0.023310300187542085 0.11604017253084446\n",
      "429 0.0028760079889756174 0.01972892681402231 0.12723419784326503\n",
      "430 0.0022745937784413644 0.017231723788192793 0.09178145614939129\n",
      "431 0.0024204096486970905 0.020484540034713006 0.1155326522491199\n",
      "432 0.001109278489461588 0.010452488178471947 0.08956211733979527\n",
      "433 0.0021968500959089693 0.01004948471366092 0.08354769220109405\n",
      "434 0.0019249106890317602 0.017896114786896296 0.12330218112837074\n",
      "435 0.0029254979996220266 0.017730228071773935 0.1199571074682606\n",
      "436 0.0028361167094227055 0.020982774184562715 0.11195574646398054\n",
      "437 0.003958432434032263 0.024173485087556488 0.1219226465700257\n",
      "438 0.0014514116737034512 0.03993526957027923 0.12192790999195569\n",
      "439 0.003004760278271096 0.02340297340575942 0.1090687025445495\n",
      "440 0.002375888455079773 0.017545092820948186 0.11794880732000396\n",
      "441 0.003334921389129922 0.017133648372706416 0.10365523799212734\n",
      "442 0.001976187167416767 0.01301218513995532 0.09839889710528094\n",
      "443 0.001518028863484224 0.010355971198467786 0.08079259242897074\n",
      "444 0.003926525915787947 0.01691097654600124 0.09103875157919143\n",
      "445 0.002608961385485918 0.015901556465357194 0.09615953457511174\n",
      "446 0.0032029586169973205 0.018761020591648157 0.10979660639705542\n",
      "447 0.003528964569176702 0.014865611273057206 0.10490689442930037\n",
      "448 0.003909382934013128 0.021918514096671733 0.10665393457874645\n",
      "449 0.0031645303704701455 0.014708913509557815 0.11757920490741777\n",
      "450 0.003617796106595926 0.01662481973733041 0.09822118377384845\n",
      "451 0.0026159806853160114 0.013611054394750471 0.08340173103975818\n",
      "452 0.00215950547088529 0.015856690802247532 0.1358962535587523\n",
      "453 0.002020163030828165 0.01984718942301699 0.12662840017616672\n",
      "454 0.0031383177110393336 0.01685709046043444 0.0826439958888242\n",
      "455 0.003680799933087033 0.02093919744577192 0.11570469674676868\n",
      "456 0.004241115233463013 0.024450274805844243 0.11172948435481433\n",
      "457 0.0022587473815955427 0.012787997257273647 0.10191919645562839\n",
      "458 0.00151736056789299 0.01189352356308069 0.09950582164864807\n",
      "459 0.004570045835965701 0.022969583823973513 0.1114205434197976\n",
      "460 0.002923068757833154 0.016145327246239877 0.08670642469575\n",
      "461 0.0029226819619058014 0.020028174864191985 0.09227865006702221\n",
      "462 0.003880436146706962 0.021228659958396318 0.10465350770325235\n",
      "463 0.0041218144016269475 0.024432049823625266 0.11091821151578907\n",
      "464 0.0035568594690086235 0.021608162440804136 0.10443244911230826\n",
      "465 0.004179567775831159 0.02577992344521726 0.11680725451348516\n",
      "466 0.003780213858024045 0.022922778325560486 0.12704610313367865\n",
      "467 0.00408484135973794 0.022206170055811072 0.14157651072323632\n",
      "468 0.0015445253009538753 0.017948572979663284 0.09466592375418292\n",
      "469 0.004393610736835814 0.018804185311748108 0.07696878634835796\n",
      "470 0.0029265438429209897 0.017920098798696313 0.11393860739653214\n",
      "471 0.0038136603590027806 0.022936753002799463 0.1479364591889874\n",
      "472 0.001893596922569352 0.014289164564514959 0.11409697265397897\n",
      "473 0.003933615094868103 0.019440115902242948 0.11225947430606761\n",
      "474 0.002879356676613307 0.021901535264777006 0.11636255256387433\n",
      "475 0.0019212076735279342 0.0213956985316275 0.12938619785594346\n",
      "476 0.0011681189705637519 0.0139687318129878 0.06347953529811493\n",
      "477 0.00352145652484999 0.016991879360283964 0.0932434340436655\n",
      "478 0.0049957947077887395 0.025844518761828042 0.11511547733709156\n",
      "479 0.004155834883942772 0.016615523357784066 0.0900174900619232\n",
      "480 0.004418476739796633 0.019038392552716367 0.08804449624615215\n",
      "481 0.0017178035690975332 0.013882163752677458 0.11210329069669411\n",
      "482 0.0034263185858353752 0.01622595513430548 0.12726477186530494\n",
      "483 0.0028460924752468243 0.026312716949060646 0.12166308438506081\n",
      "484 0.0024903381548298372 0.017074813711574564 0.10570996389377564\n",
      "485 0.004875534195376289 0.027871473345398447 0.10066379411485091\n",
      "486 0.0027531793844830805 0.013185631385333844 0.11069459912493465\n",
      "487 0.004059731550053364 0.02847105989986963 0.09802872415233357\n",
      "488 0.002733762952896743 0.013992482581075046 0.10467617114757603\n",
      "489 0.002943521683619838 0.02317481204193837 0.11490092163377397\n",
      "490 0.0024953318705875673 0.011403162688779635 0.08696663576972322\n",
      "491 0.0019160796547358994 0.01494974235502629 0.10925024653743268\n",
      "492 0.0020412581876482283 0.019061657322260873 0.10935426592553034\n",
      "493 0.0033467991855903097 0.01634414324147087 0.1087710610339282\n",
      "494 0.002829304368305296 0.014841497676569896 0.08119390613665745\n",
      "495 0.0032078047608592535 0.017226984474838728 0.10276880726761758\n",
      "496 0.0023327260185587316 0.019115791542894653 0.131206027934005\n",
      "497 0.004272096821618649 0.018790834473052326 0.090270739657634\n",
      "498 0.0036086576235960025 0.029208185181251917 0.09530450608293473\n",
      "499 0.00496789239009127 0.01933640515691106 0.09302842447818134\n",
      "500 0.002701495542828873 0.0202411664525653 0.10599397345273745\n",
      "501 0.0031362135258531164 0.02424205865112533 0.09744918719912155\n",
      "502 0.004786430412847185 0.01393559065894815 0.08729529130251563\n",
      "503 0.0022769658920016567 0.016389876772633292 0.08864971132227155\n",
      "504 0.002576854809003291 0.016425301399683317 0.10754461161786597\n",
      "505 0.0028631036669077574 0.01868623593854375 0.11791077818129862\n",
      "506 0.0031521643741813073 0.02066182837496771 0.09425196328020205\n",
      "507 0.0047549558355904735 0.02646242056128276 0.12781175295472758\n",
      "508 0.001875650990516434 0.015051735329497313 0.11907692806181412\n",
      "509 0.0032281334633054125 0.011094216093136516 0.06692019608313074\n",
      "510 0.0023495960494349694 0.014398274627189199 0.1456802824978946\n",
      "511 0.004519125321592512 0.029516080893541543 0.11427977540024237\n"
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
