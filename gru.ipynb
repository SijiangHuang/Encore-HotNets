{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 2048\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615710"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2.2940549850463867 2.3428141539096834 1002 [0.0001, 0.0001] 30.83094549179077\n",
      "2000 2.291999340057373 2.318759030342102 2002 [0.0001, 0.0001] 60.09594392776489\n",
      "3000 2.351796865463257 2.3089193584918974 3002 [0.0001, 0.0001] 91.00927948951721\n",
      "4000 2.306084156036377 2.306662550449371 4002 [0.0001, 0.0001] 122.32262897491455\n",
      "5000 2.318218946456909 2.3023219056129456 5002 [0.0001, 0.0001] 154.04195046424866\n",
      "6000 2.307072639465332 2.3026534407138826 6002 [0.0001, 0.0001] 185.54656291007996\n",
      "7000 2.346050500869751 2.301287971496582 7002 [0.0001, 0.0001] 217.16300535202026\n",
      "8000 2.3199615478515625 2.299233116865158 8002 [0.0001, 0.0001] 248.89770126342773\n",
      "9000 2.305652141571045 2.298919374227524 9002 [0.0001, 0.0001] 280.306298494339\n",
      "10000 2.2928032875061035 2.297088988304138 10002 [0.0001, 0.0001] 311.7302985191345\n",
      "11000 2.3154568672180176 2.2958345646858214 11002 [0.0001, 0.0001] 343.3379714488983\n",
      "12000 2.2873177528381348 2.2979945073127745 12002 [0.0001, 0.0001] 373.6428096294403\n",
      "13000 2.2760519981384277 2.294053535223007 13002 [0.0001, 0.0001] 403.81963634490967\n",
      "14000 2.2536773681640625 2.2919307911396025 14002 [0.0001, 0.0001] 435.57901787757874\n",
      "15000 2.2838430404663086 2.295814970493317 15002 [0.0001, 0.0001] 465.2802805900574\n",
      "16000 2.2743091583251953 2.294626415014267 16002 [0.0001, 0.0001] 496.75498056411743\n",
      "17000 2.2649567127227783 2.2935266060829163 17002 [0.0001, 0.0001] 528.4464762210846\n",
      "18000 2.260758638381958 2.291459458589554 18002 [0.0001, 0.0001] 559.8859326839447\n",
      "19000 2.309988498687744 2.2922517800331117 19002 [0.0001, 0.0001] 591.1465859413147\n",
      "20000 2.2785017490386963 2.2918692467212676 20002 [0.0001, 0.0001] 622.0118916034698\n",
      "21000 2.265166997909546 2.2862358446121216 21002 [0.0001, 0.0001] 653.4697859287262\n",
      "22000 2.358975887298584 2.284125940799713 22002 [0.0001, 0.0001] 685.1464107036591\n",
      "23000 2.27518892288208 2.282896311044693 23002 [0.0001, 0.0001] 716.2422897815704\n",
      "24000 2.2572021484375 2.2719280378818514 24002 [0.0001, 0.0001] 747.8777160644531\n",
      "25000 2.283344030380249 2.26099294257164 25002 [0.0001, 0.0001] 779.354870557785\n",
      "26000 2.228923797607422 2.2403537757396697 26002 [0.0001, 0.0001] 810.7769796848297\n",
      "27000 2.1895859241485596 2.2081104464530945 27002 [0.0001, 0.0001] 842.2774024009705\n",
      "28000 2.1448814868927 2.1663104288578032 28002 [0.0001, 0.0001] 873.8665301799774\n",
      "29000 2.109065294265747 2.1039676115512846 29002 [0.0001, 0.0001] 905.4785444736481\n",
      "30000 1.9468506574630737 2.0264018359184264 30002 [0.0001, 0.0001] 937.0682156085968\n",
      "31000 1.8990952968597412 1.928021628499031 31002 [0.0001, 0.0001] 968.7825608253479\n",
      "32000 1.7771750688552856 1.818843225479126 32002 [0.0001, 0.0001] 1000.3945634365082\n",
      "33000 1.7327653169631958 1.6994223237037658 33002 [0.0001, 0.0001] 1032.0816276073456\n",
      "34000 1.4170525074005127 1.5786349195241929 34002 [0.0001, 0.0001] 1062.7063653469086\n",
      "35000 1.3844465017318726 1.4577378022670746 35002 [0.0001, 0.0001] 1092.4805824756622\n",
      "36000 1.3901314735412598 1.3406204229593277 36002 [0.0001, 0.0001] 1121.6956613063812\n",
      "37000 1.1573941707611084 1.2329238798618316 37002 [0.0001, 0.0001] 1152.6706283092499\n",
      "38000 1.1136503219604492 1.1365559523105622 38002 [0.0001, 0.0001] 1184.0089585781097\n",
      "39000 1.0323575735092163 1.0443936319947242 39002 [0.0001, 0.0001] 1215.443521976471\n",
      "40000 0.8792963624000549 0.9617283493876457 40002 [0.0001, 0.0001] 1246.0098459720612\n",
      "41000 0.8837892413139343 0.8968152193427086 41002 [0.0001, 0.0001] 1277.0141606330872\n",
      "42000 0.7297677397727966 0.8344245429635048 42002 [0.0001, 0.0001] 1308.0900483131409\n",
      "43000 0.75801020860672 0.7838593125343323 43002 [0.0001, 0.0001] 1339.298024892807\n",
      "44000 0.7496353983879089 0.7368689374923706 44002 [0.0001, 0.0001] 1370.0587284564972\n",
      "45000 0.6425866484642029 0.7016853454709053 45002 [0.0001, 0.0001] 1401.214433670044\n",
      "46000 0.6064237356185913 0.6679097564220429 46002 [0.0001, 0.0001] 1432.3649368286133\n",
      "47000 0.6236867904663086 0.6399055070281029 47002 [0.0001, 0.0001] 1462.5793459415436\n",
      "48000 0.6223722696304321 0.6164679137468339 48002 [0.0001, 0.0001] 1493.8747549057007\n",
      "49000 0.6031877398490906 0.5966184058785439 49002 [0.0001, 0.0001] 1523.5439412593842\n",
      "50000 0.5606359839439392 0.5788400766551495 50002 [0.0001, 0.0001] 1554.526295185089\n",
      "51000 0.5262761116027832 0.5637007469832898 51002 [0.0001, 0.0001] 1585.244288444519\n",
      "52000 0.5432426333427429 0.5513727942407132 52002 [0.0001, 0.0001] 1614.7492136955261\n",
      "53000 0.54342120885849 0.5364367302358151 53002 [0.0001, 0.0001] 1645.8026888370514\n",
      "54000 0.4917784631252289 0.5277693076133728 54002 [0.0001, 0.0001] 1676.7574756145477\n",
      "55000 0.5629929900169373 0.5180643151402473 55002 [0.0001, 0.0001] 1708.066632270813\n",
      "56000 0.48478788137435913 0.5098678847849369 56002 [0.0001, 0.0001] 1739.3536829948425\n",
      "57000 0.5220017433166504 0.4985128917992115 57002 [0.0001, 0.0001] 1770.76624917984\n",
      "58000 0.49732980132102966 0.492427259683609 58002 [0.0001, 0.0001] 1800.5798156261444\n",
      "59000 0.5405435562133789 0.48749173724651335 59002 [0.0001, 0.0001] 1830.1986665725708\n",
      "60000 0.47820544242858887 0.47921649184823034 60002 [0.0001, 0.0001] 1858.8554153442383\n",
      "61000 0.4529557526111603 0.4731758927702904 61002 [0.0001, 0.0001] 1890.1018106937408\n",
      "62000 0.45674195885658264 0.46759114909172056 62002 [0.0001, 0.0001] 1921.408643245697\n",
      "63000 0.4793446660041809 0.46331521394848824 63002 [0.0001, 0.0001] 1952.586228132248\n",
      "64000 0.4700515568256378 0.4579306938946247 64002 [0.0001, 0.0001] 1983.7982051372528\n",
      "65000 0.47946029901504517 0.4543514489233494 65002 [0.0001, 0.0001] 2014.9981355667114\n",
      "66000 0.4319518804550171 0.44757460471987726 66002 [0.0001, 0.0001] 2046.2187762260437\n",
      "67000 0.42079225182533264 0.4449887446165085 67002 [0.0001, 0.0001] 2077.3322942256927\n",
      "68000 0.4042578339576721 0.4395446988940239 68002 [0.0001, 0.0001] 2108.6461782455444\n",
      "69000 0.4499789774417877 0.4365612052679062 69002 [0.0001, 0.0001] 2139.9422466754913\n",
      "70000 0.42739254236221313 0.4352901292145252 70002 [0.0001, 0.0001] 2171.2130918502808\n",
      "71000 0.4316602051258087 0.4297490655183792 71002 [0.0001, 0.0001] 2200.3825345039368\n",
      "72000 0.37781569361686707 0.42753455668687823 72002 [0.0001, 0.0001] 2231.8465628623962\n",
      "73000 0.39438408613204956 0.4236010489165783 73002 [0.0001, 0.0001] 2263.314794778824\n",
      "74000 0.43736889958381653 0.42009856256842615 74002 [0.0001, 0.0001] 2294.7113375663757\n",
      "75000 0.3905843496322632 0.4185124545693398 75002 [0.0001, 0.0001] 2325.8826756477356\n",
      "76000 0.4140572249889374 0.4144145045876503 76002 [0.0001, 0.0001] 2357.189616203308\n",
      "77000 0.3801526129245758 0.412831390529871 77002 [0.0001, 0.0001] 2388.5633075237274\n",
      "78000 0.4047926366329193 0.4096697779595852 78002 [0.0001, 0.0001] 2419.6354715824127\n",
      "79000 0.41222426295280457 0.40724712496995924 79002 [0.0001, 0.0001] 2450.9667675495148\n",
      "80000 0.42621949315071106 0.404867621421814 80002 [0.0001, 0.0001] 2482.3546030521393\n",
      "81000 0.4078946113586426 0.40323667573928834 81002 [0.0001, 0.0001] 2511.71156001091\n",
      "82000 0.34427765011787415 0.4008957567512989 82002 [0.0001, 0.0001] 2542.9726140499115\n",
      "83000 0.3679468035697937 0.3991515034735203 83002 [0.0001, 0.0001] 2574.1872639656067\n",
      "84000 0.357869029045105 0.39643731862306597 84002 [0.0001, 0.0001] 2602.887650489807\n",
      "85000 0.3822519779205322 0.3943599009513855 85002 [0.0001, 0.0001] 2631.5008170604706\n",
      "86000 0.37574198842048645 0.3909554841518402 86002 [0.0001, 0.0001] 2660.592209339142\n",
      "87000 0.36796724796295166 0.3904445141553879 87002 [0.0001, 0.0001] 2690.0257618427277\n",
      "88000 0.352523535490036 0.38834326776862144 88002 [0.0001, 0.0001] 2719.6929454803467\n",
      "89000 0.38185715675354004 0.38703426671028135 89002 [0.0001, 0.0001] 2750.9742472171783\n",
      "90000 0.3961191773414612 0.38681591671705245 90002 [0.0001, 0.0001] 2781.670637369156\n",
      "91000 0.36255866289138794 0.3828739184141159 91002 [0.0001, 0.0001] 2812.0214836597443\n",
      "92000 0.4257361888885498 0.3824895823299885 92002 [0.0001, 0.0001] 2842.2269463539124\n",
      "93000 0.36141982674598694 0.3785865298807621 93002 [0.0001, 0.0001] 2873.25754404068\n",
      "94000 0.3792373836040497 0.37880606251955035 94002 [0.0001, 0.0001] 2904.4823100566864\n",
      "95000 0.376319020986557 0.3767068647146225 95002 [0.0001, 0.0001] 2935.5543944835663\n",
      "96000 0.3504958748817444 0.3771895488202572 96002 [0.0001, 0.0001] 2966.6371743679047\n",
      "97000 0.37492260336875916 0.37305009773373604 97002 [0.0001, 0.0001] 2996.4116513729095\n",
      "98000 0.3651648163795471 0.37386885422468186 98002 [0.0001, 0.0001] 3025.4664602279663\n",
      "99000 0.36183828115463257 0.3716215038597584 99002 [0.0001, 0.0001] 3056.457645893097\n",
      "100000 0.36544135212898254 0.3699135978519916 100002 [0.0001, 0.0001] 3087.532618522644\n",
      "101000 0.35549983382225037 0.3683523226082325 101002 [0.0001, 0.0001] 3116.538421154022\n",
      "102000 0.37284016609191895 0.36761276242136953 102002 [0.0001, 0.0001] 3145.5524673461914\n",
      "103000 0.3723194897174835 0.36655150029063227 103002 [0.0001, 0.0001] 3175.0491399765015\n",
      "104000 0.35114675760269165 0.366329359292984 104002 [0.0001, 0.0001] 3206.5509407520294\n",
      "105000 0.33361145853996277 0.3643189795613289 105002 [0.0001, 0.0001] 3238.1462082862854\n",
      "106000 0.33536675572395325 0.3632249071598053 106002 [0.0001, 0.0001] 3269.5066261291504\n",
      "107000 0.36089321970939636 0.36125357735157015 107002 [0.0001, 0.0001] 3301.0495240688324\n",
      "108000 0.35035422444343567 0.36092310047149656 108002 [0.0001, 0.0001] 3332.537878036499\n",
      "109000 0.3646523952484131 0.35991052159667014 109002 [0.0001, 0.0001] 3364.177757501602\n",
      "110000 0.33153432607650757 0.35759556233882905 110002 [0.0001, 0.0001] 3395.789144515991\n",
      "111000 0.3457035720348358 0.3579035259783268 111002 [0.0001, 0.0001] 3427.343235015869\n",
      "112000 0.3525172472000122 0.35590593999624254 112002 [0.0001, 0.0001] 3458.910578727722\n",
      "113000 0.32975104451179504 0.3564494127929211 113002 [0.0001, 0.0001] 3490.314063310623\n",
      "114000 0.3470194637775421 0.35364265835285186 114002 [0.0001, 0.0001] 3521.7665271759033\n",
      "115000 0.3876880705356598 0.35352756997942925 115002 [0.0001, 0.0001] 3553.2265498638153\n",
      "116000 0.3460942208766937 0.35236348313093185 116002 [0.0001, 0.0001] 3584.0897583961487\n",
      "117000 0.3342790901660919 0.35175504797697066 117002 [0.0001, 0.0001] 3616.646070957184\n",
      "118000 0.3678935766220093 0.3501250447034836 118002 [0.0001, 0.0001] 3647.7227013111115\n",
      "119000 0.37264660000801086 0.349460019826889 119002 [0.0001, 0.0001] 3679.0619416236877\n",
      "120000 0.3505626916885376 0.34818137177824976 120002 [0.0001, 0.0001] 3710.6699640750885\n",
      "121000 0.35444316267967224 0.3486272928416729 121002 [0.0001, 0.0001] 3742.0589261054993\n",
      "122000 0.34990206360816956 0.3480945554077625 122002 [0.0001, 0.0001] 3772.782378911972\n",
      "123000 0.33059176802635193 0.3459016788303852 123002 [0.0001, 0.0001] 3804.289992570877\n",
      "124000 0.3500971794128418 0.34598249292373656 124002 [0.0001, 0.0001] 3835.7585051059723\n",
      "125000 0.3199635148048401 0.34467134335637095 125002 [0.0001, 0.0001] 3867.175799369812\n",
      "126000 0.35581764578819275 0.34424983587861063 126002 [0.0001, 0.0001] 3898.722073316574\n",
      "127000 0.3407936990261078 0.3422582824826241 127002 [0.0001, 0.0001] 3928.884621143341\n",
      "128000 0.36428406834602356 0.34278216764330866 128002 [0.0001, 0.0001] 3959.977560520172\n",
      "129000 0.32162269949913025 0.3416669323146343 129002 [0.0001, 0.0001] 3989.7303173542023\n",
      "130000 0.3598889112472534 0.3400185530185699 130002 [0.0001, 0.0001] 4019.3737485408783\n",
      "131000 0.33056899905204773 0.3403598450124264 131002 [0.0001, 0.0001] 4048.365331888199\n",
      "132000 0.3398534059524536 0.33810109141469 132002 [0.0001, 0.0001] 4077.421663045883\n",
      "133000 0.3308488726615906 0.3391946178674698 133002 [0.0001, 0.0001] 4108.310131549835\n",
      "134000 0.35543254017829895 0.3374834484755993 134002 [0.0001, 0.0001] 4139.365219593048\n",
      "135000 0.32822179794311523 0.3370168563723564 135002 [0.0001, 0.0001] 4170.245704650879\n",
      "136000 0.35423552989959717 0.33694253545999525 136002 [0.0001, 0.0001] 4201.634540557861\n",
      "137000 0.30612632632255554 0.33659767362475396 137002 [0.0001, 0.0001] 4232.849609613419\n",
      "138000 0.3412846624851227 0.33578713738918303 138002 [0.0001, 0.0001] 4263.68203163147\n",
      "139000 0.3431755602359772 0.33503651493787767 139002 [0.0001, 0.0001] 4295.0046310424805\n",
      "140000 0.3294108510017395 0.33379054865241053 140002 [0.0001, 0.0001] 4326.342333316803\n",
      "141000 0.3194619119167328 0.33263963779807093 141002 [0.0001, 0.0001] 4357.6903347969055\n",
      "142000 0.3444370627403259 0.3332593580186367 142002 [0.0001, 0.0001] 4388.9288947582245\n",
      "143000 0.3046739995479584 0.3317320949435234 143002 [0.0001, 0.0001] 4420.356471776962\n",
      "144000 0.3309710919857025 0.3305215312838554 144002 [0.0001, 0.0001] 4451.537979364395\n",
      "145000 0.31072959303855896 0.33194453129172324 145002 [0.0001, 0.0001] 4482.7231278419495\n",
      "146000 0.3204488456249237 0.33080959832668305 146002 [0.0001, 0.0001] 4512.013196468353\n",
      "147000 0.3216160535812378 0.32921470952034 147002 [0.0001, 0.0001] 4543.542708158493\n",
      "148000 0.3210066258907318 0.3284078131318092 148002 [0.0001, 0.0001] 4574.895797729492\n",
      "149000 0.3285582363605499 0.32951382371783255 149002 [0.0001, 0.0001] 4606.306522846222\n",
      "150000 0.3384549021720886 0.3283018141388893 150002 [0.0001, 0.0001] 4637.542510271072\n",
      "151000 0.33106011152267456 0.32711297968029973 151002 [0.0001, 0.0001] 4668.790245056152\n",
      "152000 0.3336034119129181 0.3257633798718452 152002 [0.0001, 0.0001] 4700.227787017822\n",
      "153000 0.3652775287628174 0.3270170419216156 153002 [0.0001, 0.0001] 4729.576991319656\n",
      "154000 0.2948468029499054 0.3248359737098217 154002 [0.0001, 0.0001] 4759.002785205841\n",
      "155000 0.3294098377227783 0.3262006520032883 155002 [0.0001, 0.0001] 4790.279156446457\n",
      "156000 0.35586586594581604 0.32458253225684164 156002 [0.0001, 0.0001] 4821.735798835754\n",
      "157000 0.3243665397167206 0.3244384949505329 157002 [0.0001, 0.0001] 4853.287215709686\n",
      "158000 0.3105618357658386 0.32347215357422826 158002 [0.0001, 0.0001] 4883.886853933334\n",
      "159000 0.30950403213500977 0.3236051230430603 159002 [0.0001, 0.0001] 4915.35045838356\n",
      "160000 0.3223169445991516 0.32260976842045785 160002 [0.0001, 0.0001] 4946.793546915054\n",
      "161000 0.32877543568611145 0.3227173108160496 161002 [0.0001, 0.0001] 4978.326988935471\n",
      "162000 0.31142696738243103 0.32130757814645766 162002 [0.0001, 0.0001] 5009.146191835403\n",
      "163000 0.31814494729042053 0.3204251696169376 163002 [0.0001, 0.0001] 5040.479152441025\n",
      "164000 0.33092501759529114 0.32042134332656863 164002 [0.0001, 0.0001] 5071.885867118835\n",
      "165000 0.2999367415904999 0.3201815299987793 165002 [0.0001, 0.0001] 5101.732787370682\n",
      "166000 0.3083897829055786 0.3198987843394279 166002 [0.0001, 0.0001] 5133.042257785797\n",
      "167000 0.32518088817596436 0.3189899048507214 167002 [0.0001, 0.0001] 5164.408720254898\n",
      "168000 0.3079122006893158 0.31914155331254007 168002 [0.0001, 0.0001] 5195.839249610901\n",
      "169000 0.30309435725212097 0.31833085414767265 169002 [0.0001, 0.0001] 5227.383829832077\n",
      "170000 0.3107169270515442 0.3179904161989689 170002 [0.0001, 0.0001] 5257.010271787643\n",
      "171000 0.3153490424156189 0.31825742807984353 171002 [0.0001, 0.0001] 5286.792435407639\n",
      "172000 0.299867182970047 0.3173591628670692 172002 [0.0001, 0.0001] 5315.8199281692505\n",
      "173000 0.3229368329048157 0.3166692986190319 173002 [0.0001, 0.0001] 5344.819100618362\n",
      "174000 0.295741468667984 0.31671365931630135 174002 [0.0001, 0.0001] 5375.174338340759\n",
      "175000 0.29084259271621704 0.31683911830186845 175002 [0.0001, 0.0001] 5406.8501698970795\n",
      "176000 0.304066926240921 0.314430424451828 176002 [0.0001, 0.0001] 5438.582966566086\n",
      "177000 0.3148801922798157 0.3154556103646755 177002 [0.0001, 0.0001] 5470.134546518326\n",
      "178000 0.3198419213294983 0.3145098769366741 178002 [0.0001, 0.0001] 5500.390222787857\n",
      "179000 0.3373519480228424 0.31490595495700835 179002 [0.0001, 0.0001] 5529.497172355652\n",
      "180000 0.3127848207950592 0.3139048230350018 180002 [0.0001, 0.0001] 5561.035586833954\n",
      "181000 0.3171834945678711 0.3126973751485348 181002 [0.0001, 0.0001] 5592.59768986702\n",
      "182000 0.32933709025382996 0.31366162246465684 182002 [0.0001, 0.0001] 5624.062016963959\n",
      "183000 0.3144751787185669 0.312356043368578 183002 [0.0001, 0.0001] 5655.603181600571\n",
      "184000 0.3123820424079895 0.31215279138088226 184002 [0.0001, 0.0001] 5687.222701311111\n",
      "185000 0.3137127757072449 0.31128711998462677 185002 [0.0001, 0.0001] 5718.682224988937\n",
      "186000 0.3214792311191559 0.312125962972641 186002 [0.0001, 0.0001] 5749.574024915695\n",
      "187000 0.30420175194740295 0.31081334817409517 187002 [0.0001, 0.0001] 5781.0630259513855\n",
      "188000 0.3082115650177002 0.3116990923285484 188002 [0.0001, 0.0001] 5812.484158992767\n",
      "189000 0.3194158673286438 0.31101858365535734 189002 [0.0001, 0.0001] 5844.00698184967\n",
      "190000 0.29783228039741516 0.3101601198613644 190002 [0.0001, 0.0001] 5875.200501918793\n",
      "191000 0.30651211738586426 0.3093415475487709 191002 [0.0001, 0.0001] 5906.455609798431\n",
      "192000 0.3230414092540741 0.3100212523937225 192002 [0.0001, 0.0001] 5937.738634824753\n",
      "193000 0.3083953559398651 0.30889371222257617 193002 [0.0001, 0.0001] 5969.309232950211\n",
      "194000 0.3157360553741455 0.3092263621389866 194002 [0.0001, 0.0001] 5999.402507066727\n",
      "195000 0.29599472880363464 0.30807445335388184 195002 [0.0001, 0.0001] 6030.154215335846\n",
      "196000 0.303774356842041 0.308729810744524 196002 [0.0001, 0.0001] 6060.382298946381\n",
      "197000 0.32668939232826233 0.30792755469679833 197002 [0.0001, 0.0001] 6091.722385883331\n",
      "198000 0.28337690234184265 0.30791168493032456 198002 [0.0001, 0.0001] 6123.017883777618\n",
      "199000 0.28603601455688477 0.3064219189286232 199002 [0.0001, 0.0001] 6152.682660102844\n",
      "200000 0.31385892629623413 0.307707309871912 200002 [0.0001, 0.0001] 6183.882403373718\n",
      "201000 0.3067539632320404 0.3056611977517605 201002 [0.0001, 0.0001] 6215.230801582336\n",
      "202000 0.29691585898399353 0.3058595240712166 202002 [0.0001, 0.0001] 6246.549428939819\n",
      "203000 0.336922287940979 0.30583634427189826 203002 [0.0001, 0.0001] 6278.274600505829\n",
      "204000 0.2954719364643097 0.30555997088551523 204002 [0.0001, 0.0001] 6309.80720448494\n",
      "205000 0.2945472300052643 0.3052646114528179 205002 [0.0001, 0.0001] 6338.950866937637\n",
      "206000 0.3128286600112915 0.3057826009094715 206002 [0.0001, 0.0001] 6370.310261249542\n",
      "207000 0.2964845597743988 0.3044593399167061 207002 [0.0001, 0.0001] 6401.640335798264\n",
      "208000 0.31053122878074646 0.3056492468714714 208002 [0.0001, 0.0001] 6433.0979907512665\n",
      "209000 0.3093491196632385 0.3042690872848034 209002 [0.0001, 0.0001] 6463.6409521102905\n",
      "210000 0.30021801590919495 0.304473102748394 210002 [0.0001, 0.0001] 6492.690855979919\n",
      "211000 0.31479084491729736 0.3030656575858593 211002 [0.0001, 0.0001] 6521.948456525803\n",
      "212000 0.28308138251304626 0.30391378977894784 212002 [0.0001, 0.0001] 6553.492720127106\n",
      "213000 0.279803991317749 0.3028752368688583 213002 [0.0001, 0.0001] 6584.922391414642\n",
      "214000 0.29671937227249146 0.30300243955850603 214002 [0.0001, 0.0001] 6616.562494277954\n",
      "215000 0.30142685770988464 0.3021608504652977 215002 [0.0001, 0.0001] 6648.829500198364\n",
      "216000 0.28504249453544617 0.3026408723294735 216002 [0.0001, 0.0001] 6680.608150720596\n",
      "217000 0.3166477084159851 0.30184914284944536 217002 [0.0001, 0.0001] 6712.162065029144\n",
      "218000 0.31699758768081665 0.30132871216535567 218002 [0.0001, 0.0001] 6743.819501399994\n",
      "219000 0.3117961585521698 0.30131762704253195 219002 [0.0001, 0.0001] 6774.567911624908\n",
      "220000 0.28354939818382263 0.3008665748536587 220002 [0.0001, 0.0001] 6804.2017204761505\n",
      "221000 0.31451407074928284 0.30104463985562324 221002 [0.0001, 0.0001] 6835.671648025513\n",
      "222000 0.29377803206443787 0.29998519469797613 222002 [0.0001, 0.0001] 6867.243104219437\n",
      "223000 0.3056740164756775 0.3003586288392544 223002 [0.0001, 0.0001] 6898.813631534576\n",
      "224000 0.3141534924507141 0.3006667110621929 224002 [0.0001, 0.0001] 6930.318701505661\n",
      "225000 0.29568609595298767 0.30007945269346237 225002 [0.0001, 0.0001] 6961.862721681595\n",
      "226000 0.32667168974876404 0.29883623045682906 226002 [0.0001, 0.0001] 6993.50289940834\n",
      "227000 0.3071664273738861 0.2986839422881603 227002 [0.0001, 0.0001] 7025.2583527565\n",
      "228000 0.30982282757759094 0.29906018590927125 228002 [0.0001, 0.0001] 7056.224977970123\n",
      "229000 0.2914065718650818 0.2996262543797493 229002 [0.0001, 0.0001] 7086.798097610474\n",
      "230000 0.2996596395969391 0.29826825150847436 230002 [0.0001, 0.0001] 7117.938344478607\n",
      "231000 0.2728385329246521 0.29943326517939567 231002 [0.0001, 0.0001] 7146.904771327972\n",
      "232000 0.3188253343105316 0.29746735709905625 232002 [0.0001, 0.0001] 7178.068090438843\n",
      "233000 0.2779828608036041 0.29732813200354574 233002 [0.0001, 0.0001] 7209.170329093933\n",
      "234000 0.31202563643455505 0.29834446677565574 234002 [0.0001, 0.0001] 7240.429847478867\n",
      "235000 0.2876032590866089 0.2974422293752432 235002 [0.0001, 0.0001] 7270.21839261055\n",
      "236000 0.29741066694259644 0.2971780140697956 236002 [0.0001, 0.0001] 7301.40301656723\n",
      "237000 0.2910216748714447 0.2978580873906612 237002 [0.0001, 0.0001] 7332.472106933594\n",
      "238000 0.2931768298149109 0.2969409567117691 238002 [0.0001, 0.0001] 7363.234197616577\n",
      "239000 0.29608628153800964 0.29578612208366395 239002 [0.0001, 0.0001] 7394.54218006134\n",
      "240000 0.2953479290008545 0.2970846461355686 240002 [0.0001, 0.0001] 7425.6899926662445\n",
      "241000 0.29112106561660767 0.29593181106448174 241002 [0.0001, 0.0001] 7456.7856361866\n",
      "242000 0.2913694977760315 0.2947570373415947 242002 [0.0001, 0.0001] 7487.173820972443\n",
      "243000 0.3022126257419586 0.2952499030828476 243002 [0.0001, 0.0001] 7517.637859106064\n",
      "244000 0.28087684512138367 0.2957345209121704 244002 [0.0001, 0.0001] 7547.687403678894\n",
      "245000 0.30835509300231934 0.2956384120881557 245002 [0.0001, 0.0001] 7576.663228750229\n",
      "246000 0.2970730662345886 0.29579250356554987 246002 [0.0001, 0.0001] 7606.910778045654\n",
      "247000 0.3011518716812134 0.29418301504850386 247002 [0.0001, 0.0001] 7637.889608860016\n",
      "248000 0.2886461019515991 0.29486850795149805 248002 [0.0001, 0.0001] 7668.986745595932\n",
      "249000 0.27149343490600586 0.29413514682650566 249002 [0.0001, 0.0001] 7700.190139770508\n",
      "250000 0.3055228292942047 0.2951429077982903 250002 [0.0001, 0.0001] 7730.966519832611\n",
      "251000 0.3067508339881897 0.29447692131996156 251002 [0.0001, 0.0001] 7761.17564034462\n",
      "252000 0.29329779744148254 0.29243767765164375 252002 [0.0001, 0.0001] 7791.026660203934\n",
      "253000 0.31574469804763794 0.29378160679340365 253002 [0.0001, 0.0001] 7822.437917947769\n",
      "254000 0.31329602003097534 0.2933222467452288 254002 [0.0001, 0.0001] 7853.810344696045\n",
      "255000 0.28447893261909485 0.29350374346971514 255002 [0.0001, 0.0001] 7885.00208234787\n",
      "256000 0.2777831554412842 0.2932024087011814 256002 [0.0001, 0.0001] 7916.373348712921\n",
      "257000 0.29353994131088257 0.29318339422345163 257002 [0.0001, 0.0001] 7947.638544797897\n",
      "258000 0.3083533048629761 0.2930916327387095 258002 [0.0001, 0.0001] 7978.734711647034\n",
      "259000 0.2920786142349243 0.29128247529268264 259002 [0.0001, 0.0001] 8009.9127633571625\n",
      "260000 0.3095610439777374 0.2921039499938488 260002 [0.0001, 0.0001] 8041.00622344017\n",
      "261000 0.29361897706985474 0.2922112277150154 261002 [0.0001, 0.0001] 8072.098347425461\n",
      "262000 0.28856030106544495 0.29256999137997625 262002 [0.0001, 0.0001] 8103.214555263519\n",
      "263000 0.2977774441242218 0.29179583513736723 263002 [0.0001, 0.0001] 8134.078945398331\n",
      "264000 0.2708847224712372 0.29152158802747724 264002 [0.0001, 0.0001] 8164.312631845474\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m plot_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(i, loss, avg_loss \u001b[39m/\u001b[39m plot_every, scheduler\u001b[39m.\u001b[39m_step_count, scheduler\u001b[39m.\u001b[39mget_last_lr(),time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m s_time)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(gru, \u001b[39m'\u001b[39;49m\u001b[39mmodel/gru-0914.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     torch\u001b[39m.\u001b[39msave(s2h, \u001b[39m'\u001b[39m\u001b[39mmodel/s2h-0914.pth\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     save_dict \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m:i,\u001b[39m\"\u001b[39m\u001b[39moptimizer\u001b[39m\u001b[39m\"\u001b[39m:optimizer\u001b[39m.\u001b[39mstate_dict(),\u001b[39m\"\u001b[39m\u001b[39m_step_count\u001b[39m\u001b[39m\"\u001b[39m:scheduler\u001b[39m.\u001b[39m_step_count,\u001b[39m\"\u001b[39m\u001b[39mlast_epoch\u001b[39m\u001b[39m\"\u001b[39m:scheduler\u001b[39m.\u001b[39mlast_epoch}\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/serialization.py:376\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \n\u001b[1;32m    342\u001b[0m \u001b[39mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[39m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    374\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 376\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    377\u001b[0m     \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    378\u001b[0m         \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/serialization.py:214\u001b[0m, in \u001b[0;36m_open_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> 214\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mclose()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-0914.pth')\n",
    "        torch.save(s2h, 'model/s2h-0914.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-0914.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7, 4] False\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 11, 10, 7, 7, 4, 6, 7, 6, 6, 4, 13, 10, 8, 8] False\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 6, 7, 11, 10, 4, 10, 4, 11, 4, 4, 13, 7, 13, 16, 4] False\n",
      "[8, 4, 4, 12, 10, 7, 6, 4, 4, 11, 13, 7, 4, 8, 4, 8] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6, 4] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 7, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6] False\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 13, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 4, 4, 8, 13, 7, 4, 4, 4, 15] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 4, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 9, 10, 8, 4] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 8, 4, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] False\n",
      "[8, 11, 4, 4, 13, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8] False\n",
      "[8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4, 8] False\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 13, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] False\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 11, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4] False\n",
      "[8, 4, 6, 8, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] False\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 11, 6, 4, 13, 6, 6, 4, 13, 10, 8, 8, 7, 11, 7, 4] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 11, 11, 13, 2, 11] False\n",
      "[8, 6, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13] False\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8] False\n",
      "[8, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 11, 7, 7, 11, 11, 9, 10, 8, 4, 7, 7, 10, 4, 9, 7] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 4, 13, 7, 13, 16, 4, 4] False\n",
      "[8, 4, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 9, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 6, 8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.003944565208398479 0.016781148245429797 0.06808682802283574\n",
      "1 0.0025903342227649364 0.02987089331031155 0.09587070479026971\n",
      "2 0.0033360525254956825 0.022556632148657958 0.09041966129037701\n",
      "3 0.004628760346000186 0.031107602786443492 0.08037825775341918\n",
      "4 0.0041003293955222024 0.032061916835403576 0.11541280624370863\n",
      "5 0.0031500095381508508 0.016476131574397507 0.055246079434388404\n",
      "6 0.0030228677172197123 0.031109569141396908 0.12312332078551216\n",
      "7 0.004535425256679167 0.033120491896628315 0.11718434610348075\n",
      "8 0.0019924735180910534 0.026278246159958676 0.13076504713529877\n",
      "9 0.0018775322611223706 0.017818543573847184 0.09984615243879238\n",
      "10 0.004939289427890514 0.018804570710435887 0.08451325312891833\n",
      "11 0.0048257212210418374 0.03366987608090261 0.12444833584924142\n",
      "12 0.0025714024696180046 0.02351317146542839 0.11170162928225534\n",
      "13 0.004739098225634076 0.02584743148973347 0.1058306659955253\n",
      "14 0.0034977180231552767 0.02087466508641523 0.09919430254377648\n",
      "15 0.002963545007574366 0.03358075196038686 0.09651893234206828\n",
      "16 0.0034330737112921073 0.018948669102329337 0.08211342620477911\n",
      "17 0.003459843712910209 0.0315111948625599 0.1338076640505814\n",
      "18 0.003671117382610471 0.0296495657599222 0.09505802848246517\n",
      "19 0.004066415317950814 0.02501116262346634 0.10725497113284456\n",
      "20 0.004060033013308256 0.026446888352832362 0.1037537812791009\n",
      "21 0.003249364821115845 0.019190188295992346 0.10996115827061187\n",
      "22 0.0034092094903381757 0.031052180704355253 0.11219160432682712\n",
      "23 0.002529888886092302 0.032457070880914686 0.12603446948128155\n",
      "24 0.0033259628334575764 0.0214532638719022 0.10531520669260094\n",
      "25 0.0028567477842109212 0.022599446089425497 0.12676180410838037\n",
      "26 0.003549021183940344 0.023620360547455428 0.09891848859700221\n",
      "27 0.0037966226057197607 0.020755801627446994 0.0730846074721768\n",
      "28 0.0022537901278215627 0.023850716536450503 0.1104124405284788\n",
      "29 0.004674845293714287 0.024610822959824724 0.13541102398212612\n",
      "30 0.003996994657372173 0.028091997416483043 0.08280698660367523\n",
      "31 0.0031969970609726974 0.0138994153012564 0.04635265419617785\n",
      "32 0.004006603422663624 0.018434688666331512 0.11417219516820398\n",
      "33 0.002376949776474093 0.018909365000371353 0.09203515388689569\n",
      "34 0.004588397344629568 0.03365619292571951 0.11629595595033732\n",
      "35 0.0022622196153927045 0.019768721156729716 0.09665539912796595\n",
      "36 0.004414625905587849 0.027458968161381794 0.11038901171921124\n",
      "37 0.0029507479441609474 0.016180107863359883 0.08587515511754218\n",
      "38 0.0037312517972098026 0.02250610402555424 0.07490635155715515\n",
      "39 0.0025735404452048843 0.03313785161217285 0.11519057774560024\n",
      "40 0.004323708248716687 0.026830546414221595 0.10653623464363625\n",
      "41 0.0036505179856898746 0.0267430116356302 0.08413754987357422\n",
      "42 0.0033555296705755026 0.017299430524387253 0.08516542744974342\n",
      "43 0.004816847391863971 0.02812234338817535 0.09813521422555554\n",
      "44 0.00457373260020507 0.02756096943068871 0.10673038793264576\n",
      "45 0.0048561220495496734 0.03838436837738016 0.1045416717245708\n",
      "46 0.004284139500995762 0.030403364854631704 0.10131811258050497\n",
      "47 0.004792654027969931 0.030021937379460406 0.09352312731515616\n",
      "48 0.003887274600261192 0.021500205946386297 0.11032567787826482\n",
      "49 0.0021278854703712354 0.012162942190704993 0.08237362303622925\n",
      "50 0.004180853384418922 0.024565365566829235 0.1040230247965717\n",
      "51 0.004399836510789634 0.02950632538508915 0.12283278093789532\n",
      "52 0.004472799098492164 0.023270320957876192 0.10298109460560273\n",
      "53 0.004925425357145951 0.020779939843391132 0.08920788958351944\n",
      "54 0.003024610822416697 0.020143480428514812 0.11421734937305407\n",
      "55 0.004515633494128169 0.024368233728205617 0.10328101049662033\n",
      "56 0.004069880119259075 0.027072945072694664 0.10315916506533149\n",
      "57 0.004327103694223044 0.027435261709412736 0.08492055734442056\n",
      "58 0.003990416212336466 0.038779897376701185 0.13174019180173754\n",
      "59 0.0029779566823644283 0.021952372146525664 0.0829196744059342\n",
      "60 0.004968946752595795 0.03911959480833689 0.1306220140729357\n",
      "61 0.003359939692981648 0.021068894589858403 0.07898722130023553\n",
      "62 0.002352167370560483 0.02401767157351104 0.09239453235357423\n",
      "63 0.003072428128590859 0.033466056292739986 0.1156648415831107\n",
      "64 0.003798304292182489 0.024727161689338605 0.05288173229587856\n",
      "65 0.0030761866558553716 0.030580575481997156 0.09313685758381249\n",
      "66 0.0030594326011931005 0.019983893106866216 0.09539199109774618\n",
      "67 0.004478604677261732 0.02096158387757191 0.08935499447206391\n",
      "68 0.0036351148393429927 0.035257765920650455 0.11256503973884295\n",
      "69 0.002262278859174557 0.020001300590032516 0.09782239060365072\n",
      "70 0.0031557251784055276 0.021834733420004034 0.09645210707340438\n",
      "71 0.0038128503946233496 0.025069222533354987 0.09208198734110978\n",
      "72 0.004990933869811773 0.03419060558788757 0.09612596438433921\n",
      "73 0.0026438712956384277 0.021276182721576668 0.11424021784363969\n",
      "74 0.004474969102061984 0.040689986840873396 0.11966501316647711\n",
      "75 0.002768384869059546 0.02628221687112075 0.129072299761036\n",
      "76 0.004225910293242761 0.03423016980215593 0.09235306177950497\n",
      "77 0.004395346781094272 0.02921902613131822 0.1097189511006955\n",
      "78 0.0032050139705974284 0.017591506693889716 0.10872402078336675\n",
      "79 0.004877039446411191 0.028516062839422224 0.1110879621573403\n",
      "80 0.003997810961037607 0.043544190292719206 0.1291119242741445\n",
      "81 0.0034397328699812928 0.022640582178759916 0.09893653787270035\n",
      "82 0.00264717838416159 0.020254461018510653 0.1239971912601549\n",
      "83 0.004801969007644156 0.025274422806776803 0.0837604983927544\n",
      "84 0.0036416700517723044 0.02032106687004262 0.08718830825250004\n",
      "85 0.004902184659839812 0.03715737872354484 0.11699409538241046\n",
      "86 0.003921823493303632 0.028238655372175844 0.1175586511420501\n",
      "87 0.0038747029878889454 0.040231339643137576 0.1056548716485837\n",
      "88 0.004730074855119356 0.027506853633525098 0.10558963026951812\n",
      "89 0.0024721777496743676 0.020238520414776758 0.09345523384602483\n",
      "90 0.004731342363303113 0.03388691465868339 0.08669679087887328\n",
      "91 0.0029862549240347747 0.023328300982691997 0.1022206765504856\n",
      "92 0.002293000975903595 0.018678301331231578 0.06611252741223159\n",
      "93 0.0025880727353715825 0.02386443216789446 0.08674519970533176\n",
      "94 0.0020061622579370594 0.01749016896472977 0.08863416448218087\n",
      "95 0.00199941574589305 0.022851486210290407 0.1165412214199228\n",
      "96 0.00337904016285808 0.02231093846617628 0.08983328362148077\n",
      "97 0.004300406935706644 0.02855535706352614 0.110467308637513\n",
      "98 0.002382594442825862 0.01966358217852781 0.11332896683555777\n",
      "99 0.0016063366925487509 0.01930053402174954 0.10379974340790225\n",
      "100 0.004146053330269372 0.021879297410051672 0.13692131367277433\n",
      "101 0.0032612591377402895 0.021831674656082628 0.10134940635999988\n",
      "102 0.003638546911208803 0.023809891891379022 0.08852600013635087\n",
      "103 0.004910850427797679 0.038989648372462354 0.1233876304530812\n",
      "104 0.003006764182184499 0.024598507996733885 0.10170668247132991\n",
      "105 0.0033996379875588104 0.023454311724233776 0.08884725333651065\n",
      "106 0.0030003652448031934 0.03011656873514754 0.10861723104534445\n",
      "107 0.004370343915454383 0.038758417444742836 0.12555721217236226\n",
      "108 0.004707092860676274 0.02542473961181021 0.10326605029160557\n",
      "109 0.0036799000732363764 0.03370893610806333 0.10615126756144289\n",
      "110 0.00354917913192797 0.01585802259466111 0.10128003927610213\n",
      "111 0.0039053240395300537 0.018636797924690262 0.07256352029682542\n",
      "112 0.0042766650260303235 0.029276110644327752 0.12788513309612023\n",
      "113 0.004012999262979593 0.02971478641311577 0.12638875080914658\n",
      "114 0.003401961316295929 0.024945882066983005 0.12538845333782622\n",
      "115 0.00393904836859023 0.02340474336730161 0.10652496228845723\n",
      "116 0.0032319835183962594 0.022937423048348445 0.11258895932137199\n",
      "117 0.004361347554091461 0.030999172906710146 0.09995493399080008\n",
      "118 0.0022995545815584334 0.019094368640868498 0.08121152664316994\n",
      "119 0.004924852136953305 0.03244525075256812 0.138250433309481\n",
      "120 0.0038953694565109814 0.02442890052561251 0.11502864659880893\n",
      "121 0.0044307805242684755 0.028720564763816514 0.09329764008179678\n",
      "122 0.004720926606314476 0.021134150969521266 0.07073966493734425\n",
      "123 0.004185965271306744 0.019795927035749075 0.07782755384879206\n",
      "124 0.003939645920335557 0.017502540842219298 0.06420187588269041\n",
      "125 0.004244612278301163 0.028690295450942344 0.10392707744431326\n",
      "126 0.0027169403944473326 0.023426940739181563 0.1163588196417264\n",
      "127 0.004483644017112223 0.02509794694290425 0.08360067206775501\n",
      "128 0.004503232141247132 0.028289747863260814 0.12537942197044644\n",
      "129 0.0030453923494857746 0.022272719615307156 0.06821774495692742\n",
      "130 0.0027057706918692543 0.019881680124580626 0.10037983821453339\n",
      "131 0.004536679272410249 0.024443632334061674 0.10089131270805735\n",
      "132 0.004732032764912792 0.02340590078515165 0.09297148849780391\n",
      "133 0.004524480931550541 0.047866771066571154 0.10515749732217408\n",
      "134 0.0018524372118653518 0.020105158234747518 0.10716850891620353\n",
      "135 0.00442407918900251 0.032774682104883976 0.0918415288049867\n",
      "136 0.0023503047586405155 0.0218005607059387 0.12431621740871861\n",
      "137 0.0025969160079311725 0.022517403786468652 0.10609606676928945\n",
      "138 0.004728905549084434 0.028033164874914855 0.10434777736504153\n",
      "139 0.004802581909779335 0.02770172945548616 0.09763832990793875\n",
      "140 0.0033151826604245955 0.026208539910222095 0.10255466969416974\n",
      "141 0.004422298632434295 0.027483356614291066 0.11259642168878181\n",
      "142 0.002658732041818477 0.017486345096147576 0.09193579208817079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, seq_len, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m inputTensor(np\u001b[39m.\u001b[39marray([[size]]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39;49m, hn)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, hidden):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     out, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgru(x, hidden)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m     i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/rnn.py:950\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 950\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mgru(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    951\u001b[0m                      \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    952\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    953\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mgru(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    954\u001b[0m                      \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
