{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 512\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 2\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153255"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = 'final'\n",
    "gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2.3560237884521484 2.337338850021362 35.08525800704956\n",
      "2000 2.3442065715789795 2.3189493651390074 69.29851531982422\n",
      "3000 2.2512567043304443 2.315930300235748 103.50826382637024\n",
      "4000 2.357316017150879 2.312451336860657 138.03640031814575\n",
      "5000 2.3247456550598145 2.3104261608123777 170.9530153274536\n",
      "6000 2.2565419673919678 2.3092987895011903 203.3507399559021\n",
      "7000 2.295680522918701 2.304775906562805 235.79189920425415\n",
      "8000 2.3097705841064453 2.293024516820908 268.24331164360046\n",
      "9000 2.278989315032959 2.2736584129333495 300.76643109321594\n",
      "10000 2.223881721496582 2.230084918498993 332.99957489967346\n",
      "11000 2.1110618114471436 2.169562368154526 365.38419556617737\n",
      "12000 2.0909457206726074 2.0942262637615205 397.2796416282654\n",
      "13000 1.9633607864379883 2.0059597256183626 429.4832797050476\n",
      "14000 1.8726255893707275 1.91761769258976 460.7541344165802\n",
      "15000 1.794520616531372 1.8459372512102128 492.3358759880066\n",
      "16000 1.7628390789031982 1.7954390431642533 523.6681759357452\n",
      "17000 1.7232913970947266 1.759653204679489 554.7708835601807\n",
      "18000 1.7547988891601562 1.7340285799503325 585.9335861206055\n",
      "19000 1.705254316329956 1.7140514630079269 617.506059885025\n",
      "20000 1.6967525482177734 1.6993998725414277 648.7431371212006\n",
      "21000 1.6767237186431885 1.6849319522380828 681.150582075119\n",
      "22000 1.6773691177368164 1.6729297552108764 713.3797450065613\n",
      "23000 1.6773673295974731 1.661775460600853 744.601645231247\n",
      "24000 1.649911642074585 1.6502748119831085 775.6909651756287\n",
      "25000 1.6385126113891602 1.6417661144733429 806.7842857837677\n",
      "26000 1.6198430061340332 1.6323147766590118 837.9241468906403\n",
      "27000 1.629763126373291 1.6253926571607589 869.0177040100098\n",
      "28000 1.5925006866455078 1.6166836496591568 901.3799839019775\n",
      "29000 1.6318597793579102 1.6108644043207168 933.403913974762\n",
      "30000 1.6199235916137695 1.606836233139038 965.7231814861298\n",
      "31000 1.5648558139801025 1.6016309869289398 998.0759723186493\n",
      "32000 1.5993785858154297 1.5961810747385026 1030.4758310317993\n",
      "33000 1.5765882730484009 1.593486959695816 1062.6877324581146\n",
      "34000 1.5982441902160645 1.590251663684845 1094.7406527996063\n",
      "35000 1.5501558780670166 1.586901986002922 1127.0943746566772\n",
      "36000 1.5428225994110107 1.5843919539451599 1158.9147176742554\n",
      "37000 1.5991185903549194 1.5822285153865814 1191.056798696518\n",
      "38000 1.571063756942749 1.5794825180768968 1223.3591232299805\n",
      "39000 1.5512897968292236 1.5762617243528365 1254.3892908096313\n",
      "40000 1.588279128074646 1.5755961647033692 1285.7000410556793\n",
      "41000 1.6112593412399292 1.5733049738407134 1317.9741234779358\n",
      "42000 1.5507529973983765 1.5719573003053666 1349.7790114879608\n",
      "43000 1.5916614532470703 1.570943190574646 1381.589833498001\n",
      "44000 1.5893112421035767 1.569640257716179 1412.45228433609\n",
      "45000 1.603840947151184 1.5682294710874558 1442.5644710063934\n",
      "46000 1.5711108446121216 1.5666856603622437 1474.2208285331726\n",
      "47000 1.537670373916626 1.5655310997962952 1505.778085231781\n",
      "48000 1.5847952365875244 1.5649243685007095 1538.2793724536896\n",
      "49000 1.5575535297393799 1.564423706650734 1570.6523480415344\n",
      "50000 1.560975193977356 1.5632952817678452 1603.1260697841644\n",
      "51000 1.558258056640625 1.561157559990883 1635.576812028885\n",
      "52000 1.546515703201294 1.5621481022834778 1667.9749953746796\n",
      "53000 1.5664029121398926 1.5606035726070404 1699.2546548843384\n",
      "54000 1.6025629043579102 1.5603374518156052 1730.859792470932\n",
      "55000 1.5730476379394531 1.5590089888572694 1763.3105273246765\n",
      "56000 1.5470494031906128 1.5582519228458405 1795.1715159416199\n",
      "57000 1.5732243061065674 1.556564297914505 1826.4077262878418\n",
      "58000 1.5369672775268555 1.5576042742729188 1858.122874736786\n",
      "59000 1.5277409553527832 1.556809531569481 1889.7830667495728\n",
      "60000 1.5596370697021484 1.556343817591667 1922.2412214279175\n",
      "61000 1.5607125759124756 1.555700698852539 1954.2169570922852\n",
      "62000 1.5405160188674927 1.557175189256668 1986.5343294143677\n",
      "63000 1.5563338994979858 1.5549680713415146 2019.0346229076385\n",
      "64000 1.5728222131729126 1.5543984379768372 2051.3559999465942\n",
      "65000 1.5552923679351807 1.5542144221067429 2083.694315433502\n",
      "66000 1.570504903793335 1.5537748357057573 2116.1475434303284\n",
      "67000 1.5373761653900146 1.5537045178413391 2148.376601934433\n",
      "68000 1.5426609516143799 1.5544028384685515 2179.4892127513885\n",
      "69000 1.5192455053329468 1.5527100026607514 2210.852009534836\n",
      "70000 1.587450385093689 1.5520614179372787 2242.903305530548\n",
      "71000 1.5580992698669434 1.5526877703666686 2273.459052324295\n",
      "72000 1.555145263671875 1.5514786858558656 2304.7792875766754\n",
      "73000 1.5680601596832275 1.5531039484739304 2335.7534563541412\n",
      "74000 1.5482616424560547 1.5514478046894074 2366.1004724502563\n",
      "75000 1.5258574485778809 1.5506422625780105 2397.7073493003845\n",
      "76000 1.5145710706710815 1.551285550236702 2429.50545334816\n",
      "77000 1.5558842420578003 1.5488237431049348 2461.0131809711456\n",
      "78000 1.555767297744751 1.5501318565607072 2492.692496061325\n",
      "79000 1.5407061576843262 1.5503609501123428 2523.580565214157\n",
      "80000 1.5219416618347168 1.5502900003194808 2554.288714647293\n",
      "81000 1.532743215560913 1.549132696390152 2584.9048845767975\n",
      "82000 1.5420328378677368 1.549302796125412 2616.231549024582\n",
      "83000 1.5261800289154053 1.5494863113164903 2646.7426652908325\n",
      "84000 1.51460599899292 1.547981651544571 2678.2473320961\n",
      "85000 1.5398671627044678 1.5499964005947113 2709.256103992462\n",
      "86000 1.5607717037200928 1.5486066020727158 2740.6320176124573\n",
      "87000 1.5828683376312256 1.5490920503139496 2772.3759434223175\n",
      "88000 1.517685890197754 1.5473884860277176 2804.2195088863373\n",
      "89000 1.553857445716858 1.5474274439811706 2836.000710248947\n",
      "90000 1.5623071193695068 1.5473646380901336 2865.951291322708\n",
      "91000 1.538205623626709 1.5477080155611038 2895.9105117321014\n",
      "92000 1.5561944246292114 1.5458330875635147 2925.905261039734\n",
      "93000 1.5016722679138184 1.5473460702896118 2955.8416316509247\n",
      "94000 1.5421645641326904 1.5474605536460877 2985.8235569000244\n",
      "95000 1.5286543369293213 1.5465986486673355 3016.399379968643\n",
      "96000 1.5802679061889648 1.5467449038028718 3047.572217464447\n",
      "97000 1.5839934349060059 1.5469543406963349 3079.1464138031006\n",
      "98000 1.5331051349639893 1.5457642319202423 3110.5544788837433\n",
      "99000 1.5047045946121216 1.5458855960369111 3141.912596464157\n",
      "100000 1.5108519792556763 1.5452937438488006 3173.4228899478912\n",
      "101000 1.5557509660720825 1.545748563528061 3205.324316263199\n",
      "102000 1.5462020635604858 1.5457959775924683 3236.7048337459564\n",
      "103000 1.5442349910736084 1.5458175412416457 3268.093451976776\n",
      "104000 1.5367578268051147 1.545273897767067 3299.9584064483643\n",
      "105000 1.5827621221542358 1.545572608232498 3331.9280939102173\n",
      "106000 1.5836998224258423 1.54538914000988 3363.2144916057587\n",
      "107000 1.5323867797851562 1.5445940568447114 3393.70884513855\n",
      "108000 1.5216481685638428 1.5443019542694092 3424.1734380722046\n",
      "109000 1.5617570877075195 1.5442307987213135 3454.6206636428833\n",
      "110000 1.5395480394363403 1.543887822508812 3485.3595645427704\n",
      "111000 1.5181777477264404 1.5443928008079528 3516.9766898155212\n",
      "112000 1.5572888851165771 1.5441530349254609 3548.79532623291\n",
      "113000 1.5433002710342407 1.5446744310855864 3580.395452976227\n",
      "114000 1.5331337451934814 1.5445691591501236 3612.6786320209503\n",
      "115000 1.5211666822433472 1.5434296199083328 3645.0475833415985\n",
      "116000 1.544884443283081 1.5426857463121415 3677.5008132457733\n",
      "117000 1.5447280406951904 1.5440678398609162 3709.962968826294\n",
      "118000 1.5426141023635864 1.5441635440587997 3742.3754992485046\n",
      "119000 1.5509536266326904 1.5431286234855652 3774.7775869369507\n",
      "120000 1.5419491529464722 1.5448019098043442 3807.075383424759\n",
      "121000 1.5806827545166016 1.5438260265588761 3839.431827545166\n",
      "122000 1.5208172798156738 1.5430291821956634 3871.4386701583862\n",
      "123000 1.5793931484222412 1.5439041420221329 3903.9208147525787\n",
      "124000 1.5118749141693115 1.5426492080688476 3936.167085647583\n",
      "125000 1.5420968532562256 1.542546709895134 3968.5518424510956\n",
      "126000 1.5511736869812012 1.543043959736824 4000.9470493793488\n",
      "127000 1.5349628925323486 1.5433899257183075 4033.3925919532776\n",
      "128000 1.5381394624710083 1.5435462579727173 4065.7767519950867\n",
      "129000 1.5216481685638428 1.542345915555954 4098.168144464493\n",
      "130000 1.5273373126983643 1.5434868525266647 4130.635691642761\n",
      "131000 1.5727860927581787 1.5426499445438384 4163.038761138916\n",
      "132000 1.5557270050048828 1.5434223647117615 4195.451550245285\n",
      "133000 1.51845383644104 1.542831622838974 4227.844605922699\n",
      "134000 1.5368729829788208 1.541220271706581 4260.336308956146\n",
      "135000 1.5344736576080322 1.5408353197574616 4292.2148542404175\n",
      "136000 1.564109444618225 1.5425217261314392 4324.415416955948\n",
      "137000 1.5289180278778076 1.5424918718338012 4356.630433082581\n",
      "138000 1.54201340675354 1.5439019293785095 4389.0804669857025\n",
      "139000 1.499992847442627 1.541788732290268 4421.360185146332\n",
      "140000 1.5061495304107666 1.5419802222251893 4453.651757478714\n",
      "141000 1.5456483364105225 1.5418305038213729 4486.042303562164\n",
      "142000 1.5291070938110352 1.5411445673704147 4518.317920684814\n",
      "143000 1.5633363723754883 1.5417685803174972 4550.003101110458\n",
      "144000 1.5885114669799805 1.5420352325439453 4582.410375595093\n",
      "145000 1.5281391143798828 1.5428309652805328 4614.456797122955\n",
      "146000 1.5302691459655762 1.5413103429079056 4646.532478809357\n",
      "147000 1.5485758781433105 1.5425385893583299 4678.799997329712\n",
      "148000 1.5526148080825806 1.5415184246301652 4711.021918773651\n",
      "149000 1.5488351583480835 1.5424858700037003 4743.066588878632\n",
      "150000 1.5303585529327393 1.5413929196596146 4775.511870384216\n",
      "151000 1.5477192401885986 1.5418016245365143 4807.82065820694\n",
      "152000 1.5399649143218994 1.541245192885399 4840.215317249298\n",
      "153000 1.5075057744979858 1.540604168176651 4872.510721683502\n",
      "154000 1.5680274963378906 1.5423455272912978 4904.868705749512\n",
      "155000 1.5474843978881836 1.5404797426462173 4936.154401302338\n",
      "156000 1.5565975904464722 1.541211398601532 4967.683683395386\n",
      "157000 1.5590256452560425 1.5403576048612595 4999.952274799347\n",
      "158000 1.5662027597427368 1.5412578325271606 5032.150700569153\n",
      "159000 1.5173730850219727 1.541631921172142 5064.563841581345\n",
      "160000 1.5563104152679443 1.5401021625995637 5096.872895479202\n",
      "161000 1.5419135093688965 1.5410789510011673 5129.296491146088\n",
      "162000 1.5274724960327148 1.540268232703209 5161.531881570816\n",
      "163000 1.514789342880249 1.5414868352413178 5193.983562469482\n",
      "164000 1.5449392795562744 1.5395797225236894 5226.393709897995\n",
      "165000 1.5129817724227905 1.5415793080329896 5258.867535352707\n",
      "166000 1.5170106887817383 1.5396404601335525 5291.292454481125\n",
      "167000 1.5496898889541626 1.5395333235263824 5323.746461391449\n",
      "168000 1.5502145290374756 1.5408243794441223 5356.1471991539\n",
      "169000 1.5499153137207031 1.5403084791898727 5388.567482948303\n",
      "170000 1.563307285308838 1.5408753780126572 5421.011917352676\n",
      "171000 1.51084566116333 1.5387291040420532 5453.366232395172\n",
      "172000 1.5452826023101807 1.5406377115249634 5485.743505001068\n",
      "173000 1.5426414012908936 1.5402485643625259 5516.969707727432\n",
      "174000 1.571664571762085 1.53981749689579 5548.479329824448\n",
      "175000 1.5328333377838135 1.540102060675621 5580.604469537735\n",
      "176000 1.5408217906951904 1.540708454966545 5612.886160373688\n",
      "177000 1.5177228450775146 1.5394634528160096 5645.070594549179\n",
      "178000 1.5518227815628052 1.5397650388479234 5677.094848155975\n",
      "179000 1.5667165517807007 1.540208827853203 5709.479143619537\n",
      "180000 1.521449089050293 1.5398257540464402 5741.781102657318\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()(output[i], target_tensor[i])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=512\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 4, 8, 8, 4, 11, 7, 4, 8, 13, 13, 4, 4, 8, 4] False\n",
      "[8, 6, 6, 11, 4, 15, 13, 15, 15, 13, 8, 7, 4, 4, 16, 4] False\n",
      "[8, 7, 8, 11, 8, 11, 8, 7, 7, 4, 11, 8, 11, 6, 8, 8] False\n",
      "[8, 6, 7, 13, 13, 4, 4, 4, 8, 4, 13, 4, 6, 8, 4, 8] False\n",
      "[8, 8, 7, 8, 4, 8, 4, 4, 16, 18, 4, 1, 13, 10, 8, 4] False\n",
      "[8, 4, 7, 13, 7, 11, 6, 8, 4, 13, 11, 11, 7, 16, 8, 4] False\n",
      "[8, 6, 4, 15, 21, 11, 11, 6, 16, 6, 16, 4, 4, 4, 13, 13] False\n",
      "[8, 11, 10, 8, 7, 11, 4, 8, 8, 6, 4, 4, 7, 4, 13, 13] False\n",
      "[8, 8, 9, 11, 7, 8, 13, 8, 7, 4, 8, 4, 6, 16, 4, 18] False\n",
      "[8, 6, 11, 13, 4, 8, 4, 7, 6, 7, 4, 4, 6, 7, 4, 6] False\n",
      "[8, 4, 9, 7, 8, 8, 13, 8, 13, 4, 11, 7, 7, 11, 11, 8] False\n",
      "[8, 4, 8, 6, 11, 11, 8, 7, 7, 8, 11, 4, 4, 6, 11, 4] False\n",
      "[8, 9, 11, 10, 8, 7, 8, 8, 7, 8, 11, 13, 9, 8, 7, 7] False\n",
      "[8, 8, 8, 4, 4, 7, 7, 13, 7, 11, 6, 4, 4, 10, 8, 7] False\n",
      "[8, 4, 11, 13, 2, 7, 6, 11, 8, 9, 8, 7, 13, 9, 8, 4] False\n",
      "[8, 4, 8, 9, 8, 8, 7, 4, 4, 6, 4, 7, 7, 11, 7, 8] False\n",
      "[8, 4, 6, 11, 8, 4, 9, 4, 8, 13, 7, 8, 7, 7, 11, 8] False\n",
      "[8, 4, 8, 11, 8, 11, 4, 15, 10, 11, 6, 4, 4, 4, 13, 8] False\n",
      "[8, 6, 4, 15, 21, 11, 11, 6, 8, 14, 16, 15, 4, 4, 8, 4] False\n",
      "[8, 13, 4, 7, 4, 6, 6, 8, 4, 8, 4, 13, 11, 4, 16, 4] False\n",
      "[8, 11, 8, 7, 11, 4, 10, 8, 3, 15, 11, 15, 7, 14, 4, 13] False\n",
      "[8, 8, 11, 9, 11, 8, 7, 4, 4, 8, 13, 10, 6, 7, 7, 11] False\n",
      "[8, 6, 7, 13, 13, 4, 4, 13, 8, 7, 7, 7, 11, 8, 8, 13] False\n",
      "[8, 10, 11, 13, 7, 13, 4, 11, 7, 11, 7, 8, 4, 8, 8, 10] False\n",
      "[8, 8, 9, 4, 11, 11, 8, 7, 7, 11, 8, 8, 13, 13, 11, 8] False\n",
      "[8, 9, 4, 11, 10, 1, 4, 4, 8, 7, 15, 16, 7, 8, 4, 9] False\n",
      "[8, 4, 8, 6, 11, 11, 8, 7, 7, 11, 8, 8, 13, 13, 11, 13] False\n",
      "[8, 11, 7, 8, 11, 8, 6, 13, 4, 4, 6, 4, 4, 4, 6, 8] False\n",
      "[8, 4, 6, 4, 6, 4, 4, 9, 10, 1, 6, 9, 4, 4, 8, 8] False\n",
      "[8, 8, 7, 11, 11, 8, 8, 6, 13, 4, 4, 4, 13, 8, 8, 13] False\n",
      "[8, 11, 10, 11, 13, 8, 7, 8, 13, 13, 4, 13, 11, 8, 10, 6] False\n",
      "[8, 8, 6, 11, 11, 8, 6, 4, 4, 11, 4, 8, 6, 10, 16, 14] False\n",
      "[8, 4, 7, 7, 13, 6, 11, 8, 6, 4, 4, 7, 10, 7, 7, 11] False\n",
      "[8, 4, 6, 4, 4, 4, 7, 13, 8, 4, 11, 8, 8, 4, 13, 13] False\n",
      "[8, 4, 6, 7, 11, 13, 8, 6, 9, 10, 4, 4, 13, 8, 7, 8] False\n",
      "[8, 11, 7, 9, 8, 8, 4, 11, 8, 4, 11, 8, 8, 7, 8, 4] False\n",
      "[8, 7, 11, 7, 10, 1, 8, 11, 6, 8, 13, 13, 13, 11, 8, 7] False\n",
      "[8, 11, 7, 9, 8, 8, 7, 8, 4, 4, 8, 8, 2, 13, 9, 6] False\n",
      "[8, 8, 9, 4, 8, 8, 4, 4, 8, 11, 8, 8, 7, 8, 8, 7] False\n",
      "[8, 8, 8, 4, 15, 7, 13, 4, 7, 6, 4, 10, 7, 7, 11, 11] False\n",
      "[8, 11, 8, 6, 11, 8, 6, 4, 4, 7, 11, 11, 10, 1, 4, 7] False\n",
      "[8, 11, 13, 4, 4, 4, 6, 6, 4, 4, 9, 10, 8, 4, 9, 8] False\n",
      "[8, 6, 11, 8, 4, 6, 4, 7, 4, 8, 16, 4, 15, 16, 6, 18] False\n",
      "[8, 13, 7, 6, 7, 7, 11, 6, 4, 4, 8, 8, 13, 8, 13, 13] False\n",
      "[8, 8, 7, 9, 11, 6, 16, 4, 6, 6, 4, 4, 9, 4, 8, 11] False\n",
      "[8, 11, 10, 11, 13, 11, 7, 7, 11, 7, 8, 4, 8, 8, 13, 4] False\n",
      "[8, 13, 4, 13, 4, 7, 13, 8, 7, 8, 11, 13, 11, 8, 4, 9] False\n",
      "[8, 8, 7, 8, 8, 11, 8, 8, 4, 11, 4, 7, 7, 11, 8, 11] False\n",
      "[8, 11, 13, 4, 4, 12, 6, 6, 7, 6, 4, 4, 4, 8, 4, 7] False\n",
      "[8, 4, 8, 11, 8, 4, 11, 8, 4, 13, 13, 11, 8, 3, 7, 1] False\n",
      "[8, 8, 7, 11, 11, 4, 8, 16, 16, 4, 7, 8, 13, 8, 13, 11] False\n",
      "[8, 13, 7, 13, 13, 4, 11, 10, 8, 4, 4, 7, 7, 11, 8, 10] False\n",
      "[8, 10, 11, 13, 4, 13, 4, 9, 8, 16, 6, 11, 6, 4, 4, 7] False\n",
      "[8, 11, 13, 7, 13, 4, 11, 7, 11, 7, 4, 7, 7, 5, 7, 6] False\n",
      "[8, 8, 8, 11, 11, 8, 7, 7, 7, 11, 8, 4, 4, 4, 11, 11] False\n",
      "[8, 13, 8, 7, 8, 11, 8, 9, 7, 8, 7, 4, 8, 8, 13, 4] False\n",
      "[8, 8, 11, 7, 8, 11, 8, 7, 7, 8, 4, 4, 8, 8, 8, 13] False\n",
      "[8, 4, 8, 8, 11, 7, 11, 4, 4, 7, 7, 8, 8, 13, 13, 11] False\n",
      "[8, 11, 10, 11, 13, 11, 7, 3, 8, 7, 7, 11, 8, 4, 8, 13] False\n",
      "[8, 4, 6, 8, 4, 11, 8, 8, 11, 8, 4, 7, 8, 13, 4, 11] False\n",
      "[8, 11, 13, 7, 4, 11, 11, 15, 4, 13, 4, 4, 8, 4, 6, 6] False\n",
      "[8, 6, 7, 11, 6, 4, 4, 7, 4, 4, 13, 7, 7, 11, 8, 8] False\n",
      "[8, 4, 6, 7, 13, 4, 13, 4, 9, 8, 8, 13, 10, 6, 9, 4] False\n",
      "[8, 4, 4, 4, 7, 13, 11, 8, 7, 7, 7, 11, 8, 11, 6, 4] False\n",
      "[8, 4, 13, 10, 8, 4, 4, 4, 4, 13, 4, 13, 8, 7, 7, 2] False\n",
      "[8, 8, 4, 9, 11, 11, 8, 7, 7, 11, 8, 8, 13, 13, 11, 13] False\n",
      "[8, 4, 13, 4, 8, 7, 11, 11, 8, 6, 4, 4, 7, 6, 4, 8] False\n",
      "[8, 6, 7, 6, 4, 15, 8, 4, 6, 4, 4, 6, 4, 8, 6, 9] False\n",
      "[8, 4, 6, 7, 11, 13, 8, 4, 4, 13, 6, 6, 7, 4, 8, 11] False\n",
      "[8, 11, 13, 7, 13, 4, 11, 4, 7, 13, 4, 13, 8, 7, 8, 4] False\n",
      "[8, 13, 4, 13, 4, 13, 1, 8, 8, 11, 4, 9, 15, 8, 8, 13] False\n",
      "[8, 7, 9, 8, 8, 7, 8, 11, 8, 4, 4, 7, 11, 8, 10, 11] False\n",
      "[8, 7, 8, 11, 8, 11, 8, 8, 4, 11, 4, 8, 4, 8, 16, 4] False\n",
      "[8, 4, 8, 6, 8, 13, 11, 7, 13, 11, 11, 7, 8, 8, 6, 13] False\n",
      "[8, 4, 6, 6, 4, 7, 9, 7, 11, 6, 8, 13, 8, 4, 7, 13] False\n",
      "[8, 6, 6, 11, 4, 7, 6, 4, 8, 8, 4, 4, 13, 4, 4, 7] False\n",
      "[8, 11, 10, 7, 7, 4, 8, 11, 6, 8, 8, 10, 11, 4, 15, 4] False\n",
      "[8, 4, 8, 6, 8, 11, 7, 8, 8, 4, 11, 11, 4, 8, 13, 13] False\n",
      "[8, 4, 8, 6, 11, 7, 8, 8, 4, 11, 8, 8, 4, 11, 4, 15] False\n",
      "[8, 8, 7, 8, 9, 11, 11, 6, 4, 8, 13, 8, 10, 4, 4, 4] False\n",
      "[8, 4, 11, 8, 8, 7, 8, 11, 8, 7, 8, 11, 4, 11, 4, 8] False\n",
      "[8, 4, 8, 6, 8, 13, 11, 7, 7, 11, 11, 8, 6, 8, 13, 9] False\n",
      "[8, 8, 6, 11, 11, 8, 7, 7, 8, 4, 11, 11, 10, 4, 1, 4] False\n",
      "[8, 6, 10, 11, 13, 4, 16, 11, 9, 4, 13, 17, 6, 6, 9, 4] False\n",
      "[8, 8, 7, 11, 11, 4, 8, 13, 11, 4, 7, 8, 8, 13, 4, 8] False\n",
      "[8, 8, 7, 9, 11, 11, 6, 4, 13, 7, 6, 7, 7, 8, 6, 13] False\n",
      "[8, 8, 4, 8, 8, 7, 13, 8, 8, 13, 8, 13, 11, 11, 8, 4] False\n",
      "[8, 8, 11, 7, 11, 8, 8, 7, 7, 4, 18, 6, 11, 13, 8, 6] False\n",
      "[8, 9, 4, 11, 10, 1, 4, 4, 8, 8, 4, 8, 4, 8, 4, 18] False\n",
      "[8, 8, 9, 4, 11, 11, 8, 7, 7, 11, 8, 11, 4, 4, 7, 4] False\n",
      "[8, 4, 7, 8, 6, 8, 11, 9, 13, 9, 4, 4, 6, 4, 7, 7] False\n",
      "[8, 8, 11, 7, 9, 6, 7, 7, 8, 13, 6, 11, 13, 8, 8, 13] False\n",
      "[8, 4, 8, 4, 7, 7, 7, 8, 11, 4, 8, 8, 13, 10, 6, 7] False\n",
      "[8, 4, 6, 7, 11, 13, 8, 6, 4, 7, 7, 11, 4, 7, 4, 4] False\n",
      "[8, 8, 9, 11, 7, 8, 4, 4, 6, 4, 6, 4, 8, 6, 4, 4] False\n",
      "[8, 10, 11, 13, 4, 13, 11, 8, 8, 13, 13, 11, 8, 7, 4, 11] False\n",
      "[8, 4, 8, 8, 9, 8, 8, 13, 4, 8, 4, 8, 8, 13, 8, 13] False\n",
      "[8, 6, 11, 8, 6, 4, 4, 7, 4, 13, 13, 1, 1, 13, 15, 4] False\n",
      "[8, 4, 8, 11, 8, 4, 11, 4, 7, 7, 11, 8, 8, 6, 13, 4] False\n",
      "[8, 8, 9, 11, 7, 8, 8, 4, 11, 4, 8, 8, 7, 8, 4, 11] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.00331897105889815 0.01839365628065829 0.07472008187209314\n",
      "1 0.004596991142228685 0.018029837486857197 0.1044578440460679\n",
      "2 0.004676172671514662 0.021114776011248414 0.07923870089294857\n",
      "3 0.0034256349115668583 0.012295046672251796 0.08066860095517281\n",
      "4 0.002292086177673244 0.019630802118918097 0.11687616674596313\n",
      "5 0.6030775602216355 0.6086887416543778 0.05757907361753542\n",
      "6 0.0029073774355222884 0.02254378917842399 0.13168714766592196\n",
      "7 0.004902231558120146 0.02330229300233108 0.11659222604628197\n",
      "8 0.0027877655614399043 0.020914968321639974 0.13171893537316015\n",
      "9 0.001309472977130648 0.016391785155281086 0.10040216498871293\n",
      "10 0.0019133482992559157 0.018888326292452802 0.09630716603664415\n",
      "11 0.00345966487733859 0.018717076083040178 0.12457363144658733\n",
      "12 0.004949806110838631 0.026374415010008923 0.11117022403947054\n",
      "13 0.0025253424561793325 0.01866657701066657 0.10925810433980884\n",
      "14 0.0018676438761267775 0.018667157238943403 0.09810329858227185\n",
      "15 0.003689724253339581 0.017176353159077305 0.10852608083286995\n",
      "16 0.0033546761042763653 0.015379043664593268 0.07248808748997312\n",
      "17 0.0030376334259925616 0.022771975807745065 0.13437019961493415\n",
      "18 0.002686397401060076 0.017603772510092295 0.0954821428059297\n",
      "19 0.0030027076371513016 0.016417353945986077 0.10723082427030309\n",
      "20 0.0034879488304224663 0.020470832677646142 0.11592725995467462\n",
      "21 0.001891235785378433 0.018098969395798317 0.1045958764321702\n",
      "22 0.0041614588459151095 0.021066524477204453 0.11257861715976666\n",
      "23 0.0012654401015213812 0.01945543705517252 0.12424054738429163\n",
      "24 0.002461412033987415 0.018345645477665477 0.09959494052704962\n",
      "25 0.003284761077243783 0.02001694866953931 0.12781599511109465\n",
      "26 0.0038071037562850615 0.019390805969172996 0.10059950576968502\n",
      "27 0.003962940706142709 0.01492887575038456 0.07374913362428347\n",
      "28 0.003236379036802151 0.01498963740713349 0.11087972072234482\n",
      "29 0.00391302893229961 0.020915942326466423 0.13608221120685557\n",
      "30 0.0029805138513216214 0.01328072986939189 0.08365890246185353\n",
      "31 0.0026588476613705458 0.013183760285758284 0.048576703138912034\n",
      "32 0.004066839498033738 0.021888721185894175 0.11380168945138514\n",
      "33 0.004841130227559888 0.023703148716830144 0.09272621193373792\n",
      "34 0.004709503992955609 0.024493598150132682 0.11842876438642617\n",
      "35 0.0046332942360779155 0.017400723789727523 0.09830775275815463\n",
      "36 0.0017613681558980568 0.016520329394173246 0.11570113964950382\n",
      "37 0.0043540735668854465 0.022152466046535993 0.08675638484060316\n",
      "38 0.003586602032628803 0.015590404965329676 0.07536711671421237\n",
      "39 0.0032538010110801574 0.01881490973867289 0.11538015605330736\n",
      "40 0.003382410965343665 0.0224528167531094 0.10979963031290157\n",
      "41 0.0032286946864494145 0.018079264069390377 0.09442409822678025\n",
      "42 0.004684294066205356 0.022265408598563848 0.08655352650411686\n",
      "43 0.0048937987337930415 0.02149846413689554 0.09916393930088183\n",
      "44 0.004660000189865094 0.01920057511241117 0.10712410260517721\n",
      "45 0.0022384864467221 0.017403783936339186 0.10487610735339269\n",
      "46 0.004521664503068218 0.0228768999761022 0.10246886811888992\n",
      "47 0.004280647626560132 0.021550298567312637 0.09479551020713162\n",
      "48 0.0027588570467722297 0.01757612037731405 0.10996629454209801\n",
      "49 0.0026623940208437592 0.0150208392986065 0.08340506742666222\n",
      "50 0.0031488327451447357 0.02048804708039185 0.09690837691579639\n",
      "51 0.003030434502972716 0.02466423810367295 0.14152632781861457\n",
      "52 0.003840826722224964 0.018397065274692787 0.10326973177689258\n",
      "53 0.002217241489815855 0.01848209754342023 0.08977938769078872\n",
      "54 0.002755054686869575 0.017234743201181346 0.11816410992835111\n",
      "55 0.0037019935589745912 0.022446844454260054 0.11946854238070996\n",
      "56 0.0029465955875125328 0.019633372762068575 0.10374978941748118\n",
      "57 0.0029528117841843525 0.013163512393511614 0.08540873620109701\n",
      "58 0.004483122156843801 0.02237734342183392 0.1187480700906011\n",
      "59 0.002818555454622571 0.015454815915229098 0.08317932018431323\n",
      "60 0.004851166056774204 0.02394198394260234 0.13133637980123802\n",
      "61 0.0021606812877195025 0.012145891927257013 0.07969391692605735\n",
      "62 0.0021974659360137466 0.014441760456133216 0.09285444601422019\n",
      "63 0.0021883531197556616 0.023624775360038035 0.1058703981834383\n",
      "64 0.003294807126406542 0.012941317444477015 0.0530951870859717\n",
      "65 0.0012319931069008894 0.018919360359693466 0.09112372983565287\n",
      "66 0.003462635701553047 0.019731271528275876 0.09584507860523925\n",
      "67 0.0033885943464369854 0.019750343657698063 0.1011552758885806\n",
      "68 0.002467033742775274 0.016599296436901934 0.11312474219382408\n",
      "69 0.0037238792463322005 0.01742269033028266 0.0995903523517284\n",
      "70 0.0025842720165300944 0.02061842852605271 0.09740076842940071\n",
      "71 0.00186315210260685 0.017948580322863873 0.09154887704486865\n",
      "72 0.0026757528093580667 0.01799704786501712 0.0967886364334331\n",
      "73 0.001673597832194845 0.020097584088468762 0.11372071373015402\n",
      "74 0.004318236064403869 0.02700808483416441 0.12126724278442252\n",
      "75 0.0035187460906369655 0.024186891647962835 0.12754731954463233\n",
      "76 0.0017199657455067366 0.019233132679462007 0.08195895589913804\n",
      "77 0.00468787018854876 0.02180568204488606 0.11624458892835104\n",
      "78 0.0042982233114285465 0.02132705946860447 0.10964652179077355\n",
      "79 0.0024112407335592397 0.019641277501324446 0.11084008841094647\n",
      "80 0.0038564155805949052 0.027986581907109287 0.13168567910513854\n",
      "81 0.0038514708260439187 0.020937879316707618 0.09811669237392814\n",
      "82 0.0019137628776103388 0.013646133740341897 0.12550448222673255\n",
      "83 0.0023234358007535576 0.01787699016247776 0.0841413045021853\n",
      "84 0.00320648381419804 0.015460630700287553 0.09732842202676434\n",
      "85 0.0019639729596954616 0.019064317456970358 0.08821869905363444\n",
      "86 0.0026739454226584677 0.021303897727818443 0.11804685024410205\n",
      "87 0.004128888603588695 0.01925658615200464 0.10902768933147063\n",
      "88 0.0032083032774477696 0.020696283637990528 0.10058199621836286\n",
      "89 0.003370041908959 0.01804015220061174 0.1030209457931254\n",
      "90 0.0021586880847454548 0.020295994268989995 0.09181386270221764\n",
      "91 0.0030789360306387264 0.01709154715898375 0.10291800117692786\n",
      "92 0.004238117349875435 0.015478367099292447 0.06665824183808149\n",
      "93 0.0030184746196394293 0.020403762817854887 0.08547704835342049\n",
      "94 0.002671516268000839 0.021409050625333492 0.08897860165896244\n",
      "95 0.0020097764552139737 0.01591305834711665 0.1165653727487294\n",
      "96 0.003712174862538817 0.02246343204845263 0.09007424928151929\n",
      "97 0.004743734134874665 0.021925117367064553 0.10973655350354042\n",
      "98 0.003633602361538729 0.0233751478089962 0.11335309075855056\n",
      "99 0.0029191208259663802 0.019082200036646887 0.10332815607073004\n",
      "100 0.002726589339830939 0.019904228662521145 0.13644902826706193\n",
      "101 0.0030903400628545666 0.020510780806692837 0.10497626632682833\n",
      "102 0.004185247028333752 0.023172198809695545 0.0883114532223391\n",
      "103 0.001932831056428959 0.023133120749091768 0.1254071216955226\n",
      "104 0.0042676487951110245 0.01840466069033889 0.10339494352411865\n",
      "105 0.0035193228944173736 0.021955002525516007 0.08951328459950571\n",
      "106 0.0017893767673130044 0.018125194543230524 0.11196722193861842\n",
      "107 0.004488472620874475 0.028983135466171584 0.12121201150865327\n",
      "108 0.0036196418834301683 0.022132607632670935 0.10238493726996177\n",
      "109 0.002372463176722935 0.015600623064040952 0.10692011873465342\n",
      "110 0.002528124607436767 0.015389569886313362 0.10273494980039027\n",
      "111 0.00433653153050439 0.014055944523876734 0.07294321012996705\n",
      "112 0.003191430143339221 0.017052055798944768 0.12801414246426468\n",
      "113 0.004670564061489493 0.022817207319100363 0.12490959324713456\n",
      "114 0.0023926403850600938 0.02282422261280069 0.13410438674618658\n",
      "115 0.004057256270512347 0.023014197789888086 0.10723708088353959\n",
      "116 0.0023409907944406585 0.019589764102428006 0.11299130349813442\n",
      "117 0.004102542760510346 0.020138735551609294 0.0904401113427043\n",
      "118 0.0035937316105358157 0.021570194900268413 0.08086313011347003\n",
      "119 0.003882194669160969 0.021493034542091276 0.13405886719644697\n",
      "120 0.002599628758403701 0.014806146508386601 0.10493995637094776\n",
      "121 0.002229383959495042 0.021174398458077656 0.09417623501435264\n",
      "122 0.0030474654093225356 0.014381382382620937 0.07117710369734725\n",
      "123 0.0019995154050578227 0.01696362125228197 0.07886104204209965\n",
      "124 0.0018896919767775638 0.009695467071809783 0.0640110727553165\n",
      "125 0.003993098881688783 0.022620689131975576 0.10374253291763846\n",
      "126 0.002462173566442209 0.018504535535632194 0.11612658238218171\n",
      "127 0.004693307046954831 0.01726203625010663 0.08370707355366042\n",
      "128 0.0031975367825972088 0.022829225039033445 0.125381084257009\n",
      "129 0.003363742140856559 0.014810790284819023 0.08831935162680699\n",
      "130 0.003905687479715615 0.02246055611700784 0.09743234301855522\n",
      "131 0.002829444944359886 0.02012146045886208 0.12001122082439221\n",
      "132 0.0019399909089710837 0.017682578428496472 0.09433867105377475\n",
      "133 0.0023928146002778067 0.018633721756736915 0.10806339570793069\n",
      "134 0.0045602032112131655 0.02950841418549666 0.10809508047471651\n",
      "135 0.004562604132583494 0.019054481639338802 0.09138418673853238\n",
      "136 0.002555606827975007 0.018511641496764153 0.12576820969862032\n",
      "137 0.0019481554706749721 0.015059388560914436 0.1058315850330659\n",
      "138 0.0032338057161595962 0.01856105896271562 0.1038077222692139\n",
      "139 0.003371661547191448 0.018889951734492462 0.09761114521727667\n",
      "140 0.00439460893201146 0.024587407747420492 0.10366430878422475\n",
      "141 0.0028819129186677607 0.01742801436345426 0.10541509648562777\n",
      "142 0.0037585561389684536 0.020310683098183083 0.10768666136407357\n",
      "143 0.0035193043859253516 0.022635444409170705 0.12355259174379261\n",
      "144 0.0024531766416397488 0.015152027078684408 0.09934759166101921\n",
      "145 0.0024935927112467553 0.014968635747342125 0.11697094277184336\n",
      "146 0.0021843230772847807 0.01829483597140054 0.09786707258495361\n",
      "147 0.0019420742253348092 0.017487433693576793 0.1291573385607892\n",
      "148 0.003777675400918741 0.025344622334258778 0.09267436120691327\n",
      "149 0.0015334497987883888 0.017492144323187547 0.08416193596546413\n",
      "150 0.0036829745113230792 0.018103255323664906 0.10090319936513754\n",
      "151 0.003319454164307855 0.02217150561030789 0.09359174073075491\n",
      "152 0.004207878235319213 0.023363629455409006 0.1154515574291881\n",
      "153 0.0020914479870782574 0.01796565886018336 0.11119090138870301\n",
      "154 0.0031095163021421686 0.018640183559865624 0.1173369884863675\n",
      "155 0.0027672494150056902 0.014695100469711085 0.08024884269863652\n",
      "156 0.0038208887553312395 0.021098950454330134 0.1079788582164957\n",
      "157 0.0029624986675082362 0.02147451079530944 0.10709463182255294\n",
      "158 0.003670967041705872 0.019716975800173907 0.08625872891949882\n",
      "159 0.0019128892371480678 0.01749193107822098 0.11407814358284925\n",
      "160 0.0043225803336735275 0.02055967168842803 0.11188950993303441\n",
      "161 0.0042552331906999825 0.022725099704483123 0.09328802400659242\n",
      "162 0.0024729684810076745 0.018246185222264552 0.11068016520719873\n",
      "163 0.003469227214145684 0.01669572449724207 0.14551614931196288\n",
      "164 0.0025781872317418124 0.021697264741863183 0.12418004813112214\n",
      "165 0.0035560145862758527 0.02563266745727703 0.14482742350821665\n",
      "166 0.0010063612566685752 0.01967308441188798 0.09443705406853278\n",
      "167 0.003394275085372025 0.02186076779812339 0.1298051334839898\n",
      "168 0.0040834506000698625 0.021585411697568092 0.09205411510762278\n",
      "169 0.0035390255557459244 0.021152126709777133 0.09228037217677659\n",
      "170 0.0021452297930669864 0.019767671533945534 0.11675217208073223\n",
      "171 0.002417775878393742 0.018978970895464315 0.09019028672430009\n",
      "172 0.002265391803797089 0.015911397462193146 0.09989219266525937\n",
      "173 0.0043331083502388176 0.024250122613163713 0.10997486929701225\n",
      "174 0.001362185274883714 0.01657296156798492 0.11192855210098218\n",
      "175 0.003680868521941507 0.01391358503080623 0.07002736475542573\n",
      "176 0.0023110122239753637 0.010619422135179192 0.07102572765826766\n",
      "177 0.0041506772323989674 0.02203021481774927 0.09729235485819152\n",
      "178 0.003846335106192913 0.022909832033083513 0.1335989096111182\n",
      "179 0.0015050299099237667 0.017786845417829943 0.10767524598667286\n",
      "180 0.0024158183506620247 0.020736453835049823 0.09598554557372609\n",
      "181 0.003829139708870929 0.017602658496958523 0.11114150104699895\n",
      "182 0.002786960459360207 0.01855671206952183 0.0865505931993078\n",
      "183 0.0022416187744820916 0.01688478326014112 0.11318392684935047\n",
      "184 0.002942495606820087 0.016415488370748847 0.09994235777600763\n",
      "185 0.003938613077199997 0.026729328061018624 0.10903179600453136\n",
      "186 0.002272529840605668 0.01571080444710155 0.10078379057203365\n",
      "187 0.0032871121926027905 0.020594925388508135 0.10514934008643567\n",
      "188 0.0026711501283524144 0.01260193188428408 0.10397346381683316\n",
      "189 0.003600178252117371 0.016641895855257577 0.09283898720628317\n",
      "190 0.004035626293573135 0.02226661945562592 0.1183478841887374\n",
      "191 0.003646833922295625 0.019232115488188897 0.09103726986608493\n",
      "192 0.0025502204688909627 0.01980169847048667 0.10436961534644301\n",
      "193 0.00184228306946664 0.01950082336465805 0.10865198679291677\n",
      "194 0.004830308035883886 0.0197482242237583 0.12512883748060416\n",
      "195 0.0030486659146011586 0.020198826939020573 0.10579321849500764\n",
      "196 0.0016451657911653438 0.01894061489065671 0.11587499517822018\n",
      "197 0.003729867544058914 0.021612867945374113 0.09078150501438273\n",
      "198 0.00498669057159782 0.033738801108373746 0.11698092421416616\n",
      "199 0.004224321318750407 0.022360347206667033 0.10127987651698653\n",
      "200 0.0036146282280976137 0.022047155646366008 0.12458732113692614\n",
      "201 0.003752764297955846 0.02048112115648008 0.09117743827893805\n",
      "202 0.0019002590612849793 0.014333912024856864 0.10201630489033753\n",
      "203 0.003537664781691977 0.017595585802343473 0.08936073923267855\n",
      "204 0.002832863953045376 0.020493825611811792 0.11380692624356359\n",
      "205 0.0036006679228396394 0.019978336971788378 0.11132083254957986\n",
      "206 0.002617981613602407 0.02064803645922405 0.13365620293035996\n",
      "207 0.003760988234933575 0.017932634664178833 0.10000857333214266\n",
      "208 0.004386729623335518 0.023750033187617745 0.11516971244207676\n",
      "209 0.0026314462929360764 0.02079990066386766 0.09625449121206564\n",
      "210 0.0016686201255297086 0.023163171362691838 0.12666796362204777\n",
      "211 0.0029995831724333273 0.017686650186859675 0.11325858003394179\n",
      "212 0.004029511978789048 0.02604325467566397 0.12442052269248247\n",
      "213 0.0036203075011886224 0.01973938599069213 0.10645950311270122\n",
      "214 0.004201295955233334 0.024685759207838616 0.12622036963565253\n",
      "215 0.003170007103979503 0.02526650837052133 0.13932827242993515\n",
      "216 0.0030413972424994627 0.018999016900260627 0.10288346757325204\n",
      "217 0.0041044972576728105 0.024330111519215908 0.11182453466291417\n",
      "218 0.0038207856048515705 0.025220618103411578 0.13580535273811786\n",
      "219 0.004894914223505998 0.023435021138117178 0.12214600467127697\n",
      "220 0.002416761170901306 0.0183760257329632 0.09901146815207358\n",
      "221 0.0042237654493890355 0.022024585648727897 0.10780076659882537\n",
      "222 0.004799888700294704 0.019230452192637505 0.10590708260505116\n",
      "223 0.0026396445477428585 0.017960830509768082 0.09078012895706937\n",
      "224 0.0047868957410878675 0.023385482169603962 0.12624407601111884\n",
      "225 0.002120218382405917 0.01967723244445488 0.11940824908734357\n",
      "226 0.003380130991799614 0.020284641656417468 0.1220878627931784\n",
      "227 0.004591817101607996 0.019493515426349924 0.08369774624190741\n",
      "228 0.0018124049106507238 0.01328209027737415 0.10822644420728034\n",
      "229 0.001944635778612363 0.023277740600472455 0.11908545267155604\n",
      "230 0.003353227839657499 0.019803050987662794 0.0960900789570942\n",
      "231 0.004719146281236649 0.024183312382156555 0.10960281705352386\n",
      "232 0.0026034180628758804 0.013418504353638788 0.07484627154291759\n",
      "233 0.0040466954185104575 0.025656641072658012 0.09766678088226814\n",
      "234 0.0030005725362096396 0.020997239674221485 0.0781308070521457\n",
      "235 0.0023534061268162204 0.01696965740056268 0.11012322518094589\n",
      "236 0.004676193761761493 0.023126505975560825 0.1271613990179864\n",
      "237 0.002728178844049582 0.020353860504119956 0.11210701974451612\n",
      "238 0.004672211610866669 0.02470462547261566 0.1338275893878222\n",
      "239 0.0035100318751862185 0.01416557847937705 0.0657202592529353\n",
      "240 0.0028220631180433838 0.015031852313922147 0.0749720673502843\n",
      "241 0.0020045940808885044 0.01706388689308061 0.08749594704966243\n",
      "242 0.0028789297215171938 0.01954568317315338 0.1110546993341371\n",
      "243 0.0020265235498440866 0.01790428176214428 0.09734294348864789\n",
      "244 0.003154427256246193 0.02109087950074824 0.13915193502561524\n",
      "245 0.003991748287997643 0.020314693225076577 0.10844112940510754\n",
      "246 0.002437355432874254 0.01701300923170794 0.09632027372945566\n",
      "247 0.0038301585557294323 0.024866155315311562 0.09551230994890769\n",
      "248 0.002248532533318989 0.02147881209369759 0.10345019743979018\n",
      "249 0.004327840056757959 0.021132118436909778 0.12569475023019597\n",
      "250 0.004057439297048451 0.02303997567002425 0.08890318403780387\n",
      "251 0.004224914337960935 0.022364137005190438 0.0942525716828862\n",
      "252 0.003757881539844468 0.016828024295479877 0.09258250356782127\n",
      "253 0.001679364227791729 0.01990617408538716 0.12234844820976554\n",
      "254 0.004282825143840121 0.017581286693028036 0.09851458011439866\n",
      "255 0.003219725124687085 0.01881543751578651 0.08593868095305285\n",
      "256 0.002323967126768453 0.016132920576734964 0.10673490284945436\n",
      "257 0.002816584633364552 0.015453079490767926 0.09295207101273105\n",
      "258 0.002021056201194932 0.0119633503327119 0.10510960002502115\n",
      "259 0.0034713944604399406 0.017568522997686728 0.07756898072334065\n",
      "260 0.0031895999725703073 0.019042683864726392 0.09417419153941045\n",
      "261 0.004579682736013117 0.020681760840832647 0.10392028710128805\n",
      "262 0.002554899442364964 0.019734584032109988 0.11244148016873962\n",
      "263 0.0038589414451997203 0.019029748904821172 0.08273743262095773\n",
      "264 0.0017141869295119669 0.016357117709548598 0.0933631666067948\n",
      "265 0.001018340368215476 0.011714254518886713 0.10441239945088954\n",
      "266 0.0038230948825913606 0.022555138106560624 0.11598558449172533\n",
      "267 0.0022305916247935603 0.020357382858271225 0.11406976236701079\n",
      "268 0.0037833293640284747 0.02320174167740982 0.12163818082464295\n",
      "269 0.0028822200273458994 0.020562869197047324 0.09705180941532968\n",
      "270 0.0017540896177578168 0.018089015398848017 0.12603476903861405\n",
      "271 0.003058454044421363 0.02043992063592849 0.10119813141574703\n",
      "272 0.0018550002951816368 0.014989453827079738 0.11011813447216312\n",
      "273 0.0038002335701464076 0.02185258292587825 0.10883934777949117\n",
      "274 0.004339637002720463 0.02183268921199149 0.11568436448766384\n",
      "275 0.004641610507792821 0.027095502736046023 0.14278243693807455\n",
      "276 0.0024135033081292617 0.018517347613788808 0.10135629745872768\n",
      "277 0.002324795573947957 0.021245357176984246 0.12215244951904972\n",
      "278 0.0024603056121291237 0.017464069206053652 0.11134231093192146\n",
      "279 0.0028788286371369174 0.01570810325645539 0.10352672296889909\n",
      "280 0.004853913099149348 0.01850882967053183 0.0676617210505969\n",
      "281 0.0030324435532268585 0.020608341987367287 0.096939437513713\n",
      "282 0.002445319240427407 0.010859737033460014 0.07769309243043665\n",
      "283 0.0024957781137772824 0.01585775891589701 0.14049537740360896\n",
      "284 0.004055373695351885 0.023005224586920385 0.12005691768713259\n",
      "285 0.003966870513420465 0.018784666839623966 0.09914111963482779\n",
      "286 0.0036870526348084806 0.021927710110239168 0.11674038642650855\n",
      "287 0.0025000706510044024 0.012639963162443176 0.09513464728345372\n",
      "288 0.003685659586199562 0.02273858998557092 0.10081237553863097\n",
      "289 0.003741419091378151 0.019983881962989447 0.10962371405405452\n",
      "290 0.004025730516616729 0.021235390400350995 0.11250692334970579\n",
      "291 0.0036500534685908184 0.017431013186318737 0.08580930700515468\n",
      "292 0.004854779448325691 0.021153010238624828 0.10088764415700938\n",
      "293 0.003753682015472503 0.022587708151783032 0.11947669811249137\n",
      "294 0.003135622681743373 0.026478875264915262 0.12234058112417392\n",
      "295 0.00487640317890105 0.02077004444216192 0.08045469235779837\n",
      "296 0.003705295149428003 0.018045126810561765 0.10786313189594954\n",
      "297 0.003668297218662554 0.019892529455821092 0.10779786859372985\n",
      "298 0.002214533324114811 0.018287089593027984 0.11372921048774715\n",
      "299 0.002418550879044607 0.016194228486972502 0.09709835441181316\n",
      "300 0.0018810296533085533 0.01931184557631182 0.10090718106125536\n",
      "301 0.004262128598899085 0.025465566450522503 0.1333139756674097\n",
      "302 0.002513222706130163 0.016173236106749096 0.12576925649556647\n",
      "303 0.0038702501843500504 0.018429109269328094 0.09713733294178475\n",
      "304 0.002209912867615864 0.024235342129161317 0.1327473760604256\n",
      "305 0.003814359200883236 0.02173283830102657 0.09063518497677264\n",
      "306 0.0031965540959214295 0.015515369042488722 0.06877566651203527\n",
      "307 0.004596461869846506 0.023973809493687985 0.14568186913920883\n",
      "308 0.001856984589592824 0.016675363945761613 0.09733439735410016\n",
      "309 0.004711139258778477 0.020723177731772263 0.11200928161677653\n",
      "310 0.004187999654519665 0.021194993894686105 0.10617940443435972\n",
      "311 0.0020449102071940168 0.015089710043988435 0.10063716452758831\n",
      "312 0.003667243319694082 0.01964990094631709 0.11063288699956812\n",
      "313 0.004087219313439935 0.02485613476919664 0.12182770885607497\n",
      "314 0.0029122713042933216 0.014726625730569666 0.07147327425664059\n",
      "315 0.002534501747975514 0.011393315522446316 0.06965298352719652\n",
      "316 0.0039117283706866625 0.019188605467096973 0.1238323546223358\n",
      "317 0.004080073365697018 0.01625191220806784 0.0873080280043475\n",
      "318 0.004019781785070376 0.027071290321345722 0.11245685729135288\n",
      "319 0.0029987052753782557 0.020310620112182057 0.10934776196387926\n",
      "320 0.0036193405246466425 0.02467092062699318 0.1319767095812137\n",
      "321 0.004990215783306672 0.027613035635547647 0.12483166822677584\n",
      "322 0.003556198881578332 0.021186534801672215 0.13179414191800137\n",
      "323 0.004810702598816363 0.02161064855378179 0.08575074513562257\n",
      "324 0.0030027902904765013 0.015750208036208645 0.0995605718415686\n",
      "325 0.0035096103784332112 0.022585039956064177 0.11695787933030391\n",
      "326 0.0033390613237799568 0.018455343351402614 0.08174213971515695\n",
      "327 0.0020247668624101753 0.016882095511471802 0.10803033329059064\n",
      "328 0.0012244538006739082 0.014085829674789073 0.12154059079270577\n",
      "329 0.003932800556844097 0.017859232286679953 0.10610406502876706\n",
      "330 0.0026472646662870254 0.017327512638392904 0.12121155398243215\n",
      "331 0.0038445908615843227 0.012942037881487036 0.05566895576947499\n",
      "332 0.004481002321426343 0.02509082877715496 0.09685840919298014\n",
      "333 0.0027179018977912346 0.018482879893159523 0.11674711009886911\n",
      "334 0.003898886139459232 0.021263408277872296 0.10606884260512699\n",
      "335 0.0035968584738712735 0.019106191625400913 0.073339462171965\n",
      "336 0.004447768973671779 0.024819821650075246 0.08934576981235004\n",
      "337 0.0037575115849512383 0.018150126707687262 0.09325567565663515\n",
      "338 0.0038578995512978313 0.022504462543939646 0.10671206468452725\n",
      "339 0.002142019035431834 0.012913948217412655 0.11131565637605573\n",
      "340 0.004006838230507605 0.01737432383960704 0.09702083901719369\n",
      "341 0.004304523994353533 0.017627443062761088 0.09486022758547286\n",
      "342 0.003312846612547376 0.02067193729572879 0.10706857251598043\n",
      "343 0.0025847770053528812 0.01703435729778167 0.09692425965757173\n",
      "344 0.004375260187791516 0.021397997028515074 0.09590386995092029\n",
      "345 0.0024303050682710253 0.018794714547959904 0.0971106542046948\n",
      "346 0.004553967661451663 0.019870073345665526 0.11688780498672618\n",
      "347 0.003294463481110835 0.020314399246400036 0.10406828701408136\n",
      "348 0.0034035113892945832 0.016701282829829366 0.09223975467564838\n",
      "349 0.0034109029539188245 0.019459809995235902 0.12447331583490068\n",
      "350 0.0024551334216276564 0.022182787844755247 0.11557281804138136\n",
      "351 0.004196449732244131 0.018913797533075503 0.08479365184183042\n",
      "352 0.0038043609544896138 0.022412529080839915 0.10663417628602709\n",
      "353 0.0036443249104835367 0.023122950626586262 0.09919745072634344\n",
      "354 0.004573874304895 0.023784954302424426 0.1134231627203141\n",
      "355 0.004208799487069592 0.019147898836589808 0.11702130829570448\n",
      "356 0.00420140457949424 0.024729545496922002 0.08658992061203716\n",
      "357 0.002464669301174523 0.020145127688377368 0.10737781793015339\n",
      "358 0.003107541163740872 0.021151437678471035 0.09458033102619409\n",
      "359 0.004786571667201905 0.023882296824742102 0.09585836844050327\n",
      "360 0.003064083853750796 0.015604635269945964 0.09163777176447566\n",
      "361 0.004514981359954371 0.023918205363182957 0.11740506907101222\n",
      "362 0.003821366031137959 0.018580886473701562 0.0935672386602529\n",
      "363 0.0017587690540320717 0.01471364923950075 0.10951512276803044\n",
      "364 0.0037657857283359903 0.017448741062140838 0.09556134998228871\n",
      "365 0.001740501981305543 0.018548895952877478 0.118210813806637\n",
      "366 0.002794141048632243 0.0175330228791021 0.12684602709301918\n",
      "367 0.0037754201439285946 0.019241076396873637 0.10792171869031644\n",
      "368 0.004199974369946592 0.022625208706556953 0.0922620684441157\n",
      "369 0.0014691145924032433 0.018067682535604528 0.12216277338469066\n",
      "370 0.003527911920082465 0.017297752225089107 0.08353750861087827\n",
      "371 0.001793894941330679 0.022329593770956842 0.11495292293079648\n",
      "372 0.0027039228361376475 0.014306410955397454 0.09758682534843785\n",
      "373 0.0023184536979622876 0.013336284968502695 0.09259423640769004\n",
      "374 0.0031073572604629947 0.018171289552730008 0.1138313502098578\n",
      "375 0.0037426014219322417 0.025041395131125688 0.11763282870393071\n",
      "376 0.0035963994599824607 0.01607986217225245 0.12193384672748463\n",
      "377 0.0021519608888438783 0.013572529820513452 0.0923588147431216\n",
      "378 0.0028639829893220784 0.015764495228468896 0.07162768197476951\n",
      "379 0.004075792384259404 0.017397748275881426 0.09174918934423892\n",
      "380 0.0023244325280399394 0.015393814008890878 0.08521743066111231\n",
      "381 0.004427832684600717 0.021273141834429084 0.09685773193588694\n",
      "382 0.0028181209645105247 0.01835623658401825 0.10410938195829858\n",
      "383 0.003575951196461099 0.018766347870434172 0.1296398618182838\n",
      "384 0.003507375730891547 0.019499707913715837 0.08345784856962883\n",
      "385 0.0038623146819605353 0.019656252810385334 0.08856435906203997\n",
      "386 0.0029373323020710574 0.019408620230234173 0.09633591314961942\n",
      "387 0.00321279544558811 0.022776837965490262 0.11684261143264571\n",
      "388 0.0017080409745943142 0.015496375871194724 0.12364703272703859\n",
      "389 0.003462651517892567 0.02634415456479551 0.11790922752699246\n",
      "390 0.004299609473004833 0.02129536767185804 0.11610641921636129\n",
      "391 0.0029849034844346726 0.019335317194023184 0.10725269437259528\n",
      "392 0.0022491053713735625 0.01888227123684697 0.11947359767217199\n",
      "393 0.003748203373134 0.019041357082289007 0.10713349545173534\n",
      "394 0.003554095579853417 0.02037134812086821 0.11008078389316889\n",
      "395 0.004472999064323278 0.021366765843001748 0.10472173764008882\n",
      "396 0.002359410829803163 0.018968529310231727 0.09296069799242485\n",
      "397 0.001796309944537223 0.01804720050385134 0.09899107127410323\n",
      "398 0.0030023175437520343 0.022072678347624018 0.1032702404198044\n",
      "399 0.0037153916498389467 0.023170020264910222 0.08048445573433116\n",
      "400 0.0026071713896380798 0.020888442183836273 0.12279708033677106\n",
      "401 0.0032183598118924513 0.020016342680997813 0.10555108154539364\n",
      "402 0.0014930206508913472 0.012403690893686653 0.10947351937557545\n",
      "403 0.0035341189358200865 0.010675269266510998 0.07191805393690036\n",
      "404 0.0032059043958001434 0.016441060634831556 0.11939721791926537\n",
      "405 0.003932221080612494 0.01993328439103788 0.10206360141319568\n",
      "406 0.004686655957682157 0.020651224337599465 0.09171742850261638\n",
      "407 0.0040650361862614205 0.022724020807349386 0.11696119746864968\n",
      "408 0.002385716452479417 0.017420839893164106 0.10915613978434682\n",
      "409 0.003584488541390229 0.021272845363900804 0.1257862137896261\n",
      "410 0.0020627852947974652 0.017669615660585957 0.10654722707525927\n",
      "411 0.004111800412661142 0.023420884838557986 0.10157775139361383\n",
      "412 0.003259243553068119 0.015358047404666814 0.08119418917817757\n",
      "413 0.0027869899557935035 0.02281181718168128 0.09071584398181706\n",
      "414 0.004501995738267779 0.021393786749093917 0.09163342107932669\n",
      "415 0.0023400966897186813 0.019194106755559864 0.11410530040705955\n",
      "416 0.0030118626827432184 0.018958108927170713 0.10062973493933705\n",
      "417 0.0016066689094666688 0.016735489676053414 0.09253767128118395\n",
      "418 0.004290795302359703 0.021283759350396163 0.12641441282284765\n",
      "419 0.0034315363311367975 0.024529814466578762 0.12768465355552117\n",
      "420 0.00338001401300967 0.02379428128433752 0.12361048548824108\n",
      "421 0.002478688166914488 0.016867972537508432 0.09601834710403345\n",
      "422 0.002114892948759533 0.012730989899940882 0.09170568648380126\n",
      "423 0.0023524930149322948 0.013702148060465932 0.08807418181102669\n",
      "424 0.0036713543148492195 0.025753976230676854 0.11754737783233549\n",
      "425 0.004121898246266019 0.014217684433959906 0.08806531632638165\n",
      "426 0.0029339997475217763 0.02024139105433289 0.12292880697764966\n",
      "427 0.0028900135563089567 0.021079320907260637 0.1369867397716487\n",
      "428 0.0028120862531998813 0.021009646557947174 0.11610801407488203\n",
      "429 0.002853259207650773 0.024111347305221423 0.12739062228118744\n",
      "430 0.001838483980448033 0.014134818966759988 0.0916250215949714\n",
      "431 0.0035130075294258385 0.018844431308639328 0.11682259761741562\n",
      "432 0.0027193970015857794 0.018597444580226715 0.0894329325218305\n",
      "433 0.0037049079933553124 0.01682737238769784 0.08947086868874246\n",
      "434 0.0038564765140891525 0.024178688331354305 0.12375371737730607\n",
      "435 0.0020839498584737026 0.017218710867667693 0.12065186697713663\n",
      "436 0.0033398396664573856 0.02113042057439099 0.11233140555762638\n",
      "437 0.0016897286968195124 0.021961926285286613 0.12294921002936643\n",
      "438 0.0027662475425451983 0.02179312671305534 0.12318795170573504\n",
      "439 0.001528176055428915 0.015012860376597036 0.11071152588631558\n",
      "440 0.003253457519312381 0.01952276800091191 0.11941235328931202\n",
      "441 0.003424520087751932 0.01730494352088978 0.11206987839382483\n",
      "442 0.002668915236057653 0.02166033943146036 0.09883746636220005\n",
      "443 0.0038759767148989974 0.017649562190665845 0.08117716365614028\n",
      "444 0.0031442360811834995 0.01624608438201012 0.09164492333997039\n",
      "445 0.0030570340959252046 0.018387967661980344 0.09748366343584208\n",
      "446 0.002802092039561375 0.01858294500290345 0.11053534824262194\n",
      "447 0.004275344993178137 0.020949817520973735 0.10475320266525177\n",
      "448 0.0022383393888180253 0.021492962546992837 0.10695206216096168\n",
      "449 0.003258266741473767 0.016191775406872475 0.1174026584247422\n",
      "450 0.002277562160992274 0.01784922412023995 0.0989083724580821\n",
      "451 0.0025834635119665987 0.018448943300546573 0.08749124787881493\n",
      "452 0.004190798154995525 0.02271104050369848 0.13711138377919516\n",
      "453 0.002876414631791179 0.018510015149454186 0.12656305987230598\n",
      "454 0.00323971531367335 0.01553403387956255 0.08244852543064829\n",
      "455 0.0044082090376767035 0.02223912860100736 0.12195591653040382\n",
      "456 0.0030786312470984286 0.017171044103504667 0.11223568632815625\n",
      "457 0.002553350636483218 0.020820687827683287 0.10311641374795652\n",
      "458 0.0027579516388385812 0.02170019546442574 0.09856780957384151\n",
      "459 0.0020275361926189346 0.01865982356980766 0.11200049653097977\n",
      "460 0.003591099349219022 0.01910254760462648 0.08736484547984867\n",
      "461 0.004474684704703236 0.01930504020687162 0.09319410036901904\n",
      "462 0.004087018432087436 0.018934453780287 0.10257000739845942\n",
      "463 0.0047320999099237215 0.02258544605852358 0.10719328941766629\n",
      "464 0.0033884509357190373 0.024161135838285556 0.10411473880732486\n",
      "465 0.0038304697426500157 0.024322525744208456 0.11593091392082544\n",
      "466 0.004354571449328604 0.028138637945873096 0.12756552454302916\n",
      "467 0.004657383056326437 0.025175638528258507 0.14283916245386447\n",
      "468 0.004509267969680998 0.02276536284468788 0.09467421502296328\n",
      "469 0.003921457870354853 0.017178477173270625 0.07628195723472048\n",
      "470 0.003417875503765601 0.021518806094691995 0.11388914153972593\n",
      "471 0.002923334811809302 0.0141656508864066 0.1484741289068927\n",
      "472 0.0035750038843847085 0.02297584132638561 0.11423833799191793\n",
      "473 0.003333332812890598 0.019548269069187038 0.11310232866184841\n",
      "474 0.0033720165563505817 0.024955572044188214 0.12559140347589895\n",
      "475 0.003001096270680074 0.02061976408781699 0.12971605487617022\n",
      "476 0.0017160417901280044 0.01839494480080037 0.06512490585196518\n",
      "477 0.0033015426681136037 0.017217687763324746 0.08377670557854319\n",
      "478 0.0022094944482606 0.022325227310118927 0.11565002271617422\n",
      "479 0.002592015440498141 0.01787680006042655 0.08972128251845393\n",
      "480 0.003232816527703231 0.017262485624903873 0.08771443584938912\n",
      "481 0.0029510930530655156 0.020020324913708816 0.11361704206448454\n",
      "482 0.0019322475227632257 0.017790697942267907 0.12678042536153877\n",
      "483 0.004785140178573093 0.020372326322279807 0.11236808818743729\n",
      "484 0.004527441950527787 0.017772412593252664 0.10690532986808074\n",
      "485 0.00228024838774339 0.01877031311295685 0.10001042022392737\n",
      "486 0.0018446156221292574 0.018668185051740168 0.11094569261554992\n",
      "487 0.003284494087076379 0.01863352399866258 0.09852390794109915\n",
      "488 0.004844388323961089 0.024192108526483996 0.10557974773488957\n",
      "489 0.003163308962842438 0.022710238859402664 0.10178213809148165\n",
      "490 0.0015394804125220532 0.014036844137163114 0.08676930498406493\n",
      "491 0.0039060625824751654 0.015244269153062111 0.10981444060981198\n",
      "492 0.0046025697108223775 0.02519345213190583 0.11009894658692856\n",
      "493 0.002558876495854096 0.01598427941269361 0.10064078810081897\n",
      "494 0.0020207734480115393 0.019685114568006402 0.08221881478077832\n",
      "495 0.003721541646768285 0.018412487979416102 0.10313423463011209\n",
      "496 0.001358266510913309 0.02044180982525095 0.13236356453959555\n",
      "497 0.0025111793074005916 0.016982373640597258 0.09381111262891373\n",
      "498 0.0037094231527206424 0.0242156658973462 0.09751322636388698\n",
      "499 0.003347781176585022 0.020820592537964512 0.09317388225998793\n",
      "500 0.0033414441892964408 0.01767826590094792 0.1124853234802696\n",
      "501 0.004133654560498008 0.01870473772945252 0.09873228682425392\n",
      "502 0.003795937484399924 0.02276932702784072 0.0873950654727117\n",
      "503 0.0025945535496054235 0.014946749253717693 0.08931414297211845\n",
      "504 0.004024796510342142 0.022163997214387342 0.10208273041898092\n",
      "505 0.0015794797826551603 0.019374495593225433 0.117326214026194\n",
      "506 0.0038032266762871173 0.01990247960857427 0.09560689624667151\n",
      "507 0.0012847957583264676 0.013151889887589203 0.12701932433063243\n",
      "508 0.0040728933135542305 0.020557673194650593 0.12017065505096035\n",
      "509 0.0021645216713820593 0.009962556071403561 0.06614545137136692\n",
      "510 0.0032260165177791992 0.019877982434214826 0.14586119191975822\n",
      "511 0.0033049250616442004 0.022131348636484416 0.1146732552848527\n"
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
