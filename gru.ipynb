{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 1000\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - 16):\n",
    "        seq_set[pair].append(size_index[i:i+16])\n",
    "        target_set[pair].append(target_index[i:i+16])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    for pair in range(pairs):\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        in_dim = input_size \n",
    "        for h_dim in hidden_dims:\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(in_dim, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for lin in self.lins:\n",
    "            x = lin(x)\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.h2o(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 1024\n",
    "gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "s2h = SizeToHidden(n_size, [32, 64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tensor, size_tensor, target_tensor = next(iter(dataloader))\n",
    "seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "size_tensor = size_tensor.float().to(device)\n",
    "target_tensor = target_tensor.T.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(16):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2.31351375579834 2.4997995686531067 12.5507173538208\n",
      "200 2.326669216156006 2.3284449005126953 24.640657663345337\n",
      "300 2.3212709426879883 2.3239692568778993 36.265108585357666\n",
      "400 2.317700147628784 2.3220677375793457 47.77649283409119\n",
      "500 2.33556866645813 2.321940860748291 59.89788103103638\n",
      "600 2.3117847442626953 2.318962426185608 70.96683096885681\n",
      "700 2.310422658920288 2.317437171936035 81.98051166534424\n",
      "800 2.3102073669433594 2.315555765628815 93.40983080863953\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-3\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "\n",
    "s_time = time.time()\n",
    "plot_every = 100\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i)\n",
    "    dataloader = DataLoader(dataset[:1000], batch_size=1000, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.25:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 11, 9, 7, 13, 15, 4, 13, 9, 4, 9, 11, 9, 3] False\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7] True\n",
      "[8, 13, 8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6] False\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 6, 11, 11, 8, 11, 9, 7, 4, 7, 4, 6, 11, 8, 13, 8] False\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 7, 8, 10, 4, 8, 4, 11, 10] False\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 1, 7, 11, 6, 8, 7, 10, 7, 7, 4, 8, 7, 7, 8, 4] False\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7] True\n",
      "[8, 7, 8, 13, 10, 6, 6, 4, 13, 2, 6, 7, 3, 7, 4, 4] False\n",
      "[8, 8, 4, 7, 13, 6, 13, 4, 11, 8, 10, 8, 4, 7, 7, 7] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 9, 11, 8, 4, 8, 9, 18, 7, 6, 8, 8, 4, 13, 11, 4] False\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 8, 7, 7, 8, 6, 7, 13, 7, 6, 15, 8, 11, 8, 8, 8] False\n",
      "[8, 11, 9, 8, 4, 4, 4, 11, 4, 4, 11, 6, 4, 7, 11, 4] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 8, 6, 8, 11, 9, 4, 11, 6] False\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 9, 7, 7, 8, 4, 7, 13, 7, 13, 15, 4, 13, 10, 8, 4] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 8, 6, 10, 11, 10, 8, 10, 4] False\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 4, 13, 8, 6, 4, 6, 8, 4, 6, 13, 9, 13, 4, 4, 7] False\n",
      "[8, 11, 8, 11, 10, 2, 4, 6, 11, 11, 8, 11, 10, 4, 10, 4] False\n",
      "[8, 6, 4, 13, 4, 11, 8, 10, 8, 4, 7, 11, 6, 8, 6, 4] False\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 11, 8, 11, 10, 2, 4, 6, 11, 7, 8, 8, 10, 7, 2, 4] False\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 4, 11, 2, 4, 8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 11, 8, 13, 4, 7, 4, 13, 11, 9, 11, 10, 8, 10, 4, 8] False\n",
      "[8, 4, 11, 2, 4, 8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11] True\n",
      "[8, 8, 4, 11, 13, 4, 6, 4, 10, 4, 2, 13, 4, 11, 10, 9] False\n",
      "[8, 4, 13, 2, 6, 8, 6, 6, 11, 4, 10, 11, 13, 4, 7, 10] False\n",
      "[8, 13, 4, 4, 16, 4, 15, 11, 7, 18, 4, 7, 8, 6, 8, 6] False\n",
      "[8, 13, 11, 6, 7, 13, 7, 7, 4, 3, 4, 11, 13, 10, 8, 8] False\n",
      "[8, 11, 9, 11, 4, 9, 11, 4, 7, 12, 13, 4, 8, 11, 4, 7] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7] True\n",
      "[8, 8, 4, 9, 13, 4, 6, 10, 8, 4, 4, 4, 11, 6, 13, 4] False\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7] True\n",
      "[8, 11, 8, 8, 4, 7, 13, 6, 8, 4, 7, 8, 10, 8, 4, 4] False\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 15, 4, 8, 11, 4, 4, 11, 4, 4, 13, 4, 11, 8, 4, 8] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 8, 8, 9, 13, 4, 8, 8, 8, 6, 9, 6, 4, 10, 4] False\n",
      "[8, 11, 9, 10, 4, 8, 11, 6, 13, 4, 11, 7, 6, 11, 4, 7] False\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 8, 6, 10, 11, 2, 7, 4, 4] False\n",
      "[8, 4, 11, 15, 4, 9, 4, 7, 13, 8, 11, 13, 4, 10, 4, 11] False\n",
      "[8, 7, 8, 8, 7, 8, 10, 11, 13, 8, 16, 6, 8, 7, 4, 8] False\n",
      "[8, 4, 7, 8, 4, 7, 8, 10, 7, 8, 8, 7, 7, 11, 6, 4] False\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 8, 6, 10, 11, 10, 8, 10, 4] False\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7] True\n",
      "[8, 8, 6, 11, 13, 6, 7, 13, 7, 6, 11, 1, 7, 13, 11, 13] False\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 7, 7, 8, 4, 7, 13, 10, 4, 6, 4, 4, 4, 8, 6] False\n",
      "[8, 11, 9, 7, 4, 11, 4, 4, 13, 4, 7, 11, 6, 13, 8, 10] False\n",
      "[8, 11, 9, 4, 11, 7, 11, 11, 8, 6, 10, 11, 2, 7, 6, 7] False\n",
      "[8, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13, 4] True\n",
      "[8, 7, 8, 13, 7, 6, 13, 7, 13, 3, 7, 11, 11, 4, 10, 8] False\n",
      "[8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 8, 13, 7, 8, 4, 8, 6, 11, 13, 4, 13, 11, 11, 10, 8] False\n",
      "[8, 4, 11, 2, 4, 8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11] True\n",
      "[8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11] True\n",
      "[8, 15, 4, 8, 11, 4, 4, 11, 4, 6, 8, 6, 11, 13, 8, 8] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0038716170737736148 0.0334946824248467 0.07438397298765258\n",
      "1 0.003924176129509277 0.04170961682719737 0.09808818260648836\n",
      "2 0.0036995575234810667 0.039357536449574876 0.09041966129037701\n",
      "3 0.0029913073643941667 0.034230932882543655 0.08008451724201542\n",
      "4 0.004532570165581325 0.08088103899181086 0.11541280624370863\n",
      "5 0.012657855019812037 0.029606571126727783 0.050218032756635694\n",
      "6 0.005070062278684478 0.06306576987904955 0.12974426772181177\n",
      "7 0.003081014418050916 0.051662114385593164 0.11663942549248547\n",
      "8 0.004497555768371532 0.06699032928774182 0.13076504713529877\n",
      "9 0.0037933070292663212 0.04325757443013087 0.10401575541806748\n",
      "10 0.005238816234082512 0.04031293741550937 0.09367942631796168\n",
      "11 0.005224614964242319 0.07372294326708359 0.12397888223972284\n",
      "12 0.00405656634502131 0.04629792353953572 0.1110045283327368\n",
      "13 0.004564122399224744 0.047542732097450774 0.1098784278877494\n",
      "14 0.0020401052020273236 0.04258841206187983 0.09919430254377648\n",
      "15 0.002174418873336614 0.056132876399306 0.10782548105820262\n",
      "16 0.00406781145167244 0.040565231446620574 0.08239837371891137\n",
      "17 0.004029213498887635 0.05822203332749679 0.1326804655305015\n",
      "18 0.0039347006889500985 0.04868488996712927 0.09185345647702392\n",
      "19 0.005137349743054463 0.060775931779084746 0.10725497113284456\n",
      "20 0.004170464690608272 0.046239083145756095 0.11493331334406842\n",
      "21 0.0040427940784426 0.041537859241084045 0.10763090966776567\n",
      "22 0.0063896597901374 0.05197562304548021 0.11059624170777654\n",
      "23 0.002950845655063974 0.0648950724923173 0.12332418676646878\n",
      "24 0.003587013072468171 0.05608576176702454 0.10013014329839998\n",
      "25 0.0048032106468612334 0.04763560618157317 0.12676180410838037\n",
      "26 0.004274905660822411 0.045183713876796344 0.1003170252173371\n",
      "27 0.008190498625908753 0.04641697938856241 0.07269617239396638\n",
      "28 0.003166135648650859 0.06754771953815358 0.1104149745043953\n",
      "29 0.0032394985969043024 0.07042438774197615 0.13586432048295022\n",
      "30 0.0017682076562900596 0.02505964299878163 0.0835288796129211\n",
      "31 0.005447094689865917 0.030450594174124955 0.04972551246408553\n",
      "32 0.003932537722285018 0.056345986009763635 0.11270218954767512\n",
      "33 0.003528544538006476 0.05142923798964 0.09381805099468268\n",
      "34 0.00517270027339307 0.06670756783730282 0.11434627658813978\n",
      "35 0.0015815955847054372 0.04193949516097348 0.09750218191289137\n",
      "36 0.005305581021772651 0.05515732545756026 0.11453779848575152\n",
      "37 0.003224584323242864 0.043784350870606774 0.0856513571068094\n",
      "38 0.0023344625257816325 0.025900400054382086 0.07618958913951733\n",
      "39 0.0019801211251904895 0.043933572008982 0.11519057774560024\n",
      "40 0.004078147795531067 0.04301390401838393 0.10948058510519665\n",
      "41 0.00447675163886508 0.039690802466500294 0.09332418849834113\n",
      "42 0.0042843978656838385 0.045939855121230264 0.08173421960100616\n",
      "43 0.004183824556253585 0.051491591679861726 0.09813521422555554\n",
      "44 0.004363676744722487 0.04599867715477168 0.10794348929663344\n",
      "45 0.0061824355991317945 0.048709106069264124 0.10469541898743262\n",
      "46 0.003648175737456709 0.06051048056399917 0.10131811258050497\n",
      "47 0.0034066580486843082 0.04013114974991912 0.09387044615376289\n",
      "48 0.003717450974015213 0.04429444034414185 0.10874606804635353\n",
      "49 0.003548289001044867 0.033295719953587925 0.08528315641481618\n",
      "50 0.004413350937163109 0.049759989710748645 0.0993966562207928\n",
      "51 0.0019318634598691187 0.07165957234743228 0.12283278093789532\n",
      "52 0.00363772854848926 0.04551519947611224 0.10272673545773515\n",
      "53 0.0033518617038744277 0.040974352549593654 0.08923612381655191\n",
      "54 0.004558629615594355 0.0463406870746256 0.11799267739388283\n",
      "55 0.005921055229775251 0.06654137688929737 0.11839469004114844\n",
      "56 0.0036029206142480334 0.035137764268001644 0.10315916506533149\n",
      "57 0.005258320787282892 0.03465138122201251 0.08492055734442056\n",
      "58 0.004796739603969844 0.07068289965815445 0.13118560673491692\n",
      "59 0.004206179671501839 0.035747468344054706 0.07876220374487027\n",
      "60 0.011116156938650357 0.1003046415307026 0.1306220140729357\n",
      "61 0.0035334732557219316 0.04347538656343174 0.07898722130023553\n",
      "62 0.004862376775055291 0.05356944268157846 0.08436850477714919\n",
      "63 0.005085120321505194 0.06470777044141883 0.10603948995369913\n",
      "64 0.004025523872255484 0.0317930751102852 0.0538545901716239\n",
      "65 0.004925084780527674 0.04605018092626482 0.08952148327741329\n",
      "66 0.005511418880613852 0.05127943538303992 0.09539199109774618\n",
      "67 0.005243956549056166 0.04363085119274465 0.1015824634216016\n",
      "68 0.0029451894707419627 0.050257328910560124 0.11256503973884295\n",
      "69 0.003971196914045989 0.04329563623332089 0.09749208411470436\n",
      "70 0.0030156567801640577 0.03832633012863691 0.09645210707340438\n",
      "71 0.005256930281658199 0.05217636478041409 0.09208198734110978\n",
      "72 0.002328515556387172 0.02856006452345998 0.09400623734898014\n",
      "73 0.004757077840304002 0.06374820267931194 0.11424021784363969\n",
      "74 0.006252135158538609 0.06770132057032492 0.11779999113147142\n",
      "75 0.005447496994084055 0.05202756296395267 0.13025231117973585\n",
      "76 0.005208799471558294 0.046569898460918566 0.08227665051650707\n",
      "77 0.006045313418360996 0.049837911653985614 0.10871046086697173\n",
      "78 0.004104014550120119 0.04930758403537236 0.10812618344733423\n",
      "79 0.006323182300084695 0.06264578270780224 0.1110879621573403\n",
      "80 0.005388185571071072 0.056906107272373316 0.12951544271171134\n",
      "81 0.003414361151882123 0.04134998839662186 0.10041605267708868\n",
      "82 0.005002740914668369 0.045239428848731736 0.1252707304880256\n",
      "83 0.0028450108333315065 0.04144346549922075 0.08284776234631384\n",
      "84 0.002383741381789951 0.04145564118725097 0.09388122615895302\n",
      "85 0.0032983038813733097 0.0414885503834865 0.08855237357835141\n",
      "86 0.004418345917728994 0.05568399908782577 0.1175586511420501\n",
      "87 0.00514667831695263 0.0674632806382707 0.10284659988877401\n",
      "88 0.004774192779053676 0.050773584484337705 0.09946371859928224\n",
      "89 0.005853949676774722 0.05390409952289475 0.10856943109415274\n",
      "90 0.003362385868482129 0.04952919074926827 0.0846004822210442\n",
      "91 0.004829596091321381 0.05840579876632485 0.10466351552784936\n",
      "92 0.002518271814325011 0.022340156488539268 0.06611252741223159\n",
      "93 0.004042090572168723 0.04089238142910354 0.08484239804991969\n",
      "94 0.004330056760016016 0.04550897396740412 0.08863416448218087\n",
      "95 0.0053433543913643956 0.061251045123333445 0.11776436446576902\n",
      "96 0.00699599409435475 0.04471800762933384 0.08983328362148077\n",
      "97 0.0031953176819777707 0.06421591646847527 0.10860041798955294\n",
      "98 0.0030700544715994834 0.06471894908137893 0.11128037643126554\n",
      "99 0.001986598858630278 0.040214183793921145 0.09866602630707172\n",
      "100 0.006938293082740331 0.07652464188401978 0.14448147730158845\n",
      "101 0.0032669973829211863 0.05592503349401284 0.106615851017476\n",
      "102 0.0030873888497316973 0.02617715908633301 0.08916225768503629\n",
      "103 0.00480725386007252 0.07814070209322294 0.11950177665120335\n",
      "104 0.002335956177803071 0.04834111561764459 0.10517796336706497\n",
      "105 0.003971548245178992 0.05849057371126825 0.0868544182054524\n",
      "106 0.00303251807631424 0.05265478910369284 0.1096195322165198\n",
      "107 0.008000075012845158 0.07789895629830143 0.12065236187576686\n",
      "108 0.004607912830196188 0.0516540881363458 0.11385276206439068\n",
      "109 0.004410389193741164 0.04803397301301926 0.10615126756144289\n",
      "110 0.004856683809180789 0.050561981696239156 0.10128147226351884\n",
      "111 0.00592115186694985 0.03134649381155086 0.07439854388919773\n",
      "112 0.004143091673959661 0.05634723053950528 0.1293282320905316\n",
      "113 0.008487622541401922 0.05570687969224925 0.12598600097389734\n",
      "114 0.007190607043513099 0.0684636292777343 0.1345210871828281\n",
      "115 0.0060296458041978725 0.05807668557151747 0.10661565519327279\n",
      "116 0.003920942650123749 0.0682805383530074 0.11258895932137199\n",
      "117 0.003091787505260508 0.04196366788651862 0.09368744023044523\n",
      "118 0.0046281092579942285 0.02923228679515106 0.08104033734180843\n",
      "119 0.0022535142113583742 0.04083620869646239 0.138250433309481\n",
      "120 0.00465385626847594 0.06311461577840748 0.10294816240320928\n",
      "121 0.006125270842857385 0.06110707541972482 0.09171204201387081\n",
      "122 0.00348400974377612 0.04380746259500604 0.07073966493734425\n",
      "123 0.0035928218732491227 0.0279842007263143 0.07945943021251102\n",
      "124 0.007594918537935669 0.03918418762255243 0.06420187588269041\n",
      "125 0.003965971277994882 0.06292579917182531 0.10318886155438789\n",
      "126 0.003430348455688995 0.05850975671680371 0.11534827739098838\n",
      "127 0.00404054409331591 0.043621540357284785 0.08353149150408729\n",
      "128 0.006616461932278542 0.08986204791375724 0.12290823644083622\n",
      "129 0.0040339507313603635 0.05038943027839078 0.08974930434817055\n",
      "130 0.005822950731708003 0.06684312540457621 0.10037983821453339\n",
      "131 0.0036039310274506276 0.06220196924076924 0.12102342098606045\n",
      "132 0.003719953806370246 0.05121767007362668 0.09690805786807413\n",
      "133 0.0061363110613124015 0.0517009848558348 0.10531507924987484\n",
      "134 0.003276494585829457 0.07270376124216638 0.1050307094943411\n",
      "135 0.003994472272677772 0.052086028106530166 0.08920790308945475\n",
      "136 0.004012865321494524 0.08773694810491242 0.12851581618358557\n",
      "137 0.002739175605992194 0.06295855246956478 0.10609606676928945\n",
      "138 0.005096155008801676 0.06652857406786236 0.10760033406916603\n",
      "139 0.003966554496653767 0.05749891065836781 0.09442629070535768\n",
      "140 0.004686486617582887 0.07090322478720126 0.10624066535303917\n",
      "141 0.008776411084261252 0.06490470109392288 0.11258922138321273\n",
      "142 0.0035191812289404926 0.06846098211319157 0.0910921450542701\n",
      "143 0.002676790925787603 0.07737683499818959 0.12408745941940377\n",
      "144 0.007076445624306921 0.06385227236981514 0.09869780050820959\n",
      "145 0.003027568187601951 0.07022321522302405 0.11803138925739239\n",
      "146 0.004845225310157306 0.07322994830706467 0.09803186221028048\n",
      "147 0.003300999325629316 0.08103461373619449 0.13331304787276185\n",
      "148 0.0026306986691482116 0.040547098673411094 0.09052622701998081\n",
      "149 0.0034041479802841444 0.04453413588188915 0.07967789723116434\n",
      "150 0.006276019055498864 0.05638738439716573 0.10003694061190987\n",
      "151 0.0030065439501382194 0.07583199956043575 0.09537177594660318\n",
      "152 0.00533463200871825 0.07381898986311064 0.11451961403987419\n",
      "153 0.003960366773881199 0.06821663327791025 0.11044122528100195\n",
      "154 0.007849874966478562 0.06361461057455209 0.11373641133924185\n",
      "155 0.0033688477692086277 0.039164153590572987 0.08128164451276734\n",
      "156 0.008286875653023926 0.0811754005844682 0.10872583923040824\n",
      "157 0.003063235867037696 0.050528798035736816 0.10634811159356042\n",
      "158 0.005932039525904615 0.05174949980707537 0.08457653817014815\n",
      "159 0.004493212339421572 0.07442958454085569 0.11268500790480665\n",
      "160 0.0065894509816610165 0.06468388103578712 0.10954665027153299\n",
      "161 0.001810766846183383 0.04681637328083459 0.0909125736197453\n",
      "162 0.004084605792563698 0.048112161987395916 0.11084025562401331\n",
      "163 0.0064904529641663985 0.08694780111504394 0.1440204289680475\n",
      "164 0.0031209106301149977 0.06131279445131787 0.12467210253729477\n",
      "165 0.005258469741889763 0.07158975337404175 0.1432870831559574\n",
      "166 0.0035774377435682913 0.058911398360580944 0.09439297943571046\n",
      "167 0.0034391272898871695 0.09517019721467922 0.13018105817284256\n",
      "168 0.0071381707638644275 0.08204746356286467 0.09314842159654291\n",
      "169 0.004641949256167134 0.06118014213339874 0.09245698317615351\n",
      "170 0.005613703509433531 0.05751945166382548 0.11461953957463303\n",
      "171 0.0036789976944564718 0.049833223455870115 0.08777500204388634\n",
      "172 0.004691205487638498 0.06296894712515176 0.10048512746162414\n",
      "173 0.005154532008362738 0.06787410124438653 0.1093718026187037\n",
      "174 0.00475139115447441 0.09205520101026429 0.11053039788971583\n",
      "175 0.00834922827058951 0.0544011588023732 0.07964797781606814\n",
      "176 0.0032302623006242393 0.03238567928364186 0.06878940670923843\n",
      "177 0.0036732928417402737 0.06420084966882866 0.0894152149843582\n",
      "178 0.00447121448666102 0.06962503863795727 0.11298551643773931\n",
      "179 0.005069255170784753 0.07682951920551898 0.10811906204385663\n",
      "180 0.0026178013047471463 0.048940653261073844 0.09405027563482538\n",
      "181 0.00657298795598199 0.0598783020417924 0.11291037635365417\n",
      "182 0.007045431685663042 0.05979875765401184 0.08533517821657335\n",
      "183 0.004746046764117608 0.0692335197610796 0.10767043187472479\n",
      "184 0.0022377920902506886 0.043690886655072265 0.09820512338011766\n",
      "185 0.004321536689482125 0.06513253498230422 0.10868795978763622\n",
      "186 0.0028732506525608447 0.05258285496808103 0.10143916904734847\n",
      "187 0.002743315980278029 0.059648758509578036 0.10095943222717679\n",
      "188 0.0032818592213667403 0.03731380214502837 0.11290204423466099\n",
      "189 0.00484102432894195 0.050072907649688206 0.07968903372599809\n",
      "190 0.003752950145983977 0.07287362159972885 0.11784937718431862\n",
      "191 0.00418636116383908 0.043511996518221636 0.08940310682671432\n",
      "192 0.006008380283988778 0.07495119111299337 0.10189865833846251\n",
      "193 0.004007712924070463 0.06405173320512236 0.10811282124954152\n",
      "194 0.004862687735928339 0.06336814469497554 0.12385252431640265\n",
      "195 0.006939356903152799 0.06613963718111554 0.10649152646989012\n",
      "196 0.007064569213796756 0.08614075272775404 0.11611421458507454\n",
      "197 0.0036488558976525438 0.0569918292690604 0.08643302769476928\n",
      "198 0.004562634317810527 0.08964374010252724 0.1162532807086241\n",
      "199 0.005145917728629675 0.07110533259722768 0.1114876181949934\n",
      "200 0.005353949415842382 0.08366738080706199 0.12339959452216184\n",
      "201 0.008236870570196134 0.04860005233103394 0.09472907703012409\n",
      "202 0.004653453880085083 0.05633852420305442 0.10389311149713656\n",
      "203 0.004845606962293683 0.08530316227699522 0.09143602132128903\n",
      "204 0.005401036726107193 0.0779849800735807 0.11280835998862546\n",
      "205 0.0035223138286669476 0.058871201094493966 0.1150448802359685\n",
      "206 0.004313771023022764 0.08929939913128201 0.13322856142263045\n",
      "207 0.003165770357443886 0.059842851392997484 0.1012241012022729\n",
      "208 0.004244310002832889 0.07110557841614339 0.1140532958121987\n",
      "209 0.0061417933942686746 0.07576017842294896 0.09486216521290393\n",
      "210 0.004107253855570096 0.05986389047083315 0.12297561942915966\n",
      "211 0.006717222042523712 0.061207891417420626 0.11456612833243944\n",
      "212 0.007482849863935961 0.07455835906012687 0.12300036363205422\n",
      "213 0.005440751552282699 0.08175241128537493 0.10699814646874085\n",
      "214 0.006414971251079125 0.11056019228400629 0.12758273246357865\n",
      "215 0.003155586658850149 0.0642054093871563 0.13872778054976403\n",
      "216 0.0034255089711277957 0.06811399086865169 0.10107372789717542\n",
      "217 0.006059337951662086 0.05866607579156548 0.09943353202756396\n",
      "218 0.004187054069101773 0.09510720917548646 0.13390407879601338\n",
      "219 0.0042374766912588934 0.05416134762641883 0.12191751791804241\n",
      "220 0.0031268093351945274 0.07623147964062588 0.09208584362878441\n",
      "221 0.0065436373641920295 0.06860889373213212 0.10560656996581219\n",
      "222 0.0046501649060125645 0.0754059488534713 0.10560507104951354\n",
      "223 0.004373644893857973 0.047077589388976236 0.0893605507208346\n",
      "224 0.006612521974558032 0.0643380365375224 0.12569543493629023\n",
      "225 0.00500866777886543 0.09804074445102816 0.12032271277181444\n",
      "226 0.008697122245809671 0.07300604201169578 0.12155468956301153\n",
      "227 0.004703415887511451 0.05986548689926928 0.08232137529844762\n",
      "228 0.003432503951020463 0.052448096709657444 0.108516690380079\n",
      "229 0.0029823659908045017 0.06738073839997452 0.117468361304133\n",
      "230 0.007430453378420311 0.06400560230898136 0.09528782131028868\n",
      "231 0.005640838394780814 0.07193735665224969 0.11105506169476217\n",
      "232 0.0034876730135791393 0.02558542578066974 0.07250989194117202\n",
      "233 0.004250228386370351 0.08160353884237326 0.09824402448813203\n",
      "234 0.006865094519761856 0.06368990057116135 0.07751015606232506\n",
      "235 0.004336384586442116 0.05627322431967506 0.10709143017381262\n",
      "236 0.004430614492353037 0.07042456421652657 0.12683832434860204\n",
      "237 0.003996264490647014 0.060398368816626996 0.10761146394989049\n",
      "238 0.003875712974344939 0.08201916678856155 0.13184994220593665\n",
      "239 0.005743432782944254 0.03940469354080238 0.06276717258011746\n",
      "240 0.00382281165613459 0.044410853558411095 0.07573998496592\n",
      "241 0.005311735593450944 0.06696209950181488 0.08367176784699103\n",
      "242 0.0016643517079820542 0.09065862746463023 0.11293748789603103\n",
      "243 0.002342329294273481 0.03762692854294631 0.09877463377450128\n",
      "244 0.003656603411158399 0.08072977456008454 0.14229319714695476\n",
      "245 0.0071979337239330456 0.06385098352539109 0.10909190382558182\n",
      "246 0.0027781078725087967 0.053186961277692194 0.09198659449508995\n",
      "247 0.00374755275083697 0.05758605235279884 0.09701956756176701\n",
      "248 0.007139135012647423 0.06487827583963186 0.10187109153041701\n",
      "249 0.0033989352019624434 0.0744302689802424 0.12527938677867317\n",
      "250 0.0029827232695490897 0.0576568408752143 0.0905437507592273\n",
      "251 0.005118538613255082 0.06585509187028618 0.09469940929488022\n",
      "252 0.0020956662236754904 0.08597178924450816 0.08890603551519202\n",
      "253 0.0020387705738938613 0.05763274380094657 0.12154393439433055\n",
      "254 0.004574668508038308 0.061107298169974414 0.10088107959941289\n",
      "255 0.00424727760376038 0.07245713151821782 0.08685069655611558\n",
      "256 0.0038026225991523925 0.07581008492718179 0.10640167797327288\n",
      "257 0.004293338789587093 0.06075029433007065 0.10282686781413888\n",
      "258 0.007384305390314733 0.06796492132972691 0.1055495538523391\n",
      "259 0.007496797226708042 0.048651334166277885 0.076499973562765\n",
      "260 0.004759671456757683 0.05681214575972871 0.09814768265814042\n",
      "261 0.004209232945546539 0.06039888574685158 0.10722989801666069\n",
      "262 0.002869009382948565 0.08472905002341244 0.11576468266151335\n",
      "263 0.00705840920186659 0.05133112310048037 0.08294608179800636\n",
      "264 0.00236118185190925 0.05562029761142316 0.09264866722895626\n",
      "265 0.005736075961354131 0.0889249483784898 0.10213805945429383\n",
      "266 0.0020816119282490034 0.06376404092175163 0.11673137041527518\n",
      "267 0.0038470263435405744 0.0662398150736411 0.11505232219944674\n",
      "268 0.005709663633878098 0.07501914141602191 0.11681288095025588\n",
      "269 0.005525193914172612 0.07272427511198179 0.09818117369705769\n",
      "270 0.009805061859867853 0.10759868818156139 0.12690448473814386\n",
      "271 0.007745230403564193 0.07324692703749391 0.10088684964069353\n",
      "272 0.004973464325591901 0.06853211865544703 0.1089261638961837\n",
      "273 0.006208228066867433 0.07416759816040025 0.10829117481273301\n",
      "274 0.00588224098288115 0.1014162962338982 0.11344631006966516\n",
      "275 0.007934589623683737 0.1108978439731485 0.14242968612552628\n",
      "276 0.004528366032808196 0.07063133854821757 0.10108115506398503\n",
      "277 0.006464595929106271 0.09907437410609786 0.12454977855718441\n",
      "278 0.0059427904118880765 0.0687347667190688 0.11461697449156176\n",
      "279 0.004811802592777763 0.04580404063996947 0.10606868938805603\n",
      "280 0.006721882682522501 0.045412700311867824 0.06563804932688432\n",
      "281 0.00293657021937514 0.07802028525634006 0.09554581187061856\n",
      "282 0.004600171340755296 0.02983857162254414 0.07971294067574508\n",
      "283 0.005183603018727312 0.0732601598569281 0.13799291505039743\n",
      "284 0.004042366744999594 0.0913510282009471 0.12144883009376016\n",
      "285 0.004195340257544334 0.07882472236729512 0.09811081316973778\n",
      "286 0.003938583638063036 0.07371933007139486 0.11645773229967346\n",
      "287 0.0023136756170658726 0.06995235686236809 0.09285126360610702\n",
      "288 0.004296839148183197 0.07023492715890615 0.10078995678808324\n",
      "289 0.005310985567875249 0.06817153767813469 0.10919202339480627\n",
      "290 0.003775252714914556 0.07201530854785576 0.11898328663556898\n",
      "291 0.006293525549415647 0.05501677381350126 0.08875711419612246\n",
      "292 0.012071245575025646 0.08358550899022715 0.10221639096253146\n",
      "293 0.005957732385296961 0.07259971074871469 0.11744427139158577\n",
      "294 0.004580458405616199 0.08870821333342722 0.12272442562734903\n",
      "295 0.004254490195103916 0.04889893043362774 0.07984582092451412\n",
      "296 0.0076367766093492235 0.07059137935744147 0.10648201583182101\n",
      "297 0.006158850693254311 0.08281503410459315 0.10770138105738536\n",
      "298 0.004518833032843606 0.10060872194620327 0.1145940911208721\n",
      "299 0.004386599833083787 0.07342255802431069 0.09685983830433235\n",
      "300 0.004760549607514765 0.05693599964329038 0.10010645303744843\n",
      "301 0.006455303943612191 0.08231353657654539 0.13183396009071927\n",
      "302 0.002447296832296517 0.0813103449498338 0.12525785182883947\n",
      "303 0.004908568828365002 0.06223544943487987 0.09574309567394654\n",
      "304 0.004073921154218184 0.09939280000167489 0.13240770043825123\n",
      "305 0.003599119031838585 0.06494201009083658 0.0919907207814356\n",
      "306 0.0024111143352749747 0.037401612114894636 0.06917291880845743\n",
      "307 0.0047472115268389365 0.08244546802299152 0.1439166924671847\n",
      "308 0.003045614692351208 0.045577771565456254 0.09019924654102911\n",
      "309 0.005275267307917941 0.06402513354113792 0.11015514030459167\n",
      "310 0.00550206013249522 0.07049738661717714 0.10680299147541056\n",
      "311 0.005900874384826921 0.06423320925070487 0.09563895172548914\n",
      "312 0.004897756587828379 0.0694862403334224 0.10736309051681457\n",
      "313 0.004109868067428056 0.06937611221095906 0.12132492066986807\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     11\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, \u001b[39m16\u001b[39;49m, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(new_size)\u001b[39m.\u001b[39missubset(np\u001b[39m.\u001b[39munique(pairdata[freqpairs[pair]]\u001b[39m.\u001b[39msize_index\u001b[39m.\u001b[39mvalues)):\n\u001b[1;32m     14\u001b[0m         size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "Cell \u001b[0;32mIn[71], line 15\u001b[0m, in \u001b[0;36msample\u001b[0;34m(size_data, seq_length, start_size)\u001b[0m\n\u001b[1;32m     13\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39m, hn)\n\u001b[1;32m     14\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[0;32m---> 15\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     16\u001b[0m size \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(n_size, p\u001b[39m=\u001b[39mp_size)\n\u001b[1;32m     17\u001b[0m output_seq\u001b[39m.\u001b[39mappend(size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds = [], []\n",
    "for pair in range(1000):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(100):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, 16, start_size=start_size)\n",
    "            if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "                size_seq_gen += list(new_size[1:])\n",
    "                start_size = new_size[-1]\n",
    "                if seed > 10:\n",
    "                    start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.05:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.016796066997483282 0.07346432855368562\n",
      "90 0.07909204873483928 0.2751986895385197\n",
      "95 0.09532450370252392 0.31491465589104833\n",
      "99 0.1320168295307611 0.3700949495105308\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.012160265192768834 0.07145898533012404\n",
      "90 0.047915725836543216 0.2727736298325866\n",
      "95 0.053717750535610495 0.3153216864822403\n",
      "99 0.06597230544940674 0.36794891171362076\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [05:13<00:00, 15.93it/s]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in tqdm(range(1000)):\n",
    "    torch.manual_seed(i)\n",
    "    latent_dim = 32\n",
    "    z = torch.randn((1, latent_dim)).to(device)\n",
    "    size = decoder(z)\n",
    "    size = size.squeeze().detach().to('cpu').numpy()\n",
    "    size[size < 1e-3] = 0\n",
    "    size /= size.sum()\n",
    "\n",
    "    dis = []\n",
    "    for j in range(1000):\n",
    "        loss = JSD(size, sizedata[j])\n",
    "        dis.append(loss)\n",
    "\n",
    "    pair = np.argmin(dis)\n",
    "    a.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.37469881772994995 0.36950908958911893 11.19027829170227\n",
      "200 0.36915746331214905 0.35975850641727447 21.188878059387207\n",
      "300 0.35969191789627075 0.35673402369022367 32.09577918052673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# dataset.extend([tune_dataset[i] for i in ran_index])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     14\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m plot_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m sum_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m seq_tensor, size_tensor, target_tensor \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m----> 6\u001b[0m     seq_tensor \u001b[39m=\u001b[39m inputTensor(seq_tensor)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     size_tensor \u001b[39m=\u001b[39m size_tensor\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     target_tensor \u001b[39m=\u001b[39m target_tensor\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36minputTensor\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lines\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     13\u001b[0m         size \u001b[39m=\u001b[39m lines[line][i]\n\u001b[0;32m---> 14\u001b[0m         tensor[i][line][size] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "# optimizer = torch.optim.Adam([{'params': s2h.parameters()}], lr=lr)\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "\n",
    "s_time = time.time()\n",
    "plot_every = 100\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i)\n",
    "    ran_index = np.random.permutation(len(tune_dataset))[0:1000]\n",
    "    # dataset.extend([tune_dataset[i] for i in ran_index])\n",
    "    dataloader = DataLoader(dataset, batch_size=2000, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.1:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:58<00:00, 17.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "a = set()\n",
    "for i in tqdm(range(1000)):\n",
    "    torch.manual_seed(i)\n",
    "    latent_dim = 32\n",
    "    z = torch.randn((1, latent_dim)).to(device)\n",
    "    size = decoder(z)\n",
    "    size = size.squeeze().detach().to('cpu').numpy()\n",
    "    size[size < 1e-3] = 0\n",
    "    size /= size.sum()\n",
    "\n",
    "    dis = []\n",
    "    for j in range(1000):\n",
    "        loss = JSD(size, sizedata[j])\n",
    "        dis.append(loss)\n",
    "        # if loss < 0.02:\n",
    "        #     a.add(j)\n",
    "\n",
    "    pair = np.argmin(dis)\n",
    "    a.add(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsds = np.zeros((1000, 1000))\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        jsds[i][j] = JSD(sizedata[i], sizedata[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.0712686 , 0.0989841 , 0.10461984, 0.10732551,\n",
       "       0.11061315, 0.11413909, 0.11538036, 0.11660952, 0.11782312,\n",
       "       0.12270552, 0.12357579, 0.12381709, 0.12831985, 0.12900488,\n",
       "       0.13021052, 0.13290488, 0.13328528, 0.13464494, 0.13588953,\n",
       "       0.13698097, 0.13924533, 0.14027519, 0.14082266, 0.14095486,\n",
       "       0.14102039, 0.14222849, 0.14290341, 0.14387662, 0.14660579,\n",
       "       0.14678407, 0.14694698, 0.14717077, 0.14719107, 0.14890525,\n",
       "       0.14939437, 0.14958512, 0.15188435, 0.1524103 , 0.15251156,\n",
       "       0.15369764, 0.1559504 , 0.15639397, 0.15688702, 0.15898742,\n",
       "       0.15899087, 0.16149691, 0.16396171, 0.16474164, 0.16477304,\n",
       "       0.1657369 , 0.16581361, 0.16621777, 0.16631866, 0.1666306 ,\n",
       "       0.16702055, 0.16783559, 0.16789516, 0.16937711, 0.16999767,\n",
       "       0.17006917, 0.17089419, 0.17237322, 0.17269899, 0.17295262,\n",
       "       0.17311279, 0.1738042 , 0.17434601, 0.17441435, 0.17492409,\n",
       "       0.17498089, 0.17590966, 0.17673751, 0.1767769 , 0.17685239,\n",
       "       0.17733793, 0.17733859, 0.17755393, 0.177951  , 0.17813919,\n",
       "       0.17815045, 0.17871073, 0.17879594, 0.17892494, 0.1791654 ,\n",
       "       0.17921196, 0.1801971 , 0.1807614 , 0.18117546, 0.18125231,\n",
       "       0.18135442, 0.18198672, 0.18222288, 0.18241376, 0.18258302,\n",
       "       0.18262841, 0.18266575, 0.18293546, 0.18320284, 0.18338127,\n",
       "       0.18383586, 0.18386672, 0.18406265, 0.18417186, 0.18449631,\n",
       "       0.18469741, 0.18472404, 0.18474315, 0.18486096, 0.18635224,\n",
       "       0.18651748, 0.18662395, 0.18693884, 0.18698377, 0.18719315,\n",
       "       0.18781946, 0.18836521, 0.18884797, 0.18956284, 0.18983969,\n",
       "       0.19001956, 0.19022332, 0.1902604 , 0.19076237, 0.19097689,\n",
       "       0.19151861, 0.19152719, 0.19177246, 0.19188523, 0.19231322,\n",
       "       0.19295623, 0.1931194 , 0.19357893, 0.19446891, 0.19458126,\n",
       "       0.19465321, 0.19476943, 0.19499326, 0.19504991, 0.19534656,\n",
       "       0.19608496, 0.19671116, 0.19693814, 0.19783898, 0.19786838,\n",
       "       0.1982728 , 0.19864062, 0.19908045, 0.19972837, 0.20060773,\n",
       "       0.20075074, 0.20101976, 0.20118712, 0.20120148, 0.20132543,\n",
       "       0.20145173, 0.20146317, 0.20179804, 0.20192126, 0.20241434,\n",
       "       0.20271094, 0.20293146, 0.20404812, 0.20447014, 0.20474019,\n",
       "       0.20492326, 0.20495016, 0.20538361, 0.20572343, 0.20576312,\n",
       "       0.20612296, 0.20626037, 0.20639919, 0.20696949, 0.20826728,\n",
       "       0.20885927, 0.20887153, 0.20897336, 0.20911203, 0.20918792,\n",
       "       0.20986857, 0.21055266, 0.21120023, 0.21134245, 0.21140708,\n",
       "       0.21195211, 0.21231916, 0.2123584 , 0.21285352, 0.21290893,\n",
       "       0.21292164, 0.21301341, 0.21313637, 0.21361757, 0.21449909,\n",
       "       0.21477319, 0.21501816, 0.21519042, 0.21579753, 0.21616203,\n",
       "       0.21639014, 0.21642339, 0.21683652, 0.21722668, 0.21743179,\n",
       "       0.21770664, 0.21771587, 0.21818101, 0.2184033 , 0.21898291,\n",
       "       0.21900266, 0.21918248, 0.21927371, 0.21934558, 0.21940255,\n",
       "       0.21944778, 0.21947962, 0.21966281, 0.21973549, 0.21975252,\n",
       "       0.21984796, 0.22036319, 0.22090047, 0.22147298, 0.22156073,\n",
       "       0.22157982, 0.22160772, 0.2217407 , 0.22227462, 0.2225324 ,\n",
       "       0.22270144, 0.22272389, 0.22278591, 0.22326229, 0.22348136,\n",
       "       0.22352013, 0.22385144, 0.22414126, 0.2246308 , 0.2248496 ,\n",
       "       0.22529469, 0.22537828, 0.22555107, 0.22572806, 0.22654558,\n",
       "       0.22682547, 0.22714561, 0.22725781, 0.22762842, 0.22779162,\n",
       "       0.22790799, 0.22804214, 0.22859007, 0.22862069, 0.22929094,\n",
       "       0.22962914, 0.22984653, 0.229852  , 0.23023409, 0.23025379,\n",
       "       0.23072017, 0.23144365, 0.23192409, 0.23249705, 0.23277592,\n",
       "       0.23339237, 0.23361231, 0.23404151, 0.23500077, 0.23502402,\n",
       "       0.23528629, 0.23531764, 0.23537534, 0.23545058, 0.23553714,\n",
       "       0.23563498, 0.23594436, 0.23602324, 0.23612106, 0.23627042,\n",
       "       0.23637232, 0.23641386, 0.23655684, 0.23687559, 0.23708677,\n",
       "       0.23755224, 0.23795358, 0.23853469, 0.23857294, 0.23888587,\n",
       "       0.23897496, 0.23925227, 0.23958056, 0.2398841 , 0.24003631,\n",
       "       0.2403348 , 0.24041492, 0.24134958, 0.24139177, 0.24148694,\n",
       "       0.24198383, 0.24254673, 0.24316905, 0.24340846, 0.24362422,\n",
       "       0.24400348, 0.24474908, 0.24526318, 0.24555859, 0.24594612,\n",
       "       0.24610359, 0.24627118, 0.24662194, 0.24665233, 0.24693703,\n",
       "       0.24764772, 0.24771226, 0.24774597, 0.24781387, 0.2481245 ,\n",
       "       0.24854935, 0.24855978, 0.24890758, 0.24895393, 0.24916782,\n",
       "       0.25007749, 0.25015382, 0.2503157 , 0.25059996, 0.25081337,\n",
       "       0.25185066, 0.25229848, 0.25245055, 0.25289129, 0.25331318,\n",
       "       0.2533505 , 0.25387384, 0.25513259, 0.25525203, 0.25658397,\n",
       "       0.25737137, 0.25754914, 0.25771241, 0.25822603, 0.2583451 ,\n",
       "       0.25841318, 0.25947204, 0.25995359, 0.26002243, 0.26028342,\n",
       "       0.2603924 , 0.26097775, 0.26111144, 0.26114015, 0.26182433,\n",
       "       0.26185643, 0.26188322, 0.26223239, 0.26225009, 0.26235135,\n",
       "       0.26273467, 0.26360623, 0.26368101, 0.26409175, 0.26454406,\n",
       "       0.26492847, 0.26572468, 0.2659745 , 0.26600219, 0.26604093,\n",
       "       0.26605977, 0.26621498, 0.26650083, 0.26679597, 0.26723988,\n",
       "       0.26749053, 0.26758702, 0.26797904, 0.26821143, 0.26922938,\n",
       "       0.2704383 , 0.27076334, 0.27088524, 0.27114124, 0.27173614,\n",
       "       0.27216456, 0.27220683, 0.27262086, 0.2730809 , 0.27316495,\n",
       "       0.27450374, 0.27507727, 0.27579355, 0.2764037 , 0.27649103,\n",
       "       0.27655353, 0.27664809, 0.2766998 , 0.27776084, 0.27813467,\n",
       "       0.27830197, 0.27896472, 0.27904399, 0.27916736, 0.27925291,\n",
       "       0.27948919, 0.27952152, 0.27956051, 0.27958542, 0.27991394,\n",
       "       0.28011877, 0.28037961, 0.2804614 , 0.28073998, 0.28117339,\n",
       "       0.28124103, 0.28125505, 0.28157578, 0.28256693, 0.2828647 ,\n",
       "       0.28289178, 0.28305584, 0.28308632, 0.28308888, 0.28324722,\n",
       "       0.28330103, 0.28443007, 0.2855556 , 0.28612781, 0.28697198,\n",
       "       0.28733192, 0.28735517, 0.28749486, 0.28882455, 0.28893915,\n",
       "       0.28907958, 0.28969924, 0.28980439, 0.29037346, 0.29133943,\n",
       "       0.29138968, 0.29182541, 0.29194244, 0.29200094, 0.29218193,\n",
       "       0.29344498, 0.29582257, 0.29653179, 0.29702478, 0.29855564,\n",
       "       0.29856078, 0.29924234, 0.29950011, 0.30010905, 0.30033352,\n",
       "       0.30214963, 0.3033076 , 0.30453139, 0.30488249, 0.30516044,\n",
       "       0.30526441, 0.30598692, 0.30701168, 0.30769269, 0.3083303 ,\n",
       "       0.3087238 , 0.31053665, 0.31084658, 0.31132074, 0.31251387,\n",
       "       0.31274526, 0.31293362, 0.313071  , 0.31308757, 0.31324137,\n",
       "       0.31340172, 0.31362299, 0.31385183, 0.31410506, 0.31470971,\n",
       "       0.31474685, 0.31606627, 0.31628787, 0.3165189 , 0.31691141,\n",
       "       0.31711742, 0.31787109, 0.3191704 , 0.31955154, 0.32019762,\n",
       "       0.32047015, 0.32234559, 0.32263437, 0.32388264, 0.3240724 ,\n",
       "       0.32415959, 0.32491949, 0.32504911, 0.32576048, 0.32628987,\n",
       "       0.32675987, 0.32676961, 0.32695558, 0.32826221, 0.32881858,\n",
       "       0.33030781, 0.33097694, 0.33278704, 0.33485632, 0.33657012,\n",
       "       0.3366979 , 0.33770067, 0.33772824, 0.33937936, 0.34059703,\n",
       "       0.34201449, 0.34201449, 0.34258309, 0.34270976, 0.3435679 ,\n",
       "       0.34365086, 0.34368412, 0.34396319, 0.34424276, 0.34725629,\n",
       "       0.34745442, 0.34759679, 0.34797266, 0.34931396, 0.34974604,\n",
       "       0.35014401, 0.3523915 , 0.35342043, 0.35475613, 0.35480829,\n",
       "       0.35692294, 0.35778698, 0.35883807, 0.35956611, 0.36076733,\n",
       "       0.36090711, 0.36223054, 0.36277884, 0.36427886, 0.36464182,\n",
       "       0.3656838 , 0.36915682, 0.36916669, 0.36944556, 0.37045361,\n",
       "       0.37124232, 0.37222947, 0.37246051, 0.37473295, 0.37590078,\n",
       "       0.37647556, 0.3785312 , 0.38074558, 0.38074558, 0.38106817,\n",
       "       0.38294032, 0.38330713, 0.38645282, 0.38658861, 0.38739757,\n",
       "       0.38914022, 0.38947831, 0.39065371, 0.39065731, 0.39248109,\n",
       "       0.39301651, 0.39325365, 0.39468744, 0.39596497, 0.39868444,\n",
       "       0.40145191, 0.40210522, 0.40219783, 0.40222876, 0.40311735,\n",
       "       0.40368127, 0.40388573, 0.40417469, 0.40662853, 0.40806479,\n",
       "       0.40812735, 0.41164539, 0.41339782, 0.41340748, 0.41632306,\n",
       "       0.4172043 , 0.417269  , 0.41786134, 0.41823751, 0.41832528,\n",
       "       0.42135339, 0.42250736, 0.42340714, 0.42341431, 0.42345112,\n",
       "       0.42492069, 0.4252607 , 0.4253492 , 0.42558176, 0.42681452,\n",
       "       0.42734174, 0.4285238 , 0.42865908, 0.42908997, 0.43231994,\n",
       "       0.43236087, 0.43311036, 0.43364912, 0.43403486, 0.43457784,\n",
       "       0.43641591, 0.4364635 , 0.43674932, 0.43693445, 0.43717427,\n",
       "       0.43814126, 0.44163962, 0.44238783, 0.44615488, 0.44630847,\n",
       "       0.44636155, 0.44997723, 0.45197698, 0.45461751, 0.45573847,\n",
       "       0.45637225, 0.45665096, 0.45674449, 0.45801141, 0.45866295,\n",
       "       0.45938689, 0.46274876, 0.46289106, 0.46463032, 0.46532701,\n",
       "       0.46698491, 0.46900063, 0.46927868, 0.47050006, 0.4706744 ,\n",
       "       0.47099214, 0.47231024, 0.47316277, 0.47362045, 0.47380178,\n",
       "       0.47483461, 0.47538298, 0.47605786, 0.48345764, 0.48379543,\n",
       "       0.48572486, 0.48608424, 0.48656386, 0.48779508, 0.49040046,\n",
       "       0.49433991, 0.49658484, 0.49799957, 0.50100009, 0.50169463,\n",
       "       0.50197682, 0.50275894, 0.50316597, 0.50599772, 0.50763714,\n",
       "       0.50873996, 0.51766124, 0.51896299, 0.51931578, 0.51946329,\n",
       "       0.51964744, 0.52061773, 0.52078469, 0.52420036, 0.52473401,\n",
       "       0.52769723, 0.52816925, 0.52826627, 0.53435258, 0.53471712,\n",
       "       0.53503891, 0.53516557, 0.53526988, 0.5395895 , 0.5430481 ,\n",
       "       0.54345403, 0.54364195, 0.54394195, 0.54446072, 0.54819886,\n",
       "       0.55221941, 0.55425822, 0.55651156, 0.55713981, 0.55713981,\n",
       "       0.55713981, 0.55713981, 0.55937648, 0.55953154, 0.56022074,\n",
       "       0.562201  , 0.56262987, 0.56497805, 0.56712355, 0.56773685,\n",
       "       0.5684331 , 0.56902858, 0.56975874, 0.57052667, 0.57177206,\n",
       "       0.57296696, 0.57387289, 0.57434217, 0.57459712, 0.57498937,\n",
       "       0.57632497, 0.5772049 , 0.57811433, 0.57915652, 0.57924155,\n",
       "       0.57938496, 0.57960526, 0.58077804, 0.58155701, 0.58184202,\n",
       "       0.58194423, 0.58285732, 0.58353303, 0.58411188, 0.58426232,\n",
       "       0.58483549, 0.58489462, 0.58492763, 0.58557324, 0.58574965,\n",
       "       0.58662816, 0.58669104, 0.58686393, 0.58711208, 0.58724735,\n",
       "       0.58745714, 0.58916123, 0.58924739, 0.58953028, 0.58969996,\n",
       "       0.58979823, 0.58993588, 0.59016007, 0.59024723, 0.59056136,\n",
       "       0.59058079, 0.59100415, 0.59132933, 0.59136644, 0.59168155,\n",
       "       0.59208726, 0.5925581 , 0.59265375, 0.59297742, 0.59301719,\n",
       "       0.59333787, 0.59344549, 0.59348494, 0.59350135, 0.59391524,\n",
       "       0.59420711, 0.5943282 , 0.59439991, 0.59445306, 0.59449285,\n",
       "       0.59468332, 0.59478413, 0.59484802, 0.59535395, 0.5953704 ,\n",
       "       0.59546682, 0.59558256, 0.59559649, 0.59586556, 0.59587736,\n",
       "       0.59600706, 0.59626496, 0.59642563, 0.59651898, 0.59654944,\n",
       "       0.59659429, 0.59670746, 0.59722868, 0.59724953, 0.59735165,\n",
       "       0.59744122, 0.59767431, 0.59778276, 0.59827992, 0.59835819,\n",
       "       0.59845644, 0.59858812, 0.59861667, 0.5989453 , 0.59898782,\n",
       "       0.59904065, 0.59920902, 0.59925823, 0.59949493, 0.59960255,\n",
       "       0.59964764, 0.59970026, 0.59994921, 0.59999061, 0.60070822,\n",
       "       0.60083072, 0.60087235, 0.60116484, 0.60151962, 0.60182405,\n",
       "       0.60216109, 0.60231715, 0.60238584, 0.60267546, 0.60295766,\n",
       "       0.60302033, 0.60311346, 0.60317113, 0.60338865, 0.60359637,\n",
       "       0.60414658, 0.60417474, 0.60423159, 0.60464779, 0.60470478,\n",
       "       0.60472254, 0.6050119 , 0.60508936, 0.60517311, 0.60524052,\n",
       "       0.6052662 , 0.60539681, 0.60547162, 0.60595083, 0.60606526,\n",
       "       0.60609718, 0.60610258, 0.60631853, 0.60660426, 0.60693419,\n",
       "       0.60702112, 0.60702152, 0.60712814, 0.60724399, 0.60742562,\n",
       "       0.60751242, 0.60754729, 0.60760677, 0.60804985, 0.6080941 ,\n",
       "       0.60815761, 0.60817031, 0.6085903 , 0.60871943, 0.60883582,\n",
       "       0.60884227, 0.6090395 , 0.60924093, 0.60940234, 0.60944426,\n",
       "       0.60945427, 0.60987094, 0.61002459, 0.61008427, 0.61013957,\n",
       "       0.6102212 , 0.61042352, 0.61044386, 0.61071951, 0.61072962,\n",
       "       0.61079096, 0.61082276, 0.61087635, 0.61105787, 0.61108179,\n",
       "       0.61114502, 0.61121769, 0.61130169, 0.6113294 , 0.61137894,\n",
       "       0.61137894, 0.61154942, 0.61156151, 0.6116976 , 0.61196253,\n",
       "       0.61205161, 0.61243292, 0.61247024, 0.61265191, 0.61270809,\n",
       "       0.61281137, 0.61282159, 0.61330038, 0.61337133, 0.6136343 ,\n",
       "       0.61373917, 0.61374855, 0.6137777 , 0.61380172, 0.61398216,\n",
       "       0.61519818, 0.61596933, 0.616133  , 0.61614625, 0.6161936 ,\n",
       "       0.61635543, 0.61645556, 0.61646733, 0.61654916, 0.61666782,\n",
       "       0.61669966, 0.61679626, 0.61700956, 0.61767956, 0.61784814,\n",
       "       0.61809157, 0.61846965, 0.61853303, 0.61866373, 0.61912186,\n",
       "       0.61927461, 0.6194242 , 0.61952805, 0.61957847, 0.6201609 ,\n",
       "       0.62041038, 0.62113907, 0.62120027, 0.62157555, 0.6215841 ,\n",
       "       0.62171428, 0.6219509 , 0.62259515, 0.62341043, 0.62368792,\n",
       "       0.62405424, 0.62499239, 0.62502632, 0.62516169, 0.62542184,\n",
       "       0.62552526, 0.62670995, 0.62710415, 0.62732843, 0.62752052,\n",
       "       0.62753248, 0.62826179, 0.62885748, 0.63008957, 0.63024161,\n",
       "       0.63371014, 0.63424071, 0.63438122, 0.63438122, 0.63438122,\n",
       "       0.63438122, 0.63438122, 0.63438122, 0.63438122, 0.63445863,\n",
       "       0.63450407, 0.63461787, 0.63470187, 0.63496908, 0.63889871,\n",
       "       0.64016471, 0.64182081, 0.64714655, 0.65172356, 0.65782506,\n",
       "       0.66201054, 0.66255408, 0.66631046, 0.66790865, 0.6697703 ,\n",
       "       0.6697703 , 0.68314195, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(jsds[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   3,   1,   1,  95,   1,   1,   1,   1,   1,   2,   1,  96,\n",
       "         1,   1,   1,   4,   1,   2,   1,   1,   1,   1,   2,   1,   3,\n",
       "         1,   1,   1,   2,   3,   1,   1,   2,   4,   2,   1,   1,   1,\n",
       "         2,   1,   2,   1,   1,   1,   1,   1,   1,   1,   4,   3,   1,\n",
       "         3,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n",
       "         1,   1,   6,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   1,   6,\n",
       "         2,   1,   1,   4,   1,   2,   1,   1,   1,   1,   1,   1,   4,\n",
       "         1,   1,   1,   1,   1,   1,   1,  76,   2,   1,   1,  29,   1,\n",
       "         1,   4,   5,   1,   1,   1,   1,   1,   2,   1,   1,   5,   7,\n",
       "         4,   1,   2,   1,   5,   1,   4,   1,   1,   1,   1,  20,   7,\n",
       "         5,   1,   1,   1,   1,   1,   1,   1,   5,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   4,   2,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   1,   1,   1,   2,   1,   1,   2,   1,   1,   1,\n",
       "         1,   1,   4,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n",
       "         1,   1,   2,   1,   1,   1,   1,   2,   1,   3,   1,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   4,\n",
       "         2,   1,   1,   3,   1,   1,   1,   1,   1,   1,   1,   1,   6,\n",
       "         2,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   2,   1,\n",
       "         1,   1,   1,   1,   3,   1,   1,   1,   1,   8,   1,   2,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         6,   1,   1,   6,   1,   1,   1,   1,   8,   1,   1,   1,   1,\n",
       "         1,   1,   1,   2,   1,   1,   1,   2,   1,   1,   1,   1,   1,\n",
       "        63,  47,   1,   2,   1,   1,   6,   1,   1,   1,   1,   1,   1,\n",
       "         3,   1,   1,   3,   1,   1,   1,   1,   1,   1,   3,   1,   3,\n",
       "         1,   1,   1,   3,   2,   2,   1,   2,   1,   1,   1,   1,   1,\n",
       "         3,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   2,\n",
       "         3,   1,   3,   1,   1,   1,   1,   5,   1,   1,   2,   4,   1,\n",
       "         1,   1,   1,   1,   1,   3,   1,   2,   1,   1,   1,   1,   1,\n",
       "         2,   1,   1,   1,   1,   1,   6,   1,   6,   1,   1,   1,   1,\n",
       "         1,   1,   1,   4,   1,   1,   1,   1,   1,   1,  73,  95,   1,\n",
       "         1,   5,   1,   1,   1,   1,  80,   1,   1,   1,   1,   1,   1,\n",
       "        45,  96,   1,   1,   1,   1,   1,   1,   4,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   8,  84,   8,   8,   8,   8,  96,   8,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,\n",
       "         2,   1,   2,   1,   1,   1,   1,   1,   6,   1,   1,   1,   1,\n",
       "         1,   1,   2,   2,   1,   1,   1,   4,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   7,   1,   2,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,  96,   4,   1,   4,   1,   1,  79,   1,   1,\n",
       "         1,   1,   1,   1,   2,   1,   1,   1,  70,  98,   1,   1,   1,\n",
       "         5,   2,  96,   1,   1,   1,  65,  48,   2,   1,  87,   1,   4,\n",
       "         1,   1,   1,   1,   1,  91,  67,  85,  86,   1,   3,   1,   2,\n",
       "         1,   1,   2,   3,   1,   1,   1,   1,   1,  85,  98,   7,  96,\n",
       "         1,   1,   1,   1,   1,   1,  94,  93,  38,  84,   5,   1,   1,\n",
       "         1,   1,  29, 100,   1,   1,   1,  96,  97,  79,  75,   1,   1,\n",
       "         2,   1,  95,  84,   1,   7,   1,  90,   1,   1,   1,  79,   3,\n",
       "         1,   6,   2,   1,   1,   1,   1,   1,   4,   1,   1,   1,   1,\n",
       "         1,   3,  96,  96,  91,  86,   2,   1,  98,  51,   1,   1,  71,\n",
       "        96,   4,   1,  74,  65,  96,   1,  96,   1,  51,  99,  95,  89,\n",
       "        86,  96,  80,  86,   1,   1,   1,  97,  86,   1,   1,   1,  98,\n",
       "         1,   4,  79,   1,   1,   1,   3,  98,  92,   1,   8,   1,   2,\n",
       "         1,   1,   4,   1,   1,  96,   1,   1,  90,   1,   1,  96,  75,\n",
       "        98,  78,   1,   1,  94,  75,  96,  90,   1,   1,  95,  86,   6,\n",
       "         1,   1,  83,  95,   1,   1,   1,   1,   1,   1,  62,  98,  75,\n",
       "         1,   1,  63,  79,  96,  91,  98,   6,   1,   1,   1,   1,   3,\n",
       "         7,   2,  96,  88,  66,  96,  88,  96,  72,  94,  54,  67,  84,\n",
       "        62,  98,  84,  54,  85,   2,  95,  61,   1,  96,  78,  95,  49,\n",
       "         1,   5,  62,  96,  96,  84,  90,  96,   1,  96,  84,  96,  83,\n",
       "         1,  98,  86,  91,  89,   1,   1,   1,   1,   1,   2,  98,  88,\n",
       "         1,   1,   1,  94,  86,  76,  96,  98,  75,  92,   1,  94,   1,\n",
       "         1,   5,   1,   1,   1,   6,  52,  96,  91,  96,  96,  34,   1,\n",
       "         1,   1,   1,   1,   1,  97,  51,   1,   1,   1,   1,   1,  74,\n",
       "        99,  88,  87,   2,  70,   1,  96,   1,   1,  84,  97,  93,  90,\n",
       "        98,  98,  92,  85,   2,   1,   1,   3,  81,  96,   1,   1,  98,\n",
       "        90,  92,  99,   1,   1,  98,  77,   1,   1,  92,   1,  82,   1,\n",
       "        97,  81,  91,  98,  98,  93,   1,   1,  62,  97,   1,   1,   1,\n",
       "         1,   6,   1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   2,\n",
       "         3,   1,   1,   1,   1,  26,  86,  96,  84,   3,   1,   1,   6,\n",
       "         1,   1,   1,   1, 100,  83,   2,   1,   1,   1,   1,   1,   1,\n",
       "         1,  92,  98,  98,  71,   1,   1,   1,   2,   1,   1,  97,  93,\n",
       "         1,   1,   1,   1,   1,   1,  92,  98,   1,   2,   1,   1,   1,\n",
       "         1,  89,  94,   1,   1,  96,  94,   1,   1,   1,   1,   1,   1,\n",
       "         1,   5,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jsds < 0.01).sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
