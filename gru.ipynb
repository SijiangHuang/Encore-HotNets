{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 4096*2\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 4, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 4).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18490910, 2120192, 20611102)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.901224444444445"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951005 951004\n"
     ]
    }
   ],
   "source": [
    "gru=torch.load('model/gru-0918.pth')\n",
    "s2h=torch.load('model/s2h-0918.pth')\n",
    "save_dict = torch.load(\"model/save_dict-0918.pth\")\n",
    "optimizer.load_state_dict(save_dict['optimizer'])\n",
    "scheduler._step_count = save_dict[\"_step_count\"]\n",
    "scheduler.last_epoch = save_dict[\"last_epoch\"]\n",
    "print(save_dict[\"_step_count\"],save_dict[\"last_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：18490910\n",
      "总参数数量和：2120192\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.35683920979499817 0.36359281769394874 952006 [1e-05, 1e-05] 24.944279193878174\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m sum_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m seq_tensor, size_tensor, target_tensor \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     seq_tensor \u001b[39m=\u001b[39m inputTensor(seq_tensor)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     size_tensor \u001b[39m=\u001b[39m size_tensor\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     target_tensor \u001b[39m=\u001b[39m target_tensor\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lines\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         size \u001b[39m=\u001b[39m lines[line][i]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         tensor[i][line][size] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-0918.pth')\n",
    "        torch.save(s2h, 'model/s2h-0918.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-0918.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7, 4] False\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 11, 10, 7, 7, 4, 6, 7, 6, 6, 4, 13, 10, 8, 8] False\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 6, 7, 11, 10, 4, 10, 4, 11, 4, 4, 13, 7, 13, 16, 4] False\n",
      "[8, 4, 4, 12, 10, 7, 6, 4, 4, 11, 13, 7, 4, 8, 4, 8] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6, 4] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 7, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6] False\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 13, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 4, 4, 8, 13, 7, 4, 4, 4, 15] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 4, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 9, 10, 8, 4] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 8, 4, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] False\n",
      "[8, 11, 4, 4, 13, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8] False\n",
      "[8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4, 8] False\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 13, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] False\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 11, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4] False\n",
      "[8, 4, 6, 8, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] False\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 11, 6, 4, 13, 6, 6, 4, 13, 10, 8, 8, 7, 11, 7, 4] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 11, 11, 13, 2, 11] False\n",
      "[8, 6, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13] False\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8] False\n",
      "[8, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 11, 7, 7, 11, 11, 9, 10, 8, 4, 7, 7, 10, 4, 9, 7] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 4, 13, 7, 13, 16, 4, 4] False\n",
      "[8, 4, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 9, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 6, 8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.004414488112969265 0.034302756408307766 0.0736751094920089\n",
      "1 0.0023151405689602533 0.04677924858266993 0.1042314247342685\n",
      "2 0.004016498523262616 0.04329401650688337 0.09241951357861278\n",
      "3 0.004653925548171213 0.03295832917210835 0.08037825775341918\n",
      "4 0.004626554915476465 0.050363960085265363 0.11541280624370863\n",
      "5 0.002609153278317937 0.013915063089461402 0.04631082202975206\n",
      "6 0.004181433305232662 0.051854423302887045 0.12974426772181177\n",
      "7 0.0026585040951118077 0.03720006500578517 0.11718434610348075\n",
      "8 0.0029053382090303173 0.055384820511902556 0.13076504713529877\n",
      "9 0.0034291492353181632 0.038789810747664716 0.09984615243879238\n",
      "10 0.0012153495036881868 0.037017692820920725 0.08451325312891833\n",
      "11 0.004757813091004671 0.06624532166008394 0.13889262629016758\n",
      "12 0.004449881880384211 0.035291832608145855 0.09738875946067597\n",
      "13 0.004613815946367081 0.03702638774423582 0.1098784278877494\n",
      "14 0.0028375727473991083 0.029673180537628692 0.09919430254377648\n",
      "15 0.004190337633501887 0.055377933783825134 0.10702417609416995\n",
      "16 0.0049096658713356275 0.027021991556027553 0.08211342620477911\n",
      "17 0.0036432312755088846 0.056362682337658884 0.11824545060157078\n",
      "18 0.0027568116930494586 0.033700688294105885 0.09505802848246517\n",
      "19 0.0048772823452274475 0.030862622100513156 0.10725497113284456\n",
      "20 0.0022460607296289166 0.039942207564809484 0.1037537812791009\n",
      "21 0.003411988082721144 0.03622508247017717 0.10996115827061187\n",
      "22 0.0033580945295898965 0.0325865451457089 0.11219160432682712\n",
      "23 0.004866342120831732 0.0473397446126753 0.12260792078861099\n",
      "24 0.0028984084100203513 0.050394385496679996 0.09973637731741375\n",
      "25 0.004620044066298854 0.056475910531147276 0.12676180410838037\n",
      "26 0.004902698015895618 0.04396505493090323 0.09891848859700221\n",
      "27 0.004499668410812705 0.025256274594233404 0.0730846074721768\n",
      "28 0.003903698689208712 0.047365415833734714 0.1104124405284788\n",
      "29 0.004301803575008105 0.06063479714040315 0.13541102398212612\n",
      "30 0.004114318570410424 0.02765451805076006 0.08280698660367523\n",
      "31 0.003328968341431855 0.020173334582115038 0.04868861446856464\n",
      "32 0.003561012255282995 0.052165458486111316 0.11417219516820398\n",
      "33 0.0018880343007944317 0.040337098032502315 0.08666723835483647\n",
      "34 0.004116784918609891 0.04050507121574984 0.1139012105484031\n",
      "35 0.0024296684518738893 0.03081089813385031 0.09665539912796595\n",
      "36 0.0041910425967022265 0.053316515281116306 0.11453779848575152\n",
      "37 0.002241261844951079 0.042068507166845244 0.08568096205518708\n",
      "38 0.004248069422025524 0.024462410094038028 0.07383516639308763\n",
      "39 0.00462467993390965 0.04952719388636505 0.11519057774560024\n",
      "40 0.004165043536809736 0.0373713106741063 0.11015979719711982\n",
      "41 0.004579854927275033 0.04332355680149902 0.09759859608355356\n",
      "42 0.0032248595135026294 0.02678795470102189 0.08173421960100616\n",
      "43 0.004335957060717163 0.045455071592564254 0.09813521422555554\n",
      "44 0.001995804249642242 0.033154049764127246 0.10673038793264576\n",
      "45 0.004260727766969986 0.03935242351000859 0.09463715227825792\n",
      "46 0.002809389551228847 0.04923703309685813 0.10131811258050497\n",
      "47 0.0028902464704715497 0.028169438436378487 0.09352312731515616\n",
      "48 0.004462557372008746 0.04709987309823674 0.11032567787826482\n",
      "49 0.0020316022265258506 0.032828536968253 0.08967242311529415\n",
      "50 0.0022786874266998007 0.050860114187376876 0.09591794513505242\n",
      "51 0.0049180115234780004 0.05031498017872981 0.12283278093789532\n",
      "52 0.0038240879406826796 0.0496142825633438 0.10298109460560273\n",
      "53 0.0034991618839693714 0.03325048990425503 0.09544627687968442\n",
      "54 0.004430188377287924 0.04295545251600319 0.11421734937305407\n",
      "55 0.004106323710782239 0.053016144571814625 0.11405751110794628\n",
      "56 0.004894023964586535 0.04086012597214618 0.10315916506533149\n",
      "57 0.0031713566530771247 0.02629349964162799 0.08492055734442056\n",
      "58 0.002966219094227337 0.04673258184183131 0.13174019180173754\n",
      "59 0.002039475479490227 0.041751876850162926 0.0829196744059342\n",
      "60 0.003820538228993748 0.03657271303187317 0.12374891582389298\n",
      "61 0.002904519478832429 0.02585669436699055 0.07898722130023553\n",
      "62 0.0035179629362072137 0.03480266141259676 0.09239453235357423\n",
      "63 0.0026346756063823223 0.054800957367974856 0.1055014573275081\n",
      "64 0.0025278211331181627 0.01388666785093304 0.05288173229587856\n",
      "65 0.0022831718736578853 0.0474484036635142 0.08960023315334897\n",
      "66 0.0025202390156130908 0.0417624193335322 0.09539199109774618\n",
      "67 0.004926648459553039 0.04443379797354437 0.08935499447206391\n",
      "68 0.00470204186105825 0.050773945379002566 0.11178799729491165\n",
      "69 0.002788884420668324 0.03590471874479066 0.09782239060365072\n",
      "70 0.003871480349648283 0.03484805126420243 0.08221672556723518\n",
      "71 0.00451017902620162 0.04544083138497408 0.09208198734110978\n",
      "72 0.0035034716935655047 0.03464571029305537 0.08774217028977943\n",
      "73 0.0039375626515971065 0.04562934890522966 0.10342577439503106\n",
      "74 0.0046396407507582685 0.06015471730283134 0.11966501316647711\n",
      "75 0.004816217733889948 0.047499871808939 0.129072299761036\n",
      "76 0.0036282034292745087 0.03226259626746082 0.08164650808916038\n",
      "77 0.0032870463523791667 0.036108524426948986 0.10565417187332546\n",
      "78 0.0033629446503449824 0.033823770863226243 0.10872402078336675\n",
      "79 0.004726937433224186 0.04815754904640057 0.10489527652934064\n",
      "80 0.0038224583584796023 0.045281730617385156 0.1301729265588225\n",
      "81 0.002609060714029525 0.02565227925324519 0.09893653787270035\n",
      "82 0.003397010098262738 0.05012749783399349 0.12595795936511203\n",
      "83 0.004043089767429422 0.03427999424205473 0.0837604983927544\n",
      "84 0.0016001026949867224 0.03403934852302472 0.08834685536174339\n",
      "85 0.004144093186522084 0.02924405909997424 0.11699409538241046\n",
      "86 0.0023046858204829658 0.048671890700681404 0.1175586511420501\n",
      "87 0.004323397912660158 0.03825155093328278 0.10991133461785622\n",
      "88 0.004497831924338216 0.056791289782395026 0.10558963026951812\n",
      "89 0.004019990323175756 0.035856280621400616 0.10180175504951613\n",
      "90 0.0026379628265454803 0.03769365505694726 0.09215587187803512\n",
      "91 0.004423245557767436 0.040698818282637446 0.1022206765504856\n",
      "92 0.0036724377996371523 0.02678338971002621 0.06611252741223159\n",
      "93 0.003090000332446965 0.03551532488912688 0.08674519970533176\n",
      "94 0.0031729014998131727 0.03173872040397127 0.08863416448218087\n",
      "95 0.0033594018558012195 0.03856203093295004 0.1165412214199228\n",
      "96 0.0038636734293087185 0.035453199337502 0.08983328362148077\n",
      "97 0.004908606791316215 0.0666510403713021 0.10891586298682547\n",
      "98 0.003362854200264685 0.030551335139032132 0.11408086528523712\n",
      "99 0.0022713249325168024 0.0419867735633845 0.10379974340790225\n",
      "100 0.004480542306244808 0.062077499210896336 0.14448147730158845\n",
      "101 0.004699568957177538 0.05268532451255814 0.10413461725153723\n",
      "102 0.004758512655162462 0.039822129287402724 0.08852600013635087\n",
      "103 0.008497526174800065 0.06663948653683932 0.11672536777996693\n",
      "104 0.002391148637901112 0.0436585139642541 0.10170668247132991\n",
      "105 0.0026034167499237783 0.05825623906744984 0.10202043154201505\n",
      "106 0.0030654515224252357 0.03843510882265793 0.12243350086754246\n",
      "107 0.0034031663996790993 0.04864927302482202 0.12065236187576686\n",
      "108 0.00360679197986132 0.03973562306840965 0.1142268612149248\n",
      "109 0.0023002659535436 0.03853474581885802 0.10615126756144289\n",
      "110 0.002990858569475673 0.03154371084581287 0.10128003927610213\n",
      "111 0.00277987950653637 0.021371388211636512 0.07256352029682542\n",
      "112 0.003952071138712935 0.041335537244253776 0.12788513309612023\n",
      "113 0.0013871157824005206 0.034081872055977416 0.12638875080914658\n",
      "114 0.0029130261812516612 0.05377655841006847 0.13180672642602323\n",
      "115 0.00458671358608425 0.04180408900653888 0.10652496228845723\n",
      "116 0.004188707272183296 0.050979309894322054 0.11258895932137199\n",
      "117 0.004328954825558073 0.03821071460242565 0.09995493399080008\n",
      "118 0.0038986200349943544 0.03356508880044574 0.08121152664316994\n",
      "119 0.00469623480777282 0.05754516215250169 0.11625812860442807\n",
      "120 0.003102798622204674 0.04156745733668372 0.10294816240320928\n",
      "121 0.0033058860528600922 0.04208963954037492 0.1007969202849352\n",
      "122 0.004725084552752751 0.03184474618810782 0.07073966493734425\n",
      "123 0.0041718686592332275 0.032998874027974995 0.07782755384879206\n",
      "124 0.0036613301143785457 0.017694760486133467 0.06420187588269041\n",
      "125 0.0032949896235786613 0.05068248732106355 0.10230463796025266\n",
      "126 0.004435282734867414 0.04885591325546371 0.1163588196417264\n",
      "127 0.00401054229359502 0.03945554705246007 0.08360067206775501\n",
      "128 0.0043186830367384094 0.04825672893885553 0.12537942197044644\n",
      "129 0.004754280794385718 0.03651303085055234 0.0880989591230215\n",
      "130 0.004180761130285684 0.044405825661409125 0.10037983821453339\n",
      "131 0.004498806082794034 0.050422708014957726 0.12054625282329907\n",
      "132 0.0038492105142257137 0.036168951839416014 0.09297148849780391\n",
      "133 0.0013091359091366014 0.0324691051833079 0.11661449737791828\n",
      "134 0.0023587258782093003 0.05328467327377012 0.10716850891620353\n",
      "135 0.0038171221561576225 0.043235826821834314 0.0918415288049867\n",
      "136 0.00477074931845035 0.03781382856841701 0.1065321767746078\n",
      "137 0.004852301556933138 0.05704570885854876 0.10609606676928945\n",
      "138 0.0035993443862345786 0.053361635456877964 0.10434777736504153\n",
      "139 0.0039029297095818623 0.045583574598830806 0.09060817698733892\n",
      "140 0.004113211208188002 0.049604654964658434 0.10282209893173183\n",
      "141 0.0044865139779433055 0.04913521604421331 0.11259642168878181\n",
      "142 0.004403175747406462 0.03190627235807586 0.09193579208817079\n",
      "143 0.004504590783881181 0.05679920011937012 0.12254235741535972\n",
      "144 0.004862986779423216 0.039102932798898093 0.09869780050820959\n",
      "145 0.0023996978093981007 0.032752340012777624 0.10011380374729793\n",
      "146 0.003453497174408971 0.04440516876317259 0.09738797675314378\n",
      "147 0.004949684805300271 0.06448740531916825 0.12997263089184097\n",
      "148 0.004481350741068323 0.05065888106514386 0.0914952724087226\n",
      "149 0.003937785654574785 0.043546373294906154 0.0822411000210086\n",
      "150 0.003532777760976902 0.022519235701663417 0.09900538210233237\n",
      "151 0.003906349165882691 0.04959928639537657 0.09353343984330284\n",
      "152 0.004108587557900268 0.04665879688406502 0.11500151086600185\n",
      "153 0.0037593145822455926 0.048305225019098806 0.09942936495004198\n",
      "154 0.004796149855563701 0.0481906798174117 0.10527587627351552\n",
      "155 0.0031116177404574594 0.024803533138489486 0.08157529893210924\n",
      "156 0.004292869488859497 0.05281133167544698 0.10734611857972207\n",
      "157 0.0037212085563041435 0.04328288437022295 0.10634811159356042\n",
      "158 0.0035424393443190564 0.03557353554873055 0.08620933821924906\n",
      "159 0.0044169602259696615 0.03539368191992286 0.11268500790480665\n",
      "160 0.0030470952478960692 0.03786949572569973 0.11145145246753675\n",
      "161 0.004818211778853422 0.037717374445607355 0.10174101580966444\n",
      "162 0.0036288783506625132 0.03986400443612259 0.10938622962885272\n",
      "163 0.0037166002209932394 0.06198795712784473 0.14632489016400932\n",
      "164 0.00353067905178813 0.042390529085437216 0.12467210253729477\n",
      "165 0.003292427611851093 0.049793750634218134 0.14427100591157815\n",
      "166 0.002929923331878469 0.04121053316647928 0.09392474762174316\n",
      "167 0.004734647930832824 0.046500064601416355 0.13077286677526326\n",
      "168 0.0032079989511588176 0.03921637609540814 0.09132100516134187\n",
      "169 0.0026445047893629593 0.051037490566283775 0.09208460480028274\n",
      "170 0.00365325415250116 0.03926117198672946 0.11461953957463303\n",
      "171 0.0025986794329619593 0.03205170896199419 0.0897149320723227\n",
      "172 0.0032582170657094586 0.028594547070067415 0.0992667521589993\n",
      "173 0.004637338795166736 0.0584029134599682 0.1093718026187037\n",
      "174 0.004665715386583787 0.04277587114770147 0.11075531861558045\n",
      "175 0.0030918734117393876 0.029101978554496845 0.08001278208611441\n",
      "176 0.004221279236447663 0.02642065502191556 0.07054170276594847\n",
      "177 0.004134062529099706 0.04993489688600633 0.08820582594460868\n",
      "178 0.003992947917958259 0.04573014793971497 0.1120705454007988\n",
      "179 0.004306508063607901 0.05199693995844404 0.1080118015119227\n",
      "180 0.004759551647989621 0.04223340163083368 0.09521186218857755\n",
      "181 0.003207210006573143 0.03635672043642335 0.10944759510605569\n",
      "182 0.0038025694189654726 0.0320238427280463 0.08533517821657335\n",
      "183 0.0021175626787654483 0.03711408082447569 0.1127836210992387\n",
      "184 0.0026022860118802577 0.024884636243301957 0.09022935214861721\n",
      "185 0.0024339961979977717 0.05298481710512256 0.12375833245417371\n",
      "186 0.0038060063804688897 0.0344672658727046 0.1054124441196144\n",
      "187 0.0032653547843196472 0.05065117839201749 0.09768022310704089\n",
      "188 0.004870555703961931 0.03538349647161927 0.09161891849059485\n",
      "189 0.004732568442526615 0.03720034890872545 0.07968903372599809\n",
      "190 0.0031063188999865647 0.03792900818965971 0.12658254036092556\n",
      "191 0.0027908652377986445 0.041002774057573736 0.0899953611319127\n",
      "192 0.004671231234485731 0.04187656228376263 0.10093001003857738\n",
      "193 0.002868916471793638 0.03510549631447197 0.10880327584383938\n",
      "194 0.00411750524555415 0.05362344555775193 0.12448364366469017\n",
      "195 0.003331107210176569 0.03727987837988261 0.10649152646989012\n",
      "196 0.0033339634446343056 0.05004246996050692 0.11608650708681037\n",
      "197 0.003950773557307442 0.02305646327007837 0.08968739191306596\n",
      "198 0.0035087356571408485 0.04920168042267138 0.1162532807086241\n",
      "199 0.003152417384166607 0.05274945462820002 0.10069728110252138\n",
      "200 0.003972181985147824 0.05312698812792735 0.11413526899736937\n",
      "201 0.004848683683052549 0.04764990898394451 0.08522039375358235\n",
      "202 0.0038660388543490585 0.045617066480687946 0.1010967865078123\n",
      "203 0.004644484103002531 0.05177417628732667 0.08921113159783448\n",
      "204 0.0024212310567970057 0.044003084748405946 0.11280835998862546\n",
      "205 0.003001391671628332 0.044214716370302165 0.11463749546633542\n",
      "206 0.004598223073082606 0.05723021523727835 0.13238117227232046\n",
      "207 0.003326789383721168 0.05175546819356433 0.09998269234763446\n",
      "208 0.0030824593822322043 0.04489338650778671 0.11435944006259378\n",
      "209 0.0037535975460319757 0.033698123519695575 0.09602613714513193\n",
      "210 0.004766787523563218 0.0469876404431341 0.12544362373967938\n",
      "211 0.003844785467830991 0.046469948208247326 0.11274196769023648\n",
      "212 0.0027983885458542964 0.033471883078631116 0.12394013763312048\n",
      "213 0.004234360867920221 0.035922548519681026 0.10624420725688354\n",
      "214 0.004287855714454048 0.05781872231408496 0.1265818642438945\n",
      "215 0.0038869400740453577 0.060755655995334876 0.12645355468009517\n",
      "216 0.0034924938299089187 0.04437007540135024 0.1024581709198151\n",
      "217 0.0042678144120927184 0.05005899455925034 0.09998809311885062\n",
      "218 0.004645784876422928 0.06227252746833313 0.13390407879601338\n",
      "219 0.004543618527797985 0.06393299312694234 0.12267791169516525\n",
      "220 0.004598055066184175 0.046779394970428566 0.09895392217448609\n",
      "221 0.0030356828153275204 0.040298719354340984 0.11775318526652442\n",
      "222 0.004695147544108508 0.05156241550829804 0.10627207389423671\n",
      "223 0.0024474415135688093 0.028154491080669218 0.09168893595946148\n",
      "224 0.0036661172784930927 0.049845408252110264 0.1259548317234055\n",
      "225 0.004604692716716778 0.04609617703776156 0.12008233799634718\n",
      "226 0.004181455121682066 0.045816024523007925 0.11505206672019919\n",
      "227 0.0026281592465667246 0.03802131472659287 0.08277873521613509\n",
      "228 0.0030509969653241356 0.046368338602059106 0.11370981198412405\n",
      "229 0.0039506263689922955 0.05135517630740204 0.12074665604755051\n",
      "230 0.0032780056266554216 0.03354752376278328 0.09502743041254137\n",
      "231 0.004694598116291699 0.03428271559846076 0.10850287193337022\n",
      "232 0.0061473647135434776 0.02695464452198487 0.06252579108052168\n",
      "233 0.004961127697216513 0.0605977131149469 0.09784718333604112\n",
      "234 0.004521196104579951 0.03439134879598781 0.07664730343177653\n",
      "235 0.0035222209080269904 0.04243427663986139 0.10928412620186051\n",
      "236 0.002893776248348705 0.0431404790118807 0.1274398264630219\n",
      "237 0.0028112484467199346 0.04133698227738722 0.1112194297036622\n",
      "238 0.003287787549849235 0.056056521592337406 0.11220222824896553\n",
      "239 0.003513922129320287 0.025723430451930998 0.06549787907242777\n",
      "240 0.004626438832791435 0.02457275479507432 0.07349260550374623\n",
      "241 0.004256089264249349 0.044782033101458625 0.08724867129597366\n",
      "242 0.003493852729112775 0.05811237695298521 0.12891516880815737\n",
      "243 0.002338192599738167 0.02094347012334139 0.09631207971556355\n",
      "244 0.0030221657672973613 0.047942613115585904 0.12342855772121564\n",
      "245 0.003427220331753588 0.04404251945719642 0.10998327640011821\n",
      "246 0.003011001886883297 0.025649854936448918 0.09456617890302374\n",
      "247 0.003357873714960581 0.03393492236810612 0.10364469128371048\n",
      "248 0.002954689131249635 0.03521452572234775 0.10256951011243054\n",
      "249 0.00452808581162323 0.050030901562282716 0.12559425929999257\n",
      "250 0.0020766163985793482 0.035148780688852035 0.08795897324555735\n",
      "251 0.0020204292371075773 0.029452029135857226 0.09352315539773254\n",
      "252 0.004467910994900653 0.04788089019511052 0.09090873274665354\n",
      "253 0.0021297086456715623 0.0420157475397516 0.12274072617518009\n",
      "254 0.0024563424556003728 0.023545086025999164 0.0994207192438596\n",
      "255 0.004196634849721643 0.04663257478067981 0.08524788952982967\n",
      "256 0.00366691516215888 0.0378908574516266 0.10640167797327288\n",
      "257 0.003051507354784176 0.03736832010152688 0.1003323008937009\n",
      "258 0.00316848438545372 0.03487786653057058 0.10509251387409\n",
      "259 0.004467025421604145 0.03607888722703971 0.07691303424205301\n",
      "260 0.0032659153363175817 0.03314213977673687 0.09295846627186546\n",
      "261 0.004077116637972729 0.03424107916871366 0.10745140759019495\n",
      "262 0.004473668269836365 0.04742817802606021 0.1130272518137353\n",
      "263 0.0021930696056241816 0.027853778211088715 0.09657957396920758\n",
      "264 0.001262077078420017 0.027625072048887308 0.09222857944314894\n",
      "265 0.0037865689658571917 0.05046340759896524 0.10329143206573264\n",
      "266 0.0033615645738166826 0.06263744407277215 0.13007728955365466\n",
      "267 0.0027943117385868053 0.037652060975501014 0.10540567271315979\n",
      "268 0.004510414212615316 0.03972779768048436 0.12117776391769042\n",
      "269 0.002885258299098818 0.04403841371796205 0.09542130064176199\n",
      "270 0.004449614127096754 0.0621886266468886 0.12690448473814386\n",
      "271 0.004363078811343412 0.05197186780013467 0.10419066143159783\n",
      "272 0.0036753989188028605 0.047936662069839125 0.1089261638961837\n",
      "273 0.0018107488872668174 0.03276097256197341 0.10829117481273301\n",
      "274 0.003908898559849142 0.03974808542138322 0.12070224607594383\n",
      "275 0.004247438686963893 0.05716966201659641 0.14242968612552628\n",
      "276 0.0048320884523920995 0.04861337426650978 0.10128128990100498\n",
      "277 0.0038629116566635553 0.06154953273267269 0.1144734273391986\n",
      "278 0.004098826476294458 0.048054534543143165 0.09764734167751767\n",
      "279 0.004773153871289485 0.040386416611236875 0.09730834178869202\n",
      "280 0.003466924148459929 0.0182378372667873 0.06709648055331469\n",
      "281 0.002540167298003268 0.045380211241925095 0.10071732127715921\n",
      "282 0.0036296211065585744 0.029649773438716522 0.07958102755906199\n",
      "283 0.004944697166671962 0.04570817306749245 0.1246469580959138\n",
      "284 0.004724256112093225 0.06027709974763182 0.12926705817746248\n",
      "285 0.003317356210801002 0.05830565660857148 0.09811081316973778\n",
      "286 0.0021938323312079263 0.04132315826144767 0.117841334479689\n",
      "287 0.0029488754467758556 0.04609978123596965 0.094140735102795\n",
      "288 0.00416282260793251 0.0405455514342809 0.10111169362392305\n",
      "289 0.00465360555837702 0.0392941173652611 0.10919202339480627\n",
      "290 0.003637411361233696 0.0482857868395286 0.11177797845128236\n",
      "291 0.0034244266188012892 0.0338893582978971 0.08498675612554116\n",
      "292 0.0035052668103683635 0.0327463479426893 0.10178029982010695\n",
      "293 0.0048586784114403 0.051638719380938 0.11916860697416108\n",
      "294 0.004609679550457848 0.08339756402676804 0.13152546467307263\n",
      "295 0.0035786186736417066 0.02765245919363787 0.08941427359231997\n",
      "296 0.004823224579430436 0.0500268363569152 0.1035479957497292\n",
      "297 0.0038067832973109547 0.040004552848133465 0.09654301544906133\n",
      "298 0.00450540057026475 0.041344597026742216 0.11368192613963335\n",
      "299 0.0037706548791714214 0.035796399003645826 0.09668639999077855\n",
      "300 0.003891805923143714 0.04186049419216065 0.1003699573182019\n",
      "301 0.0036287949834953133 0.061479557515105754 0.1334201716741527\n",
      "302 0.004468217800077882 0.03242451147074829 0.12498732577704316\n",
      "303 0.003856887607647053 0.04168950276778646 0.1008838933965999\n",
      "304 0.004882563191128329 0.0736603910798989 0.13589854452381722\n",
      "305 0.004211020556399793 0.0459575562683811 0.09163576712424899\n",
      "306 0.0021756762855232206 0.021810665748656245 0.0683508570279329\n",
      "307 0.003315659013438877 0.04305375225307394 0.13019245612986152\n",
      "308 0.003338125455776855 0.040804405721478754 0.09353135973878719\n",
      "309 0.003941059455976016 0.044954909839655374 0.11015514030459167\n",
      "310 0.003605333557241194 0.042833886897370485 0.1049541110679303\n",
      "311 0.00280705608674841 0.035603395754334216 0.10009854398900911\n",
      "312 0.0024374501867724914 0.04475720826383939 0.10955659359844727\n",
      "313 0.003635752070227524 0.047659092132714576 0.12132492066986807\n",
      "314 0.00260788436806532 0.026543113908604837 0.07148901873030042\n",
      "315 0.004110723845155959 0.022977292019794517 0.06927549919376928\n",
      "316 0.00419777185345073 0.04111941337503474 0.12351848630774752\n",
      "317 0.0031725978084652757 0.03921156815635553 0.0877212047374474\n",
      "318 0.004598675602934286 0.044627622533382746 0.12225630745561429\n",
      "319 0.004624923052193575 0.04489519659065956 0.10855176717383548\n",
      "320 0.004480832227067041 0.0686964555951519 0.13093308663705333\n",
      "321 0.004282653364548805 0.057965374827596644 0.12422431344518942\n",
      "322 0.0038547748941740317 0.053850606515056965 0.13188063517959087\n",
      "323 0.003367310576735106 0.038957238006490834 0.08327823704987844\n",
      "324 0.00417854075923638 0.05482148766037734 0.0981362084624664\n",
      "325 0.004976013037509718 0.05905341684717068 0.11594091084301665\n",
      "326 0.003987024331998391 0.028333038652552094 0.08140185639989239\n",
      "327 0.0029723249386243208 0.03255169173742545 0.10662404061483288\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, seq_len, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m inputTensor(np\u001b[39m.\u001b[39marray([[size]]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39;49m, hn)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m i\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mfor\u001b[39;00m o \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mots:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m         out_\u001b[39m=\u001b[39mout\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mModuleList.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m@_copy_to_script_wrapper\u001b[39m\n\u001b[1;32m    218\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Module]:\n\u001b[0;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39miter\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_modules\u001b[39m.\u001b[39;49mvalues())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(500):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
