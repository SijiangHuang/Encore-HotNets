{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 4096*2\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1691005 1691004\n"
     ]
    }
   ],
   "source": [
    "gru=torch.load('model/gru-0914.pth')\n",
    "s2h=torch.load('model/s2h-0914.pth')\n",
    "save_dict = torch.load(\"model/save_dict-0914.pth\")\n",
    "optimizer.load_state_dict(save_dict['optimizer'])\n",
    "scheduler._step_count = save_dict[\"_step_count\"]\n",
    "scheduler.last_epoch = save_dict[\"last_epoch\"]\n",
    "print(save_dict[\"_step_count\"],save_dict[\"last_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "1.0000000000000002e-06\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.5783600211143494 0.5965690606534481 1692006 [1.0000000000000002e-06, 1.0000000000000002e-06] 22.259061336517334\n",
      "2000 0.589902937412262 0.5965112723708152 1693006 [1.0000000000000002e-06, 1.0000000000000002e-06] 45.41776728630066\n",
      "3000 0.6609735488891602 0.5953420600295067 1694006 [1.0000000000000002e-06, 1.0000000000000002e-06] 68.73607850074768\n",
      "4000 0.5708510279655457 0.5963286194205284 1695006 [1.0000000000000002e-06, 1.0000000000000002e-06] 92.09629917144775\n",
      "5000 0.5507369637489319 0.5951172899603844 1696006 [1.0000000000000002e-06, 1.0000000000000002e-06] 115.4458839893341\n",
      "6000 0.6801291704177856 0.5960663316845893 1697006 [1.0000000000000002e-06, 1.0000000000000002e-06] 138.36706829071045\n",
      "7000 0.602750837802887 0.5944200969338417 1698006 [1.0000000000000002e-06, 1.0000000000000002e-06] 160.93720293045044\n",
      "8000 0.5433800220489502 0.5945956032276154 1699006 [1.0000000000000002e-06, 1.0000000000000002e-06] 183.6121642589569\n",
      "9000 0.6033045053482056 0.5960682318210602 1700006 [1.0000000000000002e-06, 1.0000000000000002e-06] 206.89413475990295\n",
      "10000 0.5750752091407776 0.596634937107563 1701006 [1.0000000000000002e-06, 1.0000000000000002e-06] 230.1563732624054\n",
      "11000 0.6020897626876831 0.5955727683901787 1702006 [1.0000000000000002e-06, 1.0000000000000002e-06] 253.58570265769958\n",
      "12000 0.6074001789093018 0.5949631798267364 1703006 [1.0000000000000002e-06, 1.0000000000000002e-06] 276.96214962005615\n",
      "13000 0.6408206820487976 0.5950076165795326 1704006 [1.0000000000000002e-06, 1.0000000000000002e-06] 300.26193618774414\n",
      "14000 0.6877577900886536 0.5972267038226128 1705006 [1.0000000000000002e-06, 1.0000000000000002e-06] 323.67949748039246\n",
      "15000 0.5771198272705078 0.596115678191185 1706006 [1.0000000000000002e-06, 1.0000000000000002e-06] 346.9251880645752\n",
      "16000 0.6160417199134827 0.5954210886955261 1707006 [1.0000000000000002e-06, 1.0000000000000002e-06] 370.15836000442505\n",
      "17000 0.5986939668655396 0.5950003370046616 1708006 [1.0000000000000002e-06, 1.0000000000000002e-06] 393.3542101383209\n",
      "18000 0.6511167883872986 0.593996712654829 1709006 [1.0000000000000002e-06, 1.0000000000000002e-06] 416.7484178543091\n",
      "19000 0.6520320773124695 0.5954551596045494 1710006 [1.0000000000000002e-06, 1.0000000000000002e-06] 439.95358061790466\n",
      "20000 0.5660642385482788 0.5957609913945198 1711006 [1.0000000000000002e-06, 1.0000000000000002e-06] 462.83421778678894\n",
      "21000 0.546110987663269 0.5955271319150924 1712006 [1.0000000000000002e-06, 1.0000000000000002e-06] 486.05309891700745\n",
      "22000 0.5657362341880798 0.5959909284114837 1713006 [1.0000000000000002e-06, 1.0000000000000002e-06] 509.16295742988586\n",
      "23000 0.6278413534164429 0.5961961451768875 1714006 [1.0000000000000002e-06, 1.0000000000000002e-06] 531.7929794788361\n",
      "24000 0.555452823638916 0.5960323631763458 1715006 [1.0000000000000002e-06, 1.0000000000000002e-06] 555.0966763496399\n",
      "25000 0.6241829991340637 0.595423075735569 1716006 [1.0000000000000002e-06, 1.0000000000000002e-06] 578.3946847915649\n",
      "26000 0.5295476913452148 0.5938318399190903 1717006 [1.0000000000000002e-06, 1.0000000000000002e-06] 601.6490285396576\n",
      "27000 0.6121094226837158 0.5961561957597733 1718006 [1.0000000000000002e-06, 1.0000000000000002e-06] 624.5371458530426\n",
      "28000 0.503394603729248 0.5956806261539459 1719006 [1.0000000000000002e-06, 1.0000000000000002e-06] 647.6927287578583\n",
      "29000 0.6092544198036194 0.5952335631847382 1720006 [1.0000000000000002e-06, 1.0000000000000002e-06] 670.9076809883118\n",
      "30000 0.6271777749061584 0.5951254360675812 1721006 [1.0000000000000002e-06, 1.0000000000000002e-06] 694.0983653068542\n",
      "31000 0.611876368522644 0.5942232673168182 1722006 [1.0000000000000002e-06, 1.0000000000000002e-06] 717.22620844841\n",
      "32000 0.5968329310417175 0.5947961482405663 1723006 [1.0000000000000002e-06, 1.0000000000000002e-06] 740.0145735740662\n",
      "33000 0.6652401089668274 0.5944650455713272 1724006 [1.0000000000000002e-06, 1.0000000000000002e-06] 762.5305771827698\n",
      "34000 0.5864191651344299 0.595112462759018 1725006 [1.0000000000000002e-06, 1.0000000000000002e-06] 785.0692253112793\n",
      "35000 0.6365917921066284 0.5953328037261962 1726006 [1.0000000000000002e-06, 1.0000000000000002e-06] 807.6664123535156\n",
      "36000 0.5996633768081665 0.5946165053248406 1727006 [1.0000000000000002e-06, 1.0000000000000002e-06] 831.03635597229\n",
      "37000 0.6100321412086487 0.5935737124085426 1728006 [1.0000000000000002e-06, 1.0000000000000002e-06] 854.2490248680115\n",
      "38000 0.5763891935348511 0.595857294023037 1729006 [1.0000000000000002e-06, 1.0000000000000002e-06] 877.2316420078278\n",
      "39000 0.5829723477363586 0.5956674099564553 1730006 [1.0000000000000002e-06, 1.0000000000000002e-06] 900.4349050521851\n",
      "40000 0.5975940823554993 0.5942780873775482 1731006 [1.0000000000000002e-06, 1.0000000000000002e-06] 923.3619892597198\n",
      "41000 0.5608127117156982 0.5962061727046967 1732006 [1.0000000000000002e-06, 1.0000000000000002e-06] 946.5929696559906\n",
      "42000 0.5862641334533691 0.5950119978189469 1733006 [1.0000000000000002e-06, 1.0000000000000002e-06] 969.8181660175323\n",
      "43000 0.6021072864532471 0.594694319665432 1734006 [1.0000000000000002e-06, 1.0000000000000002e-06] 992.8697028160095\n",
      "44000 0.6157671809196472 0.5966808354854584 1735006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1015.9958639144897\n",
      "45000 0.5944502353668213 0.5945269963741302 1736006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1039.0880539417267\n",
      "46000 0.5416874289512634 0.5948357688784599 1737006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1062.229282617569\n",
      "47000 0.6097061038017273 0.5977501036524773 1738006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1084.8230621814728\n",
      "48000 0.565946102142334 0.5946703337430954 1739006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1107.9399361610413\n",
      "49000 0.5767679810523987 0.5949318364858628 1740006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1130.8021776676178\n",
      "50000 0.5546234250068665 0.5942910733819008 1741006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1153.6044611930847\n",
      "51000 0.6023555994033813 0.5950714960992336 1742006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1176.677609205246\n",
      "52000 0.6412824392318726 0.5953564207553863 1743006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1199.7634205818176\n",
      "53000 0.6082960963249207 0.5945849312245846 1744006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1222.4383647441864\n",
      "54000 0.6076211929321289 0.5960283455848694 1745006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1245.1322271823883\n",
      "55000 0.6401094794273376 0.5921628507375717 1746006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1268.3215973377228\n",
      "56000 0.5891398191452026 0.5928083468079567 1747006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1291.4417390823364\n",
      "57000 0.5474450588226318 0.5940343345403671 1748006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1314.5783574581146\n",
      "58000 0.6250025033950806 0.5954097242355346 1749006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1337.2928342819214\n",
      "59000 0.6094816327095032 0.5939136219620704 1750006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1359.8490190505981\n",
      "60000 0.5756435990333557 0.5935517148375511 1751006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1382.8588540554047\n",
      "61000 0.5849568247795105 0.5942549420595169 1752006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1405.8692951202393\n",
      "62000 0.5934614539146423 0.594342323243618 1753006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1428.4006612300873\n",
      "63000 0.6378990411758423 0.5934744617938995 1754006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1450.8071715831757\n",
      "64000 0.659060001373291 0.5928788126111031 1755006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1473.6006259918213\n",
      "65000 0.6530255675315857 0.5942152404785156 1756006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1496.5720756053925\n",
      "66000 0.5328420400619507 0.594746998488903 1757006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1519.5360820293427\n",
      "67000 0.6084327697753906 0.5956687287688255 1758006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1542.4173338413239\n",
      "68000 0.6434991955757141 0.5941964610219002 1759006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1565.2827491760254\n",
      "69000 0.5517700910568237 0.5945280780196189 1760006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1588.1815073490143\n",
      "70000 0.568781316280365 0.5925734946727753 1761006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1611.1172654628754\n",
      "71000 0.5980981588363647 0.5932323934137821 1762006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1634.1318702697754\n",
      "72000 0.6113020181655884 0.5944350265860557 1763006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1657.080013036728\n",
      "73000 0.6057064533233643 0.5937406335473061 1764006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1680.039500951767\n",
      "74000 0.6268802285194397 0.5944886710643769 1765006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1703.0120413303375\n",
      "75000 0.5625224113464355 0.5934899297952652 1766006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1725.9778833389282\n",
      "76000 0.6174616813659668 0.5951290734410286 1767006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1748.7191276550293\n",
      "77000 0.6011346578598022 0.5919966980516911 1768006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1771.545794725418\n",
      "78000 0.5743264555931091 0.5937246115803718 1769006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1794.0068471431732\n",
      "79000 0.5877494812011719 0.5923917708992958 1770006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1816.5694844722748\n",
      "80000 0.5907787084579468 0.5929545873403549 1771006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1839.6123535633087\n",
      "81000 0.6153693795204163 0.5938134959340096 1772006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1862.588047504425\n",
      "82000 0.6149413585662842 0.5935058507919312 1773006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1885.5980949401855\n",
      "83000 0.6083738207817078 0.5940170804262161 1774006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1908.5736491680145\n",
      "84000 0.5448693037033081 0.5934441806674003 1775006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1931.5408680438995\n",
      "85000 0.5858663320541382 0.59148524081707 1776006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1954.4983355998993\n",
      "86000 0.5739829540252686 0.5918461295962334 1777006 [1.0000000000000002e-06, 1.0000000000000002e-06] 1977.476437330246\n",
      "87000 0.5856043696403503 0.5912972314357757 1778006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2000.4423298835754\n",
      "88000 0.5974072813987732 0.5924951117038727 1779006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2023.3657803535461\n",
      "89000 0.6059060096740723 0.5934591862559319 1780006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2046.3239707946777\n",
      "90000 0.5887884497642517 0.5920692943334579 1781006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2069.27362203598\n",
      "91000 0.5767881870269775 0.5926705409288406 1782006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2092.129479408264\n",
      "92000 0.588607132434845 0.5928156514167786 1783006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2115.0411307811737\n",
      "93000 0.5538831353187561 0.5930138595104217 1784006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2138.1040966510773\n",
      "94000 0.6654154658317566 0.5927703439593315 1785006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2161.1780483722687\n",
      "95000 0.5883046984672546 0.5925605998635292 1786006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2184.3282527923584\n",
      "96000 0.6263474822044373 0.5922588084340096 1787006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2206.8328924179077\n",
      "97000 0.572266161441803 0.5928704017400741 1788006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2229.5527584552765\n",
      "98000 0.5602830648422241 0.5927670875191688 1789006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2252.5220086574554\n",
      "99000 0.5619145035743713 0.5907642013430595 1790006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2275.2331323623657\n",
      "100000 0.5625647902488708 0.5923246805071831 1791006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2297.7087750434875\n",
      "101000 0.5525505542755127 0.5923991866707802 1792006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2320.829192876816\n",
      "102000 0.5654923915863037 0.5937751511335373 1793006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2344.0498735904694\n",
      "103000 0.6152640581130981 0.5923882381319999 1794006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2367.314216375351\n",
      "104000 0.5884802341461182 0.5912416912913322 1795006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2390.444210767746\n",
      "105000 0.5817825198173523 0.5930643713474274 1796006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2413.4774358272552\n",
      "106000 0.5812848806381226 0.5927455940842629 1797006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2436.602373600006\n",
      "107000 0.5739957690238953 0.5910454870462417 1798006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2459.706635951996\n",
      "108000 0.6367223262786865 0.591149185359478 1799006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2482.518627166748\n",
      "109000 0.5402447581291199 0.591579815864563 1800006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2505.25088429451\n",
      "110000 0.5555782914161682 0.5924043904542923 1801006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2527.988736152649\n",
      "111000 0.5655044913291931 0.5918004840016365 1802006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2551.101058244705\n",
      "112000 0.5697173476219177 0.5927007327079773 1803006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2574.2912793159485\n",
      "113000 0.57044517993927 0.5914458056092262 1804006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2597.4255640506744\n",
      "114000 0.6238893270492554 0.5919823960661889 1805006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2620.241784095764\n",
      "115000 0.6531063318252563 0.5911294617652894 1806006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2643.290680170059\n",
      "116000 0.6121131181716919 0.5910231431126595 1807006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2665.9609022140503\n",
      "117000 0.5857092142105103 0.5925920867919922 1808006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2689.0045137405396\n",
      "118000 0.5708415508270264 0.5917252390980721 1809006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2712.308081150055\n",
      "119000 0.5933545231819153 0.591935013025999 1810006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2735.5142102241516\n",
      "120000 0.6273118853569031 0.592095587015152 1811006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2758.473095178604\n",
      "121000 0.5877078175544739 0.5932131069898605 1812006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2781.3704845905304\n",
      "122000 0.5783349871635437 0.5914442428946495 1813006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2804.0662729740143\n",
      "123000 0.5651648640632629 0.5915285863280296 1814006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2826.8487470149994\n",
      "124000 0.6140841841697693 0.5923349162936211 1815006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2849.44566988945\n",
      "125000 0.5830804109573364 0.5920758056044578 1816006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2872.0880074501038\n",
      "126000 0.6293948888778687 0.5918197568058967 1817006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2894.813110113144\n",
      "127000 0.5667548775672913 0.5920864359140396 1818006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2917.4265189170837\n",
      "128000 0.5819072723388672 0.5926479865014553 1819006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2939.9947016239166\n",
      "129000 0.6398321390151978 0.5911683579683303 1820006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2963.097690343857\n",
      "130000 0.587179958820343 0.5921574230790139 1821006 [1.0000000000000002e-06, 1.0000000000000002e-06] 2986.3460080623627\n",
      "131000 0.6354784369468689 0.5924383262991906 1822006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3009.310655117035\n",
      "132000 0.537905752658844 0.5933398844003678 1823006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3032.366170167923\n",
      "133000 0.6311736106872559 0.590307027131319 1824006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3055.5973567962646\n",
      "134000 0.5909618139266968 0.593063441336155 1825006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3078.6944241523743\n",
      "135000 0.5846445560455322 0.5938006591796875 1826006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3101.514231443405\n",
      "136000 0.5745048522949219 0.5918933025598526 1827006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3124.3898384571075\n",
      "137000 0.5346349477767944 0.5926718145608902 1828006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3147.1414647102356\n",
      "138000 0.5596684813499451 0.5937998712956906 1829006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3169.858611345291\n",
      "139000 0.552574098110199 0.5930699644684791 1830006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3192.720113515854\n",
      "140000 0.5961889028549194 0.5917121984362602 1831006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3215.762052297592\n",
      "141000 0.6070603132247925 0.5929004915952683 1832006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3238.925626516342\n",
      "142000 0.575524628162384 0.5920916458070278 1833006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3262.095888853073\n",
      "143000 0.5994857549667358 0.5920061288475991 1834006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3285.2162697315216\n",
      "144000 0.605438232421875 0.592270842730999 1835006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3308.45658826828\n",
      "145000 0.5928972959518433 0.590399874329567 1836006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3331.6208398342133\n",
      "146000 0.6021236777305603 0.5930389240980148 1837006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3354.9217348098755\n",
      "147000 0.58270663022995 0.5919906937479973 1838006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3378.117431640625\n",
      "148000 0.6071691513061523 0.5928666317164898 1839006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3401.371945142746\n",
      "149000 0.620112955570221 0.5908220572471619 1840006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3424.6693942546844\n",
      "150000 0.5828040838241577 0.5922961188554764 1841006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3447.9581294059753\n",
      "151000 0.5979531407356262 0.5919069605469703 1842006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3470.696354150772\n",
      "152000 0.5886504054069519 0.5931947805285454 1843006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3493.576242685318\n",
      "153000 0.5742657780647278 0.5910553130507469 1844006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3516.615093231201\n",
      "154000 0.597450852394104 0.590206423163414 1845006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3539.8383433818817\n",
      "155000 0.5782136917114258 0.5937488829791546 1846006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3562.6941950321198\n",
      "156000 0.5771687626838684 0.5927708699703217 1847006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3585.632078409195\n",
      "157000 0.6316897869110107 0.592035868525505 1848006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3608.9064009189606\n",
      "158000 0.5671533942222595 0.5919663332104683 1849006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3632.186042547226\n",
      "159000 0.606971025466919 0.5910156069099903 1850006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3655.3958837985992\n",
      "160000 0.5875396728515625 0.5917116584777832 1851006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3678.474343061447\n",
      "161000 0.5589852929115295 0.5923984310030938 1852006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3701.034898519516\n",
      "162000 0.616489589214325 0.5922626253366471 1853006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3724.0243854522705\n",
      "163000 0.5656788945198059 0.5921502075195313 1854006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3746.9225931167603\n",
      "164000 0.5574991106987 0.5922212388515472 1855006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3769.88405418396\n",
      "165000 0.5744416117668152 0.5923839046955108 1856006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3792.766992330551\n",
      "166000 0.5914332866668701 0.5939912311434746 1857006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3815.1087431907654\n",
      "167000 0.5619261860847473 0.592361499428749 1858006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3837.465245246887\n",
      "168000 0.6259397268295288 0.5914008125662804 1859006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3860.1012444496155\n",
      "169000 0.5400227308273315 0.5916761547923088 1860006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3883.2674036026\n",
      "170000 0.5990930199623108 0.5943795193433762 1861006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3906.521101474762\n",
      "171000 0.5941630601882935 0.5929991391301155 1862006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3929.743837594986\n",
      "172000 0.6095028519630432 0.5913526069521904 1863006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3952.931830406189\n",
      "173000 0.5892984867095947 0.5910673376917839 1864006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3976.0882782936096\n",
      "174000 0.5610276460647583 0.5906578769087791 1865006 [1.0000000000000002e-06, 1.0000000000000002e-06] 3999.2447621822357\n",
      "175000 0.5692890286445618 0.5925232550501823 1866006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4022.5507209300995\n",
      "176000 0.5263996124267578 0.5899213274121284 1867006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4045.7565784454346\n",
      "177000 0.6052326560020447 0.5904057310819626 1868006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4068.820238828659\n",
      "178000 0.5684248805046082 0.5918060671687126 1869006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4091.7631537914276\n",
      "179000 0.6029929518699646 0.5913510946631432 1870006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4114.5232701301575\n",
      "180000 0.6213709712028503 0.5906427821516991 1871006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4137.36297249794\n",
      "181000 0.5704237222671509 0.5902505253553391 1872006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4160.350141048431\n",
      "182000 0.5829135775566101 0.5913615620732308 1873006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4183.549082040787\n",
      "183000 0.5731906890869141 0.5929692292809486 1874006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4206.86488699913\n",
      "184000 0.5487936735153198 0.5913987381458282 1875006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4230.15389919281\n",
      "185000 0.5988045930862427 0.5910362168550491 1876006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4253.353282213211\n",
      "186000 0.5927295088768005 0.5917463352382183 1877006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4276.394604444504\n",
      "187000 0.5704982876777649 0.5912214378118515 1878006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4299.552967309952\n",
      "188000 0.5667234659194946 0.5924373765587807 1879006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4322.647140741348\n",
      "189000 0.5562744140625 0.589708189368248 1880006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4345.836982727051\n",
      "190000 0.5790606141090393 0.5924677954912185 1881006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4368.60463643074\n",
      "191000 0.5395942330360413 0.5909903268814087 1882006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4391.206364631653\n",
      "192000 0.6072145700454712 0.5913087809681893 1883006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4413.585276603699\n",
      "193000 0.5927348136901855 0.5904890016913414 1884006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4436.609800815582\n",
      "194000 0.5724959373474121 0.5920123403370381 1885006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4459.813735485077\n",
      "195000 0.6411579847335815 0.5902833297848702 1886006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4482.968447446823\n",
      "196000 0.6038886308670044 0.5901577533483505 1887006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4506.18136048317\n",
      "197000 0.5477095246315002 0.5917924436330795 1888006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4529.32870221138\n",
      "198000 0.5739272236824036 0.59050385171175 1889006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4552.436677455902\n",
      "199000 0.5999358892440796 0.590375091791153 1890006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4575.728561878204\n",
      "200000 0.5639638900756836 0.5923382301330566 1891006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4598.921715259552\n",
      "201000 0.5632323622703552 0.5911989652514458 1892006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4621.968763113022\n",
      "202000 0.5844693779945374 0.5900519095659256 1893006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4644.892431020737\n",
      "203000 0.6280361413955688 0.5895051997900009 1894006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4668.001673221588\n",
      "204000 0.5975515246391296 0.5908181951642036 1895006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4691.031863451004\n",
      "205000 0.6002627015113831 0.5881956686973572 1896006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4713.99428486824\n",
      "206000 0.6652377843856812 0.5896225748062134 1897006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4737.040219545364\n",
      "207000 0.57000333070755 0.5899864845275878 1898006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4760.109863996506\n",
      "208000 0.5837159752845764 0.5934704137444496 1899006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4782.957861661911\n",
      "209000 0.5240174531936646 0.5896102297902107 1900006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4805.76429104805\n",
      "210000 0.5786035060882568 0.5918074939846992 1901006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4828.715790748596\n",
      "211000 0.5759515762329102 0.5908912546038627 1902006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4851.911320924759\n",
      "212000 0.5875389575958252 0.5902225976586342 1903006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4875.133402109146\n",
      "213000 0.6115241050720215 0.5916130711436272 1904006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4897.849297761917\n",
      "214000 0.6147838830947876 0.5909001097679139 1905006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4921.062547445297\n",
      "215000 0.5535276532173157 0.5901136129498482 1906006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4944.111140966415\n",
      "216000 0.6189550161361694 0.5891015821695328 1907006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4967.276138782501\n",
      "217000 0.5722991228103638 0.5899247174263 1908006 [1.0000000000000002e-06, 1.0000000000000002e-06] 4990.469312906265\n",
      "218000 0.6451877951622009 0.5890328851342201 1909006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5013.633624315262\n",
      "219000 0.5539537668228149 0.5889033570885658 1910006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5036.744944572449\n",
      "220000 0.591805100440979 0.589979692876339 1911006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5059.759704589844\n",
      "221000 0.5437213778495789 0.5902089922428131 1912006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5082.58495426178\n",
      "222000 0.5746307969093323 0.5896656973063946 1913006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5105.610404014587\n",
      "223000 0.5913701057434082 0.5909232943356038 1914006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5128.629339456558\n",
      "224000 0.5620003938674927 0.5908990758061409 1915006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5151.622649908066\n",
      "225000 0.6215618848800659 0.5902842262685299 1916006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5174.722119092941\n",
      "226000 0.6007928252220154 0.5914756296873093 1917006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5197.7342319488525\n",
      "227000 0.5913225412368774 0.5899190189242363 1918006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5220.760145664215\n",
      "228000 0.6273882985115051 0.5908272442221641 1919006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5243.744982242584\n",
      "229000 0.6466633677482605 0.589069227218628 1920006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5266.696457147598\n",
      "230000 0.5915167331695557 0.5887129939198494 1921006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5289.610862731934\n",
      "231000 0.5555554032325745 0.5887731905281544 1922006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5312.601804971695\n",
      "232000 0.5873289704322815 0.5860584226846695 1923006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5335.7626080513\n",
      "233000 0.5682836771011353 0.5886749610602856 1924006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5358.690470218658\n",
      "234000 0.5997267961502075 0.5871826564967633 1925006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5381.809026002884\n",
      "235000 0.5824053287506104 0.5897532742619515 1926006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5405.021502017975\n",
      "236000 0.603894054889679 0.590646398961544 1927006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5428.164844512939\n",
      "237000 0.6103951930999756 0.5892446539402008 1928006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5450.946083784103\n",
      "238000 0.6131804585456848 0.5883024817109108 1929006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5473.778547048569\n",
      "239000 0.5604674816131592 0.5896849001049995 1930006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5496.746257543564\n",
      "240000 0.5840837955474854 0.5891344021856785 1931006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5519.9585971832275\n",
      "241000 0.6207173466682434 0.5887344201207161 1932006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5543.033315420151\n",
      "242000 0.5440547466278076 0.5888360151052475 1933006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5565.953672409058\n",
      "243000 0.5853734612464905 0.5889520931243897 1934006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5588.9416790008545\n",
      "244000 0.6014760136604309 0.5879917032122612 1935006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5611.975161552429\n",
      "245000 0.6008197665214539 0.589402735710144 1936006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5635.018271207809\n",
      "246000 0.6017258763313293 0.5889013534188271 1937006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5657.942317724228\n",
      "247000 0.5595579147338867 0.5871993994116783 1938006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5680.289705276489\n",
      "248000 0.5980877876281738 0.5893297764062881 1939006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5703.259004116058\n",
      "249000 0.5865960121154785 0.5858873881697655 1940006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5726.278125524521\n",
      "250000 0.6091797947883606 0.5889491131305694 1941006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5748.7489902973175\n",
      "251000 0.6209406852722168 0.5871123861074448 1942006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5771.641410589218\n",
      "252000 0.540800929069519 0.5878226996660233 1943006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5794.834995031357\n",
      "253000 0.6097891926765442 0.5876133561730384 1944006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5818.089712142944\n",
      "254000 0.6148597002029419 0.587212346881628 1945006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5841.372749328613\n",
      "255000 0.6779055595397949 0.5859157538414002 1946006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5864.702535152435\n",
      "256000 0.5409975051879883 0.5865500703454017 1947006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5887.964764595032\n",
      "257000 0.5570747256278992 0.586209031701088 1948006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5911.3342361450195\n",
      "258000 0.6156819462776184 0.5856155217587948 1949006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5934.580278158188\n",
      "259000 0.6038442254066467 0.5869038805067539 1950006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5957.780158996582\n",
      "260000 0.5661864280700684 0.5859133704006672 1951006 [1.0000000000000002e-06, 1.0000000000000002e-06] 5980.350502490997\n",
      "261000 0.5148948431015015 0.5856761490404606 1952006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6002.731782197952\n",
      "262000 0.5731014013290405 0.5865278777480125 1953006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6025.4890213012695\n",
      "263000 0.6329299807548523 0.5855187030136585 1954006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6048.388042926788\n",
      "264000 0.6288920640945435 0.5851952973306179 1955006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6071.3473534584045\n",
      "265000 0.5597663521766663 0.5856448303163052 1956006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6094.3203864097595\n",
      "266000 0.5734277367591858 0.586116124778986 1957006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6117.357881784439\n",
      "267000 0.5913967490196228 0.5833453015685082 1958006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6140.321116924286\n",
      "268000 0.6346743106842041 0.5849827951192856 1959006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6163.25489449501\n",
      "269000 0.5516778826713562 0.5827869887053967 1960006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6186.255913972855\n",
      "270000 0.5773349404335022 0.5835136713385582 1961006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6209.073867321014\n",
      "271000 0.5415420532226562 0.5823118932843209 1962006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6232.043774366379\n",
      "272000 0.632790744304657 0.5831207016408444 1963006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6254.861059427261\n",
      "273000 0.5438953042030334 0.581975416123867 1964006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6277.609089136124\n",
      "274000 0.6033298969268799 0.5798742390871048 1965006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6300.350262641907\n",
      "275000 0.5511093139648438 0.5815255082249642 1966006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6323.147881269455\n",
      "276000 0.5994218587875366 0.5808776028156281 1967006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6346.016371488571\n",
      "277000 0.5775572061538696 0.5802501163184642 1968006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6368.548704624176\n",
      "278000 0.5499718189239502 0.5780017068982124 1969006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6391.57316160202\n",
      "279000 0.5552969574928284 0.5774842527210713 1970006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6414.793061256409\n",
      "280000 0.5873211622238159 0.5774720349311828 1971006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6437.948623418808\n",
      "281000 0.5191864967346191 0.5779409656822682 1972006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6460.912999391556\n",
      "282000 0.5914450287818909 0.577337700009346 1973006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6483.690264940262\n",
      "283000 0.5813032388687134 0.5750926819443702 1974006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6507.04655456543\n",
      "284000 0.650377094745636 0.5725976831018925 1975006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6530.213300704956\n",
      "285000 0.5744286775588989 0.5727074719369412 1976006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6553.337203741074\n",
      "286000 0.6026984453201294 0.571354208022356 1977006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6576.50071144104\n",
      "287000 0.578794538974762 0.5686209044754506 1978006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6599.67289686203\n",
      "288000 0.554680585861206 0.5660033323168755 1979006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6622.5301949977875\n",
      "289000 0.5599497556686401 0.5619577724337578 1980006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6645.546129226685\n",
      "290000 0.6186075806617737 0.5595273315906525 1981006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6668.517958164215\n",
      "291000 0.5803549885749817 0.5557939244806767 1982006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6691.132879972458\n",
      "292000 0.5693721175193787 0.5486608943045139 1983006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6713.8691873550415\n",
      "293000 0.4994561970233917 0.5424294783771038 1984006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6737.080285787582\n",
      "294000 0.5710644721984863 0.5308293828666211 1985006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6760.267950296402\n",
      "295000 0.5102226138114929 0.5164139402508736 1986006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6783.336223363876\n",
      "296000 0.4585418701171875 0.49091287711262704 1987006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6806.041111707687\n",
      "297000 0.47423797845840454 0.4319063600897789 1988006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6829.131063938141\n",
      "298000 0.5865578651428223 0.5948266602754593 1989006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6852.418918848038\n",
      "299000 0.6149827837944031 0.5955698460936546 1990006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6875.845760345459\n",
      "300000 0.5890228748321533 0.5957679427862167 1991006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6898.999311447144\n",
      "301000 0.5835663080215454 0.5947745983600616 1992006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6922.3092703819275\n",
      "302000 0.589867353439331 0.5952458454966545 1993006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6945.6119911670685\n",
      "303000 0.6054993867874146 0.5958461074233055 1994006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6968.657132387161\n",
      "304000 0.554560661315918 0.59552157330513 1995006 [1.0000000000000002e-06, 1.0000000000000002e-06] 6991.611526012421\n",
      "305000 0.6326473951339722 0.5947986229658127 1996006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7014.578635692596\n",
      "306000 0.5338324904441833 0.5952949584126472 1997006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7037.457591056824\n",
      "307000 0.5511408448219299 0.5955042388439179 1998006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7060.457820177078\n",
      "308000 0.5253389477729797 0.595150016605854 1999006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7083.484560728073\n",
      "309000 0.5439767241477966 0.5942275422215462 2000006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7106.589204072952\n",
      "310000 0.6798365116119385 0.5941258726716041 2001006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7129.430691003799\n",
      "311000 0.6226574182510376 0.5945111322999 2002006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7152.546725988388\n",
      "312000 0.5796375274658203 0.5949306339025497 2003006 [1.0000000000000002e-06, 1.0000000000000002e-06] 7175.457903623581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(seq_len):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mNLLLoss()(output[i], target_tensor[i])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1124\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m             tracing_state\u001b[39m.\u001b[39mpop_scope()\n\u001b[1;32m   1122\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1124\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_impl\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1125\u001b[0m     forward_call \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_get_tracing_state() \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward)\n\u001b[1;32m   1126\u001b[0m     \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m     \u001b[39m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-0914.pth')\n",
    "        torch.save(s2h, 'model/s2h-0914.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-0914.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7, 4] False\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 11, 10, 7, 7, 4, 6, 7, 6, 6, 4, 13, 10, 8, 8] False\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 6, 7, 11, 10, 4, 10, 4, 11, 4, 4, 13, 7, 13, 16, 4] False\n",
      "[8, 4, 4, 12, 10, 7, 6, 4, 4, 11, 13, 7, 4, 8, 4, 8] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6, 4] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 7, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6] False\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 13, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 4, 4, 8, 13, 7, 4, 4, 4, 15] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 4, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 9, 10, 8, 4] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 8, 4, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] False\n",
      "[8, 11, 4, 4, 13, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8] False\n",
      "[8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4, 8] False\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 13, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] False\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 11, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4] False\n",
      "[8, 4, 6, 8, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] False\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 11, 6, 4, 13, 6, 6, 4, 13, 10, 8, 8, 7, 11, 7, 4] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 11, 11, 13, 2, 11] False\n",
      "[8, 6, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13] False\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8] False\n",
      "[8, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 11, 7, 7, 11, 11, 9, 10, 8, 4, 7, 7, 10, 4, 9, 7] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 4, 13, 7, 13, 16, 4, 4] False\n",
      "[8, 4, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 9, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 6, 8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.004082541521851113 0.0337065305030811 0.0736751094920089\n",
      "1 0.004693377259405371 0.08172096425303191 0.1042314247342685\n",
      "2 0.003623476205794533 0.049978884267248794 0.07722916947355046\n",
      "3 0.00490398523215316 0.048527761870073 0.08086079321732315\n",
      "4 0.0028337228124704437 0.05505688584281922 0.1126146170283329\n",
      "5 0.017480166412742065 0.038624228303103345 0.05843249684209055\n",
      "6 0.004230395405295247 0.0635676077243882 0.12974426772181177\n",
      "7 0.0029529465927298952 0.06221945814779238 0.12212496883223506\n",
      "8 0.004024165490224982 0.06014040007310334 0.11672103015604808\n",
      "9 0.00432188690151564 0.0361440380656507 0.09830841721535631\n",
      "10 0.003431598644190163 0.04402259634315614 0.08451325312891833\n",
      "11 0.008530985452907468 0.0742274829189594 0.12403968853068947\n",
      "12 0.0037716664489650968 0.05081919799406754 0.10689129894673811\n",
      "13 0.00448878391159363 0.05478143959800651 0.10637437761040613\n",
      "14 0.009004225946108866 0.05093128224363404 0.09999558257104302\n",
      "15 0.0048795425030875965 0.06131469119904691 0.10472203100377064\n",
      "16 0.0063304804738862205 0.04181317324863333 0.09012752599759008\n",
      "17 0.008705481069172469 0.08542392171321181 0.11524447608156743\n",
      "18 0.0038009667981175905 0.049812078562506336 0.10537829564423173\n",
      "19 0.004890016296689132 0.03718833468513082 0.11007005863028065\n",
      "20 0.003243589791021965 0.06571079584348133 0.0992531175510176\n",
      "21 0.013049188142013383 0.07853267093466146 0.10026866857609482\n",
      "22 0.012159656329214093 0.056887123671955954 0.0912383806024383\n",
      "23 0.004407997917941619 0.08316172199950064 0.12603446948128155\n",
      "24 0.004733139843233586 0.05370741512986503 0.10043666212992552\n",
      "25 0.010666653417939532 0.07410823547630765 0.12682986798650778\n",
      "26 0.004755511812704291 0.04154048756032047 0.09891848859700221\n",
      "27 0.004884105627205843 0.05280452932600795 0.0660232934997412\n",
      "28 0.004012946350770392 0.05764017977103557 0.10568594630988733\n",
      "29 0.003827466377509529 0.07071472937045656 0.13826907750683662\n",
      "30 0.006807367525633531 0.057152674618022495 0.09035059999987288\n",
      "31 0.0028148368982076526 0.023535134387510055 0.04868861446856464\n",
      "32 0.006841247615137776 0.06072735726352822 0.10461376791490962\n",
      "33 0.0049770701469997805 0.050540696292396534 0.08397606629308829\n",
      "34 0.004822201954132148 0.05549803995100361 0.11506215387051524\n",
      "35 0.016996859283083622 0.06916526692419721 0.08351573042379531\n",
      "36 0.011462658804395547 0.08060003027063459 0.10451082864691\n",
      "37 0.003853750454021158 0.05011232470039451 0.08568096205518708\n",
      "38 0.002409540279453882 0.0331513028146021 0.07490635155715515\n",
      "39 0.004890703980273924 0.08571738227969017 0.11519057774560024\n",
      "40 0.008319402447590744 0.058359519272859345 0.10117835158983615\n",
      "41 0.003342560599710804 0.058828385452012875 0.08413754987357422\n",
      "42 0.0038586435902852496 0.04002205297136312 0.08396054638410728\n",
      "43 0.003937649148828503 0.07399004575464371 0.09211931971990357\n",
      "44 0.0047640641150525145 0.04447562607083595 0.102618795260109\n",
      "45 0.010731605158181983 0.05987550819968124 0.0942617808089699\n",
      "46 0.010057454049228046 0.05351440977493972 0.09409308715180492\n",
      "47 0.004817406136103515 0.03648291455244415 0.08239539380723146\n",
      "48 0.010609061406061287 0.08600338749238702 0.09706201021618441\n",
      "49 0.003790656043284385 0.04549181652455006 0.08237362303622925\n",
      "50 0.003483528416496038 0.054217678003736655 0.1040230247965717\n",
      "51 0.002458431033279597 0.06992515585960604 0.12283278093789532\n",
      "52 0.003984695487310842 0.0699206952501921 0.10168760146969924\n",
      "53 0.004314974432078398 0.03935832040406218 0.07966117433054914\n",
      "54 0.0030450157860850563 0.04581431695219419 0.1114137844561092\n",
      "55 0.010520660518139099 0.06926744164938749 0.10145983265106981\n",
      "56 0.0035702479972952936 0.038456384651653755 0.11248132541702516\n",
      "57 0.004416441130173723 0.040633850687751286 0.08492055734442056\n",
      "58 0.008592391087936255 0.07170834499976914 0.13034754958139166\n",
      "59 0.004726057339112889 0.04796529082879736 0.0829196744059342\n",
      "60 0.004785725959240014 0.06333453225954518 0.1354506821136629\n",
      "61 0.004766524986605086 0.046704614007829964 0.07898722130023553\n",
      "62 0.004502224165500803 0.04607612315879202 0.08279698854486534\n",
      "63 0.004767244571204572 0.05953461709773969 0.1055014573275081\n",
      "64 0.010880060898636633 0.04858134288830455 0.06149561080178663\n",
      "65 0.007145592513913803 0.06138137170663487 0.09481670842044823\n",
      "66 0.004231706727274874 0.05317595985819421 0.09600140818767627\n",
      "67 0.004453456793607223 0.041904245923089944 0.08935499447206391\n",
      "68 0.0041003250471283 0.05927885088459043 0.11256503973884295\n",
      "69 0.004284238287749063 0.05146615839275864 0.09782239060365072\n",
      "70 0.004588061351721183 0.049838830929977845 0.08221672556723518\n",
      "71 0.007864970010606927 0.05551907708886872 0.09043504130158425\n",
      "72 0.006693268993036865 0.0532060167072269 0.08748288986527572\n",
      "73 0.0037223091457119525 0.05449318431118724 0.11054394432204662\n",
      "74 0.006683702267552052 0.06539091531327029 0.11414268075072873\n",
      "75 0.004926666935274224 0.06604048023627121 0.129072299761036\n",
      "76 0.003775420400105562 0.04431417986640719 0.07368478258891008\n",
      "77 0.003931350785399129 0.05522800870680878 0.09904718633558779\n",
      "78 0.01721423568816709 0.0794799219319414 0.09680999463288066\n",
      "79 0.007071131871379185 0.062073130447187444 0.09907484113650675\n",
      "80 0.004885466195324617 0.05765788906162886 0.1301729265588225\n",
      "81 0.003930760065650002 0.050290081493218584 0.09893653787270035\n",
      "82 0.004570895399551165 0.05289424306558045 0.12138236738833717\n",
      "83 0.0049437185553359644 0.05054523269496891 0.0837604983927544\n",
      "84 0.004667368201231355 0.04641917060426549 0.08688296598424289\n",
      "85 0.001666609876196812 0.04673774919880189 0.08823484783360139\n",
      "86 0.003287444616165416 0.0627534009856718 0.1179517668538136\n",
      "87 0.008408562728293384 0.06709179401815792 0.10644124344055941\n",
      "88 0.004445341512181781 0.06013955302411113 0.08982561057069308\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, seq_len, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m inputTensor(np\u001b[39m.\u001b[39marray([[size]]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39;49m, hn)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m         out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m out_\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     i \u001b[39m=\u001b[39m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_1(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mh2o(out)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
