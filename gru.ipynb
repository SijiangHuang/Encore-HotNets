{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(1000)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "\n",
    "for pair in range(1000):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - 8):\n",
    "        seq_set[pair].append(size_index[i:i+8])\n",
    "        target_set[pair].append(target_index[i:i+8])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    for pair in range(1000):\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        in_dim = input_size \n",
    "        for h_dim in hidden_dims:\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(in_dim, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for lin in self.lins:\n",
    "            x = lin(x)\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, 2, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.h2o(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2).to(device)\n",
    "s2h = SizeToHidden(n_size, [32, 64, 128], hidden_size, 2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_tensor, size_tensor, target_tensor = next(iter(dataloader))\n",
    "seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "size_tensor = size_tensor.float().to(device)\n",
    "target_tensor = target_tensor.T.long().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(8):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 2.3606510162353516 2.4871399211883545 7.630752086639404\n",
      "200 2.337522029876709 2.3421318554878234 15.1015043258667\n",
      "300 2.316504955291748 2.3325484824180602 22.63513445854187\n",
      "400 2.3277180194854736 2.3305913829803466 29.303078174591064\n",
      "500 2.34124755859375 2.3262123441696168 36.03553533554077\n",
      "600 2.3333215713500977 2.3252388191223146 42.206469774246216\n",
      "700 2.3093152046203613 2.324979603290558 49.31947469711304\n",
      "800 2.3331458568573 2.323228316307068 56.15637278556824\n",
      "900 2.323852062225342 2.3236997961997985 63.11879301071167\n",
      "1000 2.3378725051879883 2.3220830249786375 70.82154369354248\n",
      "1100 2.3311243057250977 2.3195905303955078 77.6230046749115\n",
      "1200 2.3114726543426514 2.3185447883605956 85.05819034576416\n",
      "1300 2.328340530395508 2.318561806678772 93.3337299823761\n",
      "1400 2.326587438583374 2.31739280462265 99.7317304611206\n",
      "1500 2.315302610397339 2.3167378377914427 105.5874674320221\n",
      "1600 2.325039863586426 2.315819251537323 112.93643999099731\n",
      "1700 2.3183064460754395 2.31498744726181 120.76215934753418\n",
      "1800 2.3130908012390137 2.3138066363334655 127.03742814064026\n",
      "1900 2.3110549449920654 2.313517994880676 134.2540783882141\n",
      "2000 2.3196828365325928 2.311446795463562 140.8037040233612\n",
      "2100 2.319343328475952 2.3108139705657957 147.65445017814636\n",
      "2200 2.3059561252593994 2.309890162944794 154.86093163490295\n",
      "2300 2.307316303253174 2.3102637124061585 162.38797187805176\n",
      "2400 2.309879779815674 2.308311336040497 170.52299213409424\n",
      "2500 2.298584461212158 2.3066655015945434 178.6190435886383\n",
      "2600 2.316340684890747 2.3050348067283633 187.16726088523865\n",
      "2700 2.3046507835388184 2.3023043847084046 195.56136345863342\n",
      "2800 2.2973532676696777 2.3013975644111633 204.17552375793457\n",
      "2900 2.302184820175171 2.2975817060470582 212.69016313552856\n",
      "3000 2.2840585708618164 2.291786615848541 221.12576961517334\n",
      "3100 2.273085594177246 2.283989450931549 229.16424822807312\n",
      "3200 2.2790651321411133 2.2761505460739135 237.4395833015442\n",
      "3300 2.268434762954712 2.2636022114753724 244.7619891166687\n",
      "3400 2.230410099029541 2.2487977719306946 251.8902280330658\n",
      "3500 2.201141119003296 2.2285166931152345 259.83133816719055\n",
      "3600 2.2028636932373047 2.2064826011657717 267.71143865585327\n",
      "3700 2.158208131790161 2.1816649222373963 275.4459080696106\n",
      "3800 2.151841163635254 2.1528159523010255 283.0322060585022\n",
      "3900 2.1103641986846924 2.125133776664734 290.64262557029724\n",
      "4000 2.0886030197143555 2.093564202785492 296.64807629585266\n",
      "4100 2.0541234016418457 2.059880185127258 303.2332260608673\n",
      "4200 2.012319326400757 2.0294050478935244 310.10316824913025\n",
      "4300 1.9939765930175781 1.9974161994457245 316.9999780654907\n",
      "4400 1.9616773128509521 1.9653298556804657 323.3876898288727\n",
      "4500 1.919151782989502 1.9357578790187835 330.3147909641266\n",
      "4600 1.8799331188201904 1.9028615903854371 337.26177167892456\n",
      "4700 1.8744850158691406 1.8729048931598664 344.89508986473083\n",
      "4800 1.832392692565918 1.84291060090065 352.93831300735474\n",
      "4900 1.8089942932128906 1.8134735548496246 361.64053750038147\n",
      "5000 1.7765671014785767 1.7873546373844147 368.7044587135315\n",
      "5100 1.7478209733963013 1.7639408993721009 375.42876052856445\n",
      "5200 1.7284706830978394 1.7387534844875336 382.6030743122101\n",
      "5300 1.7130069732666016 1.7129742467403413 389.59963607788086\n",
      "5400 1.6920127868652344 1.6895562398433686 396.2159776687622\n",
      "5500 1.6572003364562988 1.6674620866775514 403.2254855632782\n",
      "5600 1.6573147773742676 1.6465540885925294 409.824177980423\n",
      "5700 1.6264317035675049 1.6306414186954499 417.2260830402374\n",
      "5800 1.6087875366210938 1.6087916994094849 425.92229890823364\n",
      "5900 1.590399980545044 1.5922587215900421 434.3872938156128\n",
      "6000 1.5707794427871704 1.573947056531906 442.8744761943817\n",
      "6100 1.5517311096191406 1.558286772966385 451.22861981391907\n",
      "6200 1.519357442855835 1.540266100168228 458.94898676872253\n",
      "6300 1.5134711265563965 1.528549302816391 466.4026186466217\n",
      "6400 1.5031546354293823 1.511468812227249 473.1918451786041\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "\n",
    "s_time = time.time()\n",
    "plot_every = 100\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i)\n",
    "    dataloader = DataLoader(dataset[:1000], batch_size=1000, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.3:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 19, 7, 16, 3, 6, 15, 2] False\n",
      "[8, 25, 11, 6, 17, 22, 18, 17] False\n",
      "[8, 5, 20, 29, 18, 9, 24, 25] False\n",
      "[8, 12, 12, 28, 21, 0, 12, 2] False\n",
      "[8, 29, 17, 19, 10, 19, 17, 23] False\n",
      "[8, 26, 12, 26, 1, 20, 6, 16] False\n",
      "[8, 23, 22, 21, 14, 5, 6, 16] False\n",
      "[8, 13, 29, 9, 2, 21, 29, 23] False\n",
      "[8, 7, 1, 12, 26, 12, 8, 28] False\n",
      "[8, 23, 18, 0, 27, 20, 6, 0] False\n",
      "[8, 0, 27, 20, 21, 17, 16, 13] False\n",
      "[8, 17, 19, 11, 19, 13, 2, 6] False\n",
      "[8, 2, 9, 18, 11, 21, 20, 4] False\n",
      "[8, 4, 1, 7, 10, 2, 18, 21] False\n",
      "[8, 4, 10, 21, 19, 25, 16, 0] False\n",
      "[8, 1, 2, 15, 22, 0, 13, 24] False\n",
      "[8, 19, 16, 17, 26, 17, 12, 21] False\n",
      "[8, 10, 29, 26, 4, 20, 1, 26] False\n",
      "[8, 25, 15, 3, 18, 7, 18, 20] False\n",
      "[8, 3, 14, 17, 7, 6, 19, 22] False\n",
      "[8, 10, 21, 17, 24, 6, 23, 8] False\n",
      "[8, 8, 29, 1, 5, 15, 11, 7] False\n",
      "[8, 20, 19, 28, 26, 29, 10, 1] False\n",
      "[8, 9, 14, 5, 2, 13, 27, 22] False\n",
      "[8, 9, 26, 20, 13, 7, 17, 5] False\n",
      "[8, 22, 28, 19, 21, 4, 22, 14] False\n",
      "[8, 2, 11, 21, 17, 9, 29, 17] False\n",
      "[8, 28, 16, 16, 1, 3, 18, 21] False\n",
      "[8, 7, 28, 22, 7, 19, 10, 21] False\n",
      "[8, 17, 22, 7, 27, 29, 4, 15] False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m start_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     size_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((pairdata[freqpairs[pair]]\u001b[39m.\u001b[39;49msize_index\u001b[39m.\u001b[39mvalues, pairdata[freqpairs[pair]]\u001b[39m.\u001b[39msize_index\u001b[39m.\u001b[39mvalues[\u001b[39m0\u001b[39m:\u001b[39m1\u001b[39m]))\n\u001b[1;32m      5\u001b[0m     a \u001b[39m=\u001b[39m sample(sizedata[pair], \u001b[39m8\u001b[39m, start_size)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(a, is_subarray(size_index, a))\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/generic.py:5901\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5893\u001b[0m \u001b[39m# Note: obj.x will always call obj.__getattribute__('x') prior to\u001b[39;00m\n\u001b[1;32m   5894\u001b[0m \u001b[39m# calling obj.__getattr__('x').\u001b[39;00m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[0;32m-> 5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[name]\n\u001b[1;32m   5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/frame.py:3769\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3764\u001b[0m is_mi \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex)\n\u001b[1;32m   3765\u001b[0m \u001b[39m# GH#45316 Return view if key is not duplicated\u001b[39;00m\n\u001b[1;32m   3766\u001b[0m \u001b[39m# Only use drop_duplicates with duplicates for performance\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_mi \u001b[39mand\u001b[39;00m (\n\u001b[1;32m   3768\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique\n\u001b[0;32m-> 3769\u001b[0m     \u001b[39mand\u001b[39;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\n\u001b[1;32m   3770\u001b[0m     \u001b[39mor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mdrop_duplicates(keep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   3771\u001b[0m ):\n\u001b[1;32m   3772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_item_cache(key)\n\u001b[1;32m   3774\u001b[0m \u001b[39melif\u001b[39;00m is_mi \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mand\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/pandas/core/indexes/base.py:5334\u001b[0m, in \u001b[0;36mIndex.__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5332\u001b[0m \u001b[39mhash\u001b[39m(key)\n\u001b[1;32m   5333\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 5334\u001b[0m     \u001b[39mreturn\u001b[39;00m key \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\n\u001b[1;32m   5335\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOverflowError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   5336\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 8, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.683924189704679e-05 0.0022910055284155904 0.026759625955232017\n",
      "1 0.004099680423816764 0.01456277943562591 0.23650145293333028\n",
      "2 0.04470496112358387 0.08037020143473747 0.33721434353166024\n",
      "3 0.00043561584355844996 0.0019475070128023536 0.3462488346412111\n",
      "4 0.006056248010429628 0.012005518635222505 0.18358827226755522\n",
      "5 0.017016281948989687 0.038041371616718715 0.2937974611418298\n",
      "6 0.00874599419702543 0.011870306801960172 0.12237091430858037\n",
      "7 0.007250752866105811 0.04307198830517407 0.28260513058765857\n",
      "8 0.002846447107120527 0.02619473574740403 0.29456985171632805\n",
      "9 0.013895179550947804 0.04594797656483342 0.2794249364570428\n",
      "10 0.008878255676328061 0.010904393389825197 0.06903145299100663\n",
      "11 0.04907119199785718 0.06600896828200971 0.2731012455916101\n",
      "12 0.008602377043349528 0.01465406970493979 0.06125081041976111\n",
      "13 0.002403147294469263 0.033308766691192 0.177964247814853\n",
      "14 0.0 0.0 0.0\n",
      "15 0.01928682043648037 0.02693250059550536 0.16208903034875155\n",
      "16 0.019671193890775655 0.04012401827844053 0.317028631702214\n",
      "17 0.00020542918265788104 0.0014005909946785385 0.00269983609192213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     11\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, \u001b[39m8\u001b[39;49m, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(new_size)\u001b[39m.\u001b[39missubset(np\u001b[39m.\u001b[39munique(pairdata[freqpairs[pair]]\u001b[39m.\u001b[39msize_index\u001b[39m.\u001b[39mvalues)):\n\u001b[1;32m     14\u001b[0m         size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36msample\u001b[0;34m(size_data, seq_length, start_size)\u001b[0m\n\u001b[1;32m      4\u001b[0m s2h\u001b[39m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():  \u001b[39m# no need to track history in sampling\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     size_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(size_data, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat)\u001b[39m.\u001b[39;49mto(device)\n\u001b[1;32m      7\u001b[0m     hn \u001b[39m=\u001b[39m s2h(size_tensor)\n\u001b[1;32m      8\u001b[0m     output_seq \u001b[39m=\u001b[39m [start_size]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds = [], []\n",
    "for pair in range(1000):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(100):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, 8, start_size=start_size)\n",
    "            if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "                size_seq_gen += list(new_size[1:])\n",
    "                start_size = new_size[-1]\n",
    "                if seed > 10:\n",
    "                    start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.05:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABAdElEQVR4nO3deVxVZeLH8Q/7ogKCiqCIqKm45IJLao5pialZtkyWk7aXU02Z02b2q3SacWqmZcq0mqzGsnK0vbSkTS1tUsRSsVwTFxBxYRPZ7vn9cQQl0bgIPHf5vl+v+/Kew73ypWPer+c8z3N8LMuyEBERETHE13QAERER8W4qIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRvmbDlATDoeDvXv30qRJE3x8fEzHERERkRqwLIv8/HxiY2Px9T31+Q+3KCN79+4lLi7OdAwRERGphV27dtG6detTft0tykiTJk0A+4cJCwsznEZERERqIi8vj7i4uMrP8VNxizJScWkmLCxMZURERMTN/NYQCw1gFREREaNURkRERMQolRERERExSmVEREREjFIZEREREaNURkRERMQolRERERExSmVEREREjFIZEREREaNURkRERMQop8vI8uXLGTNmDLGxsfj4+PD+++//5nuWLVtGUlISwcHBtGvXjhdeeKE2WUVERMQDOV1GCgsL6dGjB7NmzarR63fs2MGoUaMYPHgwaWlpPPjgg9x555288847TocVERERz+P0jfJGjhzJyJEja/z6F154gTZt2vDMM88AkJiYyJo1a/jnP//J5Zdf7uy3FxEREQ9T73ftXbVqFcnJyVX2jRgxgrlz51JaWkpAQMBJ7ykuLqa4uLhyOy8vr75jihhX7rDIKyrlcFEph4+UkFtUWvk4fKTqr7lFJZzboTl3XXCW6dgi3qmsGIoLoKwISo9C2bFHadEJz4/aXy8vMZezvOyEjKfIWvHrsP+DdkOMxKz3MpKVlUV0dHSVfdHR0ZSVlZGTk0NMTMxJ75k5cybTp0+v72gidc6yLIpKy6stDxXbhytKxpFSDp+wP/9omVPfq0WT4Hr6KUQ8kMNxig/h4pM/rItzoegwFB2Co4ePPz9xX+kRoz9OvSjINvat672MAPj4+FTZtiyr2v0Vpk6dypQpUyq38/LyiIuLq7+AIr9SWu6oPEtRURxyj52xOGnfCWcv8opKKSl3nNH3bhzkT3hIAOEhAUSEHv81LCSAiJDAyu02kaF19NOKeJjyUti3Efamwd61sCcNstPBKq/77+UbAAEh4B8MAcHgHwL+QSfsCwG/AKD6z7t65+t/PMuJGQOCj+er+FpsLzMZaYAy0rJlS7Kysqrsy87Oxt/fn6ioqGrfExQURFBQUH1HEy9UUuYgO/8o+/KK2Zd39Njj+POsvKPszysmv9i5sxS/FuDnU1koKh5NQwPtQhEaQERIAOGhx8pFReEIsQtHgJ9m3IvUmKMccracUDzWQtZ6KC8+9Xt8A6r5YA46/jwoDEIiIKQpBB/7tbrtoDDw9WuQH9PT1XsZGTBgAB999FGVfUuXLqVPnz7VjhcRqUsOh8Wq7QdYlLqbFVv2k1Pg3LXbJsH+Vc9QhBwvFBUF4sSzFhX7QwP9TnnmT0RqqTDHPsORven4r/vSoST/5NcGh9v/0o/tfezXnhASaZcQvwa5KCBOcPqIFBQUsHXr1srtHTt2sG7dOiIjI2nTpg1Tp05lz549zJs3D4BJkyYxa9YspkyZws0338yqVauYO3cub731Vt39FCK/knHgCItSd/HO2j3sOVxU5WuBfr60CAsiOiyY6Mpfjz9v0SSYyEaBhAX746+zFCINr+iQfbbj18WjcH/1rw8IhZgedvFodax8RLYD/YPAbThdRtasWcPQoUMrtyvGdlx77bW89tprZGZmkpGRUfn1hIQEFi9ezN13383zzz9PbGwszz77rKb1Sp0rLC5j8fpMFqXu5n87DlbubxLsz8U9YhnbqxXtmzemaWiAzlqImORwQEEWHNwBB7fDoR3284pfjx4+xRt9oGlbaNEFWiQee3SBZh11tsPN+VgVo0ldWF5eHuHh4eTm5hIWFmY6jrgQy7L4fsdBFqXu5pP1mRwpsQeo+fjAuR2a8fs+cSR3iSY4QNd1RRpUeSkczqhaMiqKx6Ff7Nksp9MktmrhaJEIzTtBYKMGiS91o6af36qS4vLKHRZ7DxeRcfAIOw8cYefBQnbmHGHnwSNkHCiksOT4CPm2UaFckdSay3q3JjYixGBqES9RehS2f2VfVjnxLEfu7tPPXvHxg4g4aJoAkQn2ZZWK503bqnR4GZURcTn78o7yxnc7Wb8nl4wDR9h16Ail5ac+gdco0I/RZ8fw+z5x9IlvqkswIg2hMAdWz4XV/z71WA7/ELtYVJaNY8+bJkBEm2NTXkVURsSF7Dp4hBeWbWPhmt0nrdUR4OdDXNNQ4qNCiY9qRJvIiuehxEWGEuSvyzAiDWL/z7DqefhxwfFLLWGtoM05vzq7kQBNWmoQqdSIyogYtzW7gNlfb+WDdXspd9hnQPrEN+XS3q1IiGpEm6hQYsJD8PPVX2oiRlgW7Fhml5AtS4/vj+0FA+6ALpfoLIecEZURMWbj3lye/2orSzZkUTGMevBZzbh9aAf6J0TqcouIaWUlsOEdu4TsW39spw90Hg0Dboc2A3TmQ+qEyog0uNSdB5n15Va++vn4debhXaK5Y2gHesRFmAsmIvaKprvXwE8fw4//tafggr2WR88/wDl/hKj2ZjOKx1EZkQazbX8BD723gVXbDwDg6wMXnR3LbUPb07mlpmyLGFNaBNu/hp8+gc2fVh2Q2iQG+t0CSddBaKSphOLhVEakQXywbg8PvruewpJyAvx8uKxXa/54XnvaNtP0PREjCg/YxePnxbDty6p3oQ0Kh7OGQ+JF0Gk0+AeayyleQWVE6tXR0nL+8nE68/9nr8o7oF0U//j92bRuqjvOijS4o3n2LJgN78Ku78A6YdZaWGvoPAo6jYK252pAqjQolRGpN7/kFHL7m2vZuDcPHx/409AO3HVBR82KEWlomT/Cmrnw40IoLTy+v2V3+8xH51HQ8mwNRhVjVEakXixZn8l9i34kv7iMyEaBPDOuJ7/r2Nx0LBHvUXoU0j+A1S/D7u+P72/eGXpPhM4XQdN4c/lETqAyInWquKycmYt/4rWVvwDQt21Tnru6Ny3Dg80GE/EWB3dA6quw9nUoOnbDSF9/SLwY+t4E8QN1BkRcjsqI1JldB49wx5tr+WF3LgCThrTnnuSO+Pv5Gk4m4uEc5fZiZKvnwtbPgWML94S1hj7XQa+J0CTaZEKR01IZkTqRkr6PP/93HXlHywgPCeDpcT0Y1ll/+YnUq4JsWDsPUl+D3F3H97c/3z4LclYy+OmveXF9+lMqZ+zFZduYueQnAHq1iWDW+N600h1zReqHZUHGKnssSPqH4Ci194c0hV7XQNL1WpRM3I7KiJyRD3/YW1lEbhiUwAMjOxPor8syInWuYlru6rmwf9Px/a37Qp8boetYCNA/AsQ9qYxIraXuPMg9C38A4ObBCUwb3cVwIhEPlLXBnpb7w4Lj03IDQqH776HvjRDTw2w+kTqgMiK1svNAITfPS6WkzEFyl2geGJloOpKI5zi4Aza+ay9Otm/D8f3NOtpnQXpcBSERxuKJ1DWVEXFa7pFSrn9tNQcLS+jeKpxnruqphcxEzlTeXtj4nn2X3D2px/f7+tt3ye17E7QdrGm54pFURsQpJWUObn1jDdv3FxIbHszca/sQGqg/RiK1UpgD6e/bZ0B2rqRySq6Pr108ul0OiWN0gzrxePoUkRqzLIsH31vPd9sP0jjIn7nX9aVFmBYzE3FKeRn89JE9JXf7MrDKj3+tzQC7gHS5BBq3MJdRpIGpjEiNzf56G4tSd+Pn68Os8b1IjAkzHUnEfRzNg7Q34H9z4HDG8f2xvewC0vVSCG9tLp+IQSojUiMf/bCXf3z2MwCPXtyV8zrpX20iNXJ4F/zvBftMSHGevS806vhAVK0JIqIyIr8tdedB/nxsCu+N5yYw4RzdXEvkN+1JhVXPw8b3j1+KadYRBtwOZ4/TmiAiJ1AZkdPKOHCkcgrvBYnRPDhKU3hFTslRDj8vgVWz7FVSKyQMgQF3QIcLwFeLAor8msqInFLukVKue+17DhaW0K1VGM9erSm8Iqe07Uv4dCrst1ckxjcAul9hnwlp2d1sNhEXpzIi1bIsiz+9ncb2/YXEhAcz99q+msIrUp2DO+CzafDzJ/Z2cIS9MmrfmyEsxmg0EXehTxep1sc/ZrJ8836C/H2Ze21fojWFV6Sq4gL45ilYOQvKi8HHD/rfCkPu1+qoIk5SGZGTFBaX8ddP7Btx3T60A11iNYVXpJJlwfpFkPIw5O+197U7Dy58HFp0NhpNxF2pjMhJZn21lay8o7SJDOWW37UzHUfEdWT+AEvuPz44NSIeRvzNXq5dy7SL1JrKiFSxfX8BL6/YDsDDF3UhOMDPcCIRF1CYA1/+BVL/A1j2XXMHT4EBf4IAXcIUOVMqI1LJsiwe/Sid0nKLoZ2ac36iFjYTL+dwwNrX4PNH4Wiuva/bFTB8BoS3MplMxKOojEilpen7WL55P4F+vjw8pis+Ou0s3ixrPXx8N+xebW+37A4jn4D4gWZziXgglREB4GhpOTM+Sgfg5t8lkNCskeFEIoYUF8DXM+G7OfbKqYFNYNhD0O9m8NVlS5H6oDIiAMz5eht7DhcRGx7M7UM7mI4jYsamj2HJfZC3x97uMhYunAlhsUZjiXg6lREh48AR5izbBsBDF3XR4mbifQ5n2LNkfl5sb0fEw6h/Qsdks7lEvIQ+dYS/fJJOSZmDQR2iGNmtpek4Ig2nvBS+mw1f/x1Kj9hLuA+6EwbfA4GhptOJeA2VES/31c/ZpKTvw9/Xh0c1aFW8hWXB5k/hi79A9kZ7X5uBcNHTWrhMxACVES9WXHZ80Or1g9pyVnQTw4lE6pmjHNLfhxVPwb4N9r6QSEh+DHqO18JlIoaojHixl1fsYEdOIc2bBHHn+WeZjiNSf8pL4ccF8M3TcGCrvS+wsX1Du4F3QaMos/lEvJzKiJfae7iIWV/afyk/OKozTYIDDCcSqQelRZD2Bnz7L8jdZe8LjoBz/gj9boHQSKPxRMSmMuKl/rp4E0Wl5fRt25SxPbWSpHiY4gJY8wqsfA4Ks+19jVrAwDugzw0QpEuSIq5EZcQLrdyawyc/ZuLrA49erEGr4kFKi+B/L9hnQooO2fvC42DQXdDrGggIMZtPRKqlMuJlSssdPPKhPXvgmnPi6RobbjiRSB1wOOwxIV8+Bnm77X2R7e2b2XW/EvwDzeYTkdNSGfEy/1n5C1uyC2gaGsCU4R1NxxE5c9u+hKUPw7719nZYa3v59rOv1PLtIm5CZcSLZOcf5ZnPtwBw/4WdiQjVvxbFjWVtgJSHYdsX9nZQuH0mpP8kCAg2m01EnKIy4kX+vuQnCorL6NE6nCv7xJmOI1I7uXvgq7/CujcBy141td/N8Lt7NTtGxE2pjHiJNb8c5N219s2/pl/SDV9fDVoVN3M0z14n5LvZUHbU3tf1Ujj/YYhsZzabiJwRlREvUO6w+L8P7EGr4/rE0TMuwmwgEWfl7obXRsOhX+ztNgPsVVNb9zEaS0TqhsqIF3jzfzvZlJlHWLA/913YyXQcEefkZ8F/LraLSHgbGPl36DRKS7eLeBCVEQ93oKCYf3z2MwD3jOhEVOMgw4lEnFCw3y4iB7dBRBu4bjFEaLyTiKfxNR1A6tc/l/5M3tEyEmPCGN+vjek4IjV35CDMuwRyfoawVnDtRyoiIh5KZcSD/bDrMG+vtu/HMeOSrvj76XCLmyg6DK+PheyN0DjaLiJN2xoOJSL1RZ9OHsrhsHj4w41YFlzaqxV922rKo7iJ4nx443LI/AFCm8HEDyGqvelUIlKPVEY81KLU3fyw6zCNAv2YOrKz6TgiNVNSCPN/D3vWQEhTmPgBtNCfXxFPpzLigXKPlPL4pz8BMPmCjrQI02qU4gZKi+CtqyBjlb2a6oT3oGU306lEpAGojHigpz/fzIHCEjq0aMx1g9qajiPy28qKYcE1sGM5BDaGa96B2F6mU4lIA1EZ8TCbMvOYt+oXAKZf3JUADVoVV1dWAv+9FrZ+DgGh8IeFENfXdCoRaUD6pPIglmXx8AcbcFgwunsMgzo0Mx1J5PQKc2DR9bB5CfgHw9VvQ/xA06lEpIFp0TMP8sG6vaz+5RAhAX48ODrRdByRUys6DKtmwXdzoKQA/AJh3HxoN8R0MhExoFZnRmbPnk1CQgLBwcEkJSWxYsWK075+/vz59OjRg9DQUGJiYrj++us5cOBArQJL9fKPlvLXxZsAuGNYB1pFhBhOJFKN4nxY9g945mxY/g+7iMT0gAnvw1kXmE4nIoY4XUYWLFjA5MmTmTZtGmlpaQwePJiRI0eSkZFR7eu/+eYbJk6cyI033sjGjRtZuHAhq1ev5qabbjrj8HLcvz7fwv78YtpGhXLT4ATTcUSqKjkC3z5rl5CvHoPiXGieCOPegFuWQdtBphOKiEE+lmVZzryhf//+9O7dmzlz5lTuS0xMZOzYscycOfOk1//zn/9kzpw5bNu2rXLfc889xxNPPMGuXbtq9D3z8vIIDw8nNzeXsLAwZ+J6hZ+y8hj97DeUOyxevb4vQzu1MB1JxFZWDKn/gRX/hIJ99r7I9jD0Qeh6Kfj6mc0nIvWqpp/fTp0ZKSkpITU1leTk5Cr7k5OTWblyZbXvGThwILt372bx4sVYlsW+fftYtGgRo0ePPuX3KS4uJi8vr8pDqmdZFv/3/gbKHRYjukariIhrKC+zS8izvWHJvXYRiWgDl8yG27+H7leoiIhIJacGsObk5FBeXk50dHSV/dHR0WRlZVX7noEDBzJ//nzGjRvH0aNHKSsr4+KLL+a555475feZOXMm06dPdyaa13pn7Z7KQasPj+lqOo54O8uCnxdDyiNwYIu9r0kM/O5e6DUB/APN5hMRl1SrAaw+Pj5Vti3LOmlfhfT0dO68804efvhhUlNT+fTTT9mxYweTJk065e8/depUcnNzKx81vZzjbXKPlDLz2KDVO88/S4NWxazdqfDaaHh7vF1EQiJhxN/gzjToe6OKiIicklNnRpo1a4afn99JZ0Gys7NPOltSYebMmQwaNIh7770XgLPPPptGjRoxePBgHnvsMWJiYk56T1BQEEFBQc5E80r/XPpz5UqrN56rQatiyMEd8MUM2Piuve0fDOfcBudOhuBwo9FExD04dWYkMDCQpKQkUlJSquxPSUlh4MDqFyo6cuQIvr5Vv42fn32t2Mmxs3KC9btzeeN/OwGYcUlXAv21fp00sCMH4dOpMKvvsSLiAz3Gw59S4YJHVEREpMacXvRsypQpTJgwgT59+jBgwABeeuklMjIyKi+7TJ06lT179jBv3jwAxowZw80338ycOXMYMWIEmZmZTJ48mX79+hEbG1u3P42XKHdYPPT+eiwLLukZy8D2WmlVGlDpUfj+RVj+pD1FF6DdUEj+C7TsbjabiLglp8vIuHHjOHDgADNmzCAzM5Nu3bqxePFi4uPjAcjMzKyy5sh1111Hfn4+s2bN4s9//jMREREMGzaMxx9/vO5+Ci/z9uoMftidS5Mgf6aN0kqr0oA2fwaf/Blyj43jiu4Gw2dAh/PN5hIRt+b0OiMmaJ2R4w4UFDPsyWXkFpXyyJguXD9IY0WkATgcsOzvsOzYPyLCWsGwh+DscZqiKyKnVNPPb92bxs38fclP5BaV0iUmjAnnxJuOI96g6BC8ewtsWWpv973ZPhsSGGo2l4h4DJURN7Lml4MsTN0NwF/GdsPfT4NWpZ5lbYAF18ChHfYsmYuegZ5Xm04lIh5GZcRNlJU7eOj9DQCM6xNHUnxTw4nE461fBB/+CUqP2KunjnvDvqmdiEgdUxlxE/NW7eSnrHwiQgO4f2Rn03HEk5WX2iuofve8vd1+GFw+F0IjzeYSEY+lMuIG9uUd5amUzQDcN6IzkY20kqXUk4JsWHg97PzG3j53ij1QVYNURaQeqYy4gb9+somC4jJ6xEVwVd8403HEU+1eAwsmQP5eCGwMY+dAl4tNpxIRL6Ay4uJWbs3hwx/24usDfx3bDV/f6u8BJFJrjnJYPReWToPyEog6C66aD807mU4mIl5CZcSFWZbF45/+BMA158TTrZWW15Y6lvEdLL4Xsn60tztfZJ8RCfbu9XxEpGGpjLiwb7ce4IfduQQH+HLX+WeZjiOeJHcPfP4IrF9obweFw7Bp0O8WOMUduEVE6ovKiAub9dUWAK7u14aoxrqLsdSB0qOwahaseNKesosP9J4I5z8MjXSPIxExQ2XERaXuPMh32w8S4OfDzYPbmY4j7s6y4OfF8NmDcOgXe19cfxj5BMT2NJlMRERlxFXN/mobAJf1ak1sRIjhNOLW9v8MS+6H7V/Z201iYPhfoPsVuiQjIi5BZcQFpe/N44ufsvH1gUnntTcdR9xVcQF89Tf4/kVwlIFfIAz8k712SFBj0+lERCqpjLig2V9vBWBU9xgSmjUynEbc0rYv4aO74HCGvd1pNIx4DCJ1yU9EXI/KiIvZvr+AT9ZnAnD70A6G04jbKTpsrxeS9oa9Hd4GLnoazrrAaCwRkdNRGXExLy7bjmXB+Z1bkBijtR7ECT99Ah9PgYIswMeepnv+w7okIyIuT2XEhew9XMS7absBuE1nRaSmCnPshcs2vmtvR3WAi2dB/ACzuUREakhlxIX8e8V2SsstzmkXSVJ8U9NxxNVZFqxfBEvug6KD4ONnD1A97wEI0AwsEXEfKiMuIqegmLe+twcbaqyI/Ka8vfDx3bD5U3s7uhtcMgtie5nNJSJSCyojLuLVb3dwtNTB2a3DObeDVsKUUygusKfqfvMMFOeBbwAMuQ8GTQb/QNPpRERqRWXEBeQdLWXeyp0A3HZeB3y0EJX8WmkRrHkFVjwFR3Lsfa2S7LEh0V3MZhMROUMqIy7g9VU7yS8u46wWjUnuEm06jriSshJImwfL/wn59pRvmibAeVPtFVR9/czmExGpAyojhhWVlPPKNzsAuG1oe3x9dVZEgPIy+PFt+PpxyD22cFl4nH1JpsfV4BdgNp+ISB1SGTHs7dUZHCgsoXXTEMacHWs6jpjmcNhTdL/6Gxy0709E42j43b323XX9dfdmEfE8KiMGlZQ5eGn5dgAmDWmPv5+v4URihGVBzmZ7Cfe18yA73d4fGgXn3g19boTAULMZRUTqkcqIQe+n7SEz9yjNmwRxRVJr03GkIR05CDuWwdYvYNtXkLf7+NeCw+31QvpPgqAm5jKKiDQQlRFDyh0Wc5bZp+FvHpxAcIAGInq08lLYvcY++7HtC9izFrCOf90vCOIHQocLoNc1EBJhKqmISINTGTFkyYZMduQUEh4SwPj+8abjSH0pPABfTIeN79nrgpyoeSK0HwYdhkGbgboUIyJeS2XEAMuymP2VfVbkuoFtaRykw+BxHA5YNx9S/g+KDtn7QiKh/VC7gLQfBmEasCwiAiojRqTuPER6Zh5B/r5cN7Ct6ThS1/alwydTIGOVvR3dDS6cCfHngq8GKYuI/JrKiAHzVtmrrV7SM5amjbSEt8coKYRlT8CqWeAog4BGMHQq9P8j+Ol/NRGRU9HfkA1sf34xSzbYK2lOOKet2TBSdzZ/Bp/cc3yBss4XwYV/h4g4s7lERNyAykgDW7A6g9Jyi55xEXRvHW46jpyp3D3w6f2w6SN7O6w1jPoHdB5lNpeIiBtRGWlAZeUO3vyf/S/nCedoBo1bK9hvD1Bd/g8oKQAfPxhwOwy5H4Iam04nIuJWVEYa0Bc/ZbM39yhNQwMYfXaM6TjirKJDsOlj2PCOvWCZ5bD3t+4HFz0NLbuZzSci4qZURhrQG9/ZA1ev7BunRc7cRXEB/LzELiBbPwdH6fGvxfaGPjdAzz9oloyIyBlQGWkg2/cXsGJLDj4+cI0WOXNtpUWwJcUuIJs/g7Ki419r0RW6XWY/ItuZyygi4kFURhrIG9/ZY0WGdmpBXKRW2nRZP7wNi++D4tzj+yLbQ7fL7QLSItFcNhERD6Uy0gCOlJSxMHUXABMG6KyISyorgaXT4PuX7O3wuGNnQC6HlmeDj4/ZfCIiHkxlpAF8uG4v+UfLiIsMYchZzU3HkV/Lz4KF1x1fMXXIA/asGI0DERFpECoj9cyyrMoVV6/pH4+vr/6F7VIy/gf/nQgFWRAUDpe9BJ0uNJ1KRMSrqIzUs7UZh0nPzCPQ35cr+2g1TpdhWbD6Zfh0qj1DpnkiXDUfotqbTiYi4nVURupZxXTeMWfrPjQuo7QIPvmzvWgZQNdL4eJZWqxMRMQQlZF6lFNQzCc/2vehmaiBq67hcAYsuAYyfwAfXxg+AwbcoQGqIiIGqYzUo/+u2UVJuYOzW4fTIy7CdBzZ9hUsugGKDkJoFFzxKrQbYjqViIjXUxmpJ+UOi/nf6T40LqG0CFY8aT8sB8T2gitf1x11RURchMpIPfnqp2z2HC4iIjSAMT1iTcfxXlu/sMeHHNphb/e6BkY9CQHBZnOJiEgllZF6Mq/iPjR9dB8aI/Kz7JkyG9+1t5vEwsi/Q+LFGh8iIuJiVEbqwS85hSzfvB8fH/hD/zam43gXRzmseQW+mAHFefYg1f5/hKFTIaiJ6XQiIlINlZF6UDGdd0jH5sRHNTKcxovsXQcf3w1719rbsb1hzDMQ08NkKhER+Q0qI3WsqKScham7AQ1cbTBH8+Crv8H3L9oDVIPC4YKHIel68NUlMhERV6cyUsc++mEvuUWltG4awnmdWpiO4/k2fQSL74V8ez0Xul0BI/4GTaLN5hIRkRpTGalDlmUx77tfAPhD/3j8dB+a+lN0GJbcBz8usLcj28HoJ6H9MKOxRETEeSojdSg9M48Ne/II9PNlXF+tYVFvtn0J798O+XvtAaqDJtt32dV0XRERt6QyUoeWrM8C4LxOzYnUfWjqXkkhpDwCq/9tb0e2h0tfhLi+ZnOJiMgZURmpI5ZlsXiDPW5hVPcYw2k80K7V8N6tcHCbvd33Zhg+HQI1W0lExN2pjNSRLdkFbN9fSKCfL8MSNXC1zpSVwLLH4Zun7JkyTWLhklnQ4XzTyUREpI6ojNSRxevtsyKDz2pGWHCA4TQeYl86vHcLZK23t7tfCaOegJCmZnOJiEidUhmpI59usMeLjNQlmjNnWbBqlr2KankJhETCRU9D17Gmk4mISD3wrc2bZs+eTUJCAsHBwSQlJbFixYrTvr64uJhp06YRHx9PUFAQ7du355VXXqlVYFe0bX8BP2Xl4+/rw/BErW9xRhzl9iqqSx+yi0jHC+G271REREQ8mNNnRhYsWMDkyZOZPXs2gwYN4sUXX2TkyJGkp6fTpk3192G58sor2bdvH3PnzqVDhw5kZ2dTVlZ2xuFdRcVZkYEdmhEeqks0tVZWYl+W2fge4AMjn4B+N+vGdiIiHs7HsizLmTf079+f3r17M2fOnMp9iYmJjB07lpkzZ570+k8//ZSrrrqK7du3ExkZWauQeXl5hIeHk5ubS1hYWK1+j/o0+tkVbNybx98v685V/XRjvFopKYQFE2DbF+AbAJe9BN0uM51KRETOQE0/v526TFNSUkJqairJyclV9icnJ7Ny5cpq3/Phhx/Sp08fnnjiCVq1akXHjh255557KCoqOuX3KS4uJi8vr8rDVWUcOMLGvXn4+fqQ3LWl6TjuqegQzBtrF5GAUBj/toqIiIgXceoyTU5ODuXl5URHVx0XER0dTVZWVrXv2b59O9988w3BwcG899575OTkcNttt3Hw4MFTjhuZOXMm06dPdyaaMUuOrS3SPyFSC53VRn4WvH4ZZG+E4HD4wyKI62c6lYiINKBaDWD1+dU1fMuyTtpXweFw4OPjw/z58+nXrx+jRo3iqaee4rXXXjvl2ZGpU6eSm5tb+di1a1dtYjaIxZpFU3sHd8ArI+wi0jgarl+iIiIi4oWcOjPSrFkz/Pz8TjoLkp2dfdLZkgoxMTG0atWK8PDwyn2JiYlYlsXu3bs566yzTnpPUFAQQUFBzkQzYs/hIn7YdRgfHxjRVbNonLJvo31GpCALmraFCe9DZILpVCIiYoBTZ0YCAwNJSkoiJSWlyv6UlBQGDhxY7XsGDRrE3r17KSgoqNy3efNmfH19ad26dS0iu46KWTR920bSoolu0lZju76HV0faRaRFV7jhMxUREREv5vRlmilTpvDyyy/zyiuvsGnTJu6++24yMjKYNGkSYF9imThxYuXrx48fT1RUFNdffz3p6eksX76ce++9lxtuuIGQkJC6+0kMWHJs1dWR3TRwtca2fgHzLoGjudC6H1z/CTTRfz8REW/m9Doj48aN48CBA8yYMYPMzEy6devG4sWLiY+PByAzM5OMjIzK1zdu3JiUlBT+9Kc/0adPH6Kiorjyyit57LHH6u6nMGBf3lHW7DwEwIUqI7/NsmD1y/DpVHCUQvvzYdzrutGdiIg4v86ICa64zsi8Vb/w8Acb6d0mgndvG2Q6jmsrPAAf3A6bl9jbXS+DS18Ef80+EhHxZDX9/Na9aWqp4sZ4ozSL5vS2fw3v3mqPD/ELhOEzoP8kraoqIiKVVEZqIaegmO93HARghBY6q15ZCXz1V/j2X4AFzTrC5XMh5mzTyURExMWojNTC0o37cFhwdutw4iJDTcdxPQe2wTs3wt40ezvpOhjxN40PERGRaqmM1ELFqqsju+kSTRWWBT+8DYvvgZICCI6Ai5+FLpeYTiYiIi5MZcRJhwpLWLntAKApvVUczYVP/gzrF9rb8YPsm92Fu/daMiIiUv9URpyUkr6PcodFYkwYbZvpsgMAmT/Cgmvg8E7w8YPzpsLgKeDrZzqZiIi4AZURJ1VcohmlsyK2IwfhrashbzdEtLEHqer+MiIi4gSVESfkFpXyzdYcQDfGA+wxIh/daReRyHZw85cQ0tR0KhERcTO1umuvt/pi0z5Kyy06RjemQ4vGpuOYl/oqbPoIfAPgildUREREpFZURpyweL19Y7wLNYsG9qXbS7sDXPAIxPYym0dERNyWykgNFRSXsXzLfgBGdffy8SKlRbDoBig7Ch0ugHNuN51IRETcmMpIDX35UzYlZQ7aNWtEp+gmpuOY9dk02L8JGrWAsXPAV3+MRESk9vQpUkNLjt2LZmT3lvh4831V0j+ENXPt55e+AI1bmM0jIiJuT2WkBo6UlPH1z/YlGq9edfXwLvjwDvv5oLugw/lm84iIiEdQGamB5ZtzKCotJy4yhK6xp74FskcrL4N3b7FXWo3tDUMfMp1IREQ8hMpIDazbdRiAwWc1995LNMv/ARkrIbAJXDEX/ANNJxIREQ+hMlIDmzLzAOgS46VnRX75FpY/YT+/6Gl7gTMREZE6ojJSAxVlJNEby8iRg/DuzWA5oMd4OPv3phOJiIiHURn5DTkFxWTnF+PjA51betmUXsuCD/8EeXsgsj2M+ofpRCIi4oFURn5DxVmRtlGNaBTkZbfyWTMXfvr4+HLvQVoCX0RE6p7KyG84fonGy86KbP0cltxvPx8+HWJ7Go0jIiKeS2XkN2zKzAcgsaUXjRfZnQoLJoKjDLpdAefcZjqRiIh4MJWR3+B1g1dztsD8K6C0ENoPs5d799bpzCIi0iBURk6juKycrdkFAHTxhsXO8vbC65dC0UF7YbMrX9d6IiIiUu9URk5jy74CyhwW4SEBxIQHm45Tv4oOwRuXQ+4uiOoAf1ioAasiItIgVEZO48TBqx698mrJEXjzKshOhyYxMOE9aNTMdCoREfESKiOnUTl41ZPHi5SXwaLrYdd3EBwO17wDEW1MpxIRES+iMnIaHj941bLgo7tg86fgHwxXL4DorqZTiYiIl1EZOQXLstiU5eH3pPn8UVj3Bvj4wRWvQvwA04lERMQLqYycQlbeUQ4fKcXf14cOLTxwIOeq5+HbZ+znY/4FnUcZjSMiIt5LZeQU0vfaZ0XaN29McICf4TR17IcF8NmD9vPzH4HeE8zmERERr6Yycgoeuwz8ti/hg2Mrqp5zG5x7t9k8IiLi9VRGTsEjZ9Jk/lh1mffkv2p1VRERMU5l5BQ8bibN4V0w//dQkg9tB8PY2eCrwy8iIubp06gaR0rK2HGgEPCQMlJ0yL7fTEEWtOgC494A/yDTqURERACVkWr9lJWPZUHzJkE0b+LmH9plxfD2H2D/T9Ak1l7mPSTCdCoREZFKKiPV8JhLNA4HvDcJdn4LQWFwzSIIb206lYiISBUqI9XwmJk0nz8CG98F3wAY97pWVxUREZekMlKNipk0br3y6v9ehJXP2s8vmQXtzjMaR0RE5FRURn7F4bD4yd0v02z6CJbcbz8//2HocZXZPCIiIqehMvIruw4dobCknEB/X9o1a2Q6jvMy/gfv3ARY0OcGOHeK6UQiIiKnpTLyKxXLwHeKboK/n5v958nZCm9dBWVHoeNIGPkPLWomIiIuz80+beuf2w5eLdgPb1wGRQehVRJcMRf8/E2nEhER+U0qI7+S7o7LwDsc8N6tcHgnNE2AqxdAoBteYhIREa+kMvIrbrnGyHfPw7YvwD8Yrn4LGjc3nUhERKTGVEZOkFtUyp7DRYAblZE9a+Hz6fbzC2dCi0SzeURERJykMnKCiim9rSJCCA8JMJymBorzYdEN4CiFxIsh6XrTiURERJymMnKCdHe7RPPJPXBoB4S1houf1cwZERFxSyojJ6gYL9LFHWbS/PA2/Pg2+PjC5S9DSFPTiURERGpFZeQEm9xlJs2BbfDJn+3n502F+AFm84iIiJwBlZFjysod/LzPDcpIWYk9TqSkAOIHweA/m04kIiJyRlRGjtmRU0hJmYNGgX60iQw1HefUvpwBmevsyzKX/Rt8/UwnEhEROSMqI8dUDF7tHBOGr6+LDgTd8jmsfM5+fsnzEN7KbB4REZE6oDJyTLqrLwOfvw/en2Q/73szdB5tNo+IiEgdURk5xqUHrzocdhEp3A8tukLyY6YTiYiI1BmVkWNcehn4Vc/Bti/BPwSueAUCgk0nEhERqTMqI0BOQTH784vx8YHOLV3sMs2eVPhihv185N+hRWezeUREROqYygjHz4okRDUiNNDfcJoTlJfBh3eCowy6jIXe15pOJCIiUudURoD0vS56iWbta7BvAwRHwOintNy7iIh4pFqVkdmzZ5OQkEBwcDBJSUmsWLGiRu/79ttv8ff3p2fPnrX5tvVmkyvOpDlyEL48NlB16DRoFGU2j4iISD1xuowsWLCAyZMnM23aNNLS0hg8eDAjR44kIyPjtO/Lzc1l4sSJnH/++bUOW19ccibN1zOh6BC06AJ9bjCdRkREpN44XUaeeuopbrzxRm666SYSExN55plniIuLY86cOad936233sr48eMZMMC17qNSXFbOtv0FgAuVkX3psHqu/fzCv4OfC41jERERqWNOlZGSkhJSU1NJTk6usj85OZmVK1ee8n2vvvoq27Zt45FHHqnR9ykuLiYvL6/Ko75s2VdAmcMiIjSAmHAXmDJrWfDp/WCVQ+IYaDfEdCIREZF65VQZycnJoby8nOjo6Cr7o6OjycrKqvY9W7Zs4YEHHmD+/Pn4+9fsX/gzZ84kPDy88hEXF+dMTKdUjhdpGYaPKwwQ3fQR7FgO/sGQ/FfTaUREROpdrQaw/vpD27Ksaj/Iy8vLGT9+PNOnT6djx441/v2nTp1Kbm5u5WPXrl21iVkj6a602FlpESydZj8feCc0jTebR0REpAE4NRihWbNm+Pn5nXQWJDs7+6SzJQD5+fmsWbOGtLQ07rjjDgAcDgeWZeHv78/SpUsZNmzYSe8LCgoiKCjImWi15lIzaVbOgsMZENYKzp1sOo2IiEiDcOrMSGBgIElJSaSkpFTZn5KSwsCBA096fVhYGOvXr2fdunWVj0mTJtGpUyfWrVtH//79zyz9GbIsy3Vm0uTuhm+esp8PnwGBjczmERERaSBOT9OYMmUKEyZMoE+fPgwYMICXXnqJjIwMJk2y7yg7depU9uzZw7x58/D19aVbt25V3t+iRQuCg4NP2m9CZu5RcotK8ff14azoxmbDpDwMpUegzUDodrnZLCIiIg3I6TIybtw4Dhw4wIwZM8jMzKRbt24sXryY+Hh7fENmZuZvrjniKiou0XRo0Zggfz9zQXauhA3vAD72/WdcYSCtiIhIA/GxLMsyHeK35OXlER4eTm5uLmFhdXc55bkvtvBkymYu7dWKp8f1rLPf1ymOcnhpCGSth6TrYMy/zOQQERGpYzX9/Pbqe9NsynKBwatr59lFJCgchv2fuRwiIiKGePXSniO7xRARGki/BEP3fSk6BF/+xX4+dCo0amYmh4iIiEFeXUbG9IhlTI9YcwG+fhyOHIDmnaHvTeZyiIiIGOTVl2mMyv4Jvn/Jfn7hTPALMJtHRETEEJUREywLltxn33+m02hof/LCbyIiIt5CZcSE1S/DjmX2/WdGPGY6jYiIiFEqIw0tZyssPTZrZvgMiGxnNo+IiIhhKiMNqbwM3rsFyoqg3XnQ92bTiURERIxTGWlIK56EPakQHA6XzAZf/ecXERHRp2FD2ZMKyx63n49+CsJbmc0jIiLiIlRGGkLJEXj3Vnv2TNfLoPsVphOJiIi4DJWRhvD5I3BgCzSJgdFPmk4jIiLiUlRG6tvWL44vbnbJ8xAaaTaPiIiIi1EZqU9HDsIHt9vP+90CHc43m0dERMQFqYzUp8X3QH4mRJ0FF0w3nUZERMQlqYzUl/WLYMM74OMHl70IgaGmE4mIiLgklZH6kLsHPpliPx9yH7RKMptHRETEhamM1DWHAz64DY7m2iVk8J9NJxIREXFpKiN1bfW/YfvX4B8Cl74EfgGmE4mIiLg0lZG6lLUeUh62nyf/BZp1MJtHRETEDaiM1JWiQ7DgGig7Ch2GQ9+bTCcSERFxCyojdcHhgHduhkO/QEQ8XPYS+PiYTiUiIuIWVEbqwrK/w9YU8A+GcW9olVUREREnqIycqZ8/PX433jH/gpizzeYRERFxMyojZ+LANnj3Fvt5v1ugx1Vm84iIiLghlZHaKim0B6wW50Jcf0j+q+lEIiIibkllpDYsCz68E7LToXE0/P4/4B9oOpWIiIhbUhmpje/mwIZF4OtvF5GwGNOJRERE3JbKiLN++QaWPmQ/T/4rxA8wm0dERMTNqYw4I28vLLwOrHLofiX0v9V0IhEREbenMlJTZSXw34lQuB+iu9nTeLWwmYiIyBlTGampz6bC7tUQHA7jXofAUNOJREREPILKSE2k/gdWvwz4wGUvQ2Q704lEREQ8hsrIb9n2FXx8t/186IPQMdlsHhEREQ+jMnI62T/Bf6+1B6yePQ5+d6/pRCIiIh5HZeRUCrLhzd/bK6y2GQgXP6cBqyIiIvVAZaQ6pUXw1tVwOMMeH3LVfPAPMp1KRETEI6mM/JrDAe9Ngj1rIKQpjF8IoZGmU4mIiHgslZFf+3IGpL8PvgEwbj4062A6kYiIiEdTGTnR2tfhm6ft5xc/B20Hmc0jIiLiBVRGKmz/Gj6ebD//3X3Q82qTaURERLyGygjA/p9hwURwlEG3K+z1RERERKRBqIwU7If5x6bwxp0DlzyvKbwiIiINyLvLSGkRvH01HN4JTRPgqjchINh0KhEREa/ivWXE4YD3/3js5ncR8IeF0CjKdCoRERGv471lxMcHoruBXyCMewOanWU6kYiIiFfyNx3AGB8f+N099j1nIuJMpxEREfFa3ntmpIKKiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFG1KiOzZ88mISGB4OBgkpKSWLFixSlf++677zJ8+HCaN29OWFgYAwYM4LPPPqt1YBEREfEsTpeRBQsWMHnyZKZNm0ZaWhqDBw9m5MiRZGRkVPv65cuXM3z4cBYvXkxqaipDhw5lzJgxpKWlnXF4ERERcX8+lmVZzryhf//+9O7dmzlz5lTuS0xMZOzYscycObNGv0fXrl0ZN24cDz/8cI1en5eXR3h4OLm5uYSFhTkTV0RERAyp6ee3U2dGSkpKSE1NJTk5ucr+5ORkVq5cWaPfw+FwkJ+fT2Rk5ClfU1xcTF5eXpWHiIiIeCanykhOTg7l5eVER0dX2R8dHU1WVlaNfo8nn3ySwsJCrrzyylO+ZubMmYSHh1c+4uLinIkpIiIibqRWA1h9fHyqbFuWddK+6rz11ls8+uijLFiwgBYtWpzydVOnTiU3N7fysWvXrtrEFBERETfg78yLmzVrhp+f30lnQbKzs086W/JrCxYs4MYbb2ThwoVccMEFp31tUFAQQUFBzkQTERERN+XUmZHAwECSkpJISUmpsj8lJYWBAwee8n1vvfUW1113HW+++SajR4+uXVIRERHxSE6dGQGYMmUKEyZMoE+fPgwYMICXXnqJjIwMJk2aBNiXWPbs2cO8efMAu4hMnDiRf/3rX5xzzjmVZ1VCQkIIDw+vwx9FRERE3JHTZWTcuHEcOHCAGTNmkJmZSbdu3Vi8eDHx8fEAZGZmVllz5MUXX6SsrIzbb7+d22+/vXL/tddey2uvvXbmP4GIiIi4NafXGTFB64yIiIi4n3pZZ0RERESkrqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImJUrcrI7NmzSUhIIDg4mKSkJFasWHHa1y9btoykpCSCg4Np164dL7zwQq3CioiIiOdxuowsWLCAyZMnM23aNNLS0hg8eDAjR44kIyOj2tfv2LGDUaNGMXjwYNLS0njwwQe58847eeedd844vIiIiLg/H8uyLGfe0L9/f3r37s2cOXMq9yUmJjJ27Fhmzpx50uvvv/9+PvzwQzZt2lS5b9KkSfzwww+sWrWqRt8zLy+P8PBwcnNzCQsLcyauiIiIGFLTz29/Z37TkpISUlNTeeCBB6rsT05OZuXKldW+Z9WqVSQnJ1fZN2LECObOnUtpaSkBAQEnvae4uJji4uLK7dzcXMD+oURERMQ9VHxu/9Z5D6fKSE5ODuXl5URHR1fZHx0dTVZWVrXvycrKqvb1ZWVl5OTkEBMTc9J7Zs6cyfTp00/aHxcX50xcERERcQH5+fmEh4ef8utOlZEKPj4+VbYtyzpp32+9vrr9FaZOncqUKVMqtx0OBwcPHiQqKuq036dCXl4ecXFx7Nq1S5d1XIyOjWvScXFdOjauScelZizLIj8/n9jY2NO+zqky0qxZM/z8/E46C5KdnX3S2Y8KLVu2rPb1/v7+REVFVfueoKAggoKCquyLiIhwJioAYWFh+kPionRsXJOOi+vSsXFNOi6/7XRnRCo4NZsmMDCQpKQkUlJSquxPSUlh4MCB1b5nwIABJ71+6dKl9OnTp9rxIiIiIuJdnJ7aO2XKFF5++WVeeeUVNm3axN13301GRgaTJk0C7EssEydOrHz9pEmT2LlzJ1OmTGHTpk288sorzJ07l3vuuafufgoRERFxW06PGRk3bhwHDhxgxowZZGZm0q1bNxYvXkx8fDwAmZmZVdYcSUhIYPHixdx99908//zzxMbG8uyzz3L55ZfX3U/xK0FBQTzyyCMnXeoR83RsXJOOi+vSsXFNOi51y+l1RkRERETqku5NIyIiIkapjIiIiIhRKiMiIiJilMqIiIiIGOW2ZWT27NkkJCQQHBxMUlISK1asOO3rly1bRlJSEsHBwbRr144XXnihgZJ6F2eOS2ZmJuPHj6dTp074+voyefLkhgvqhZw5Nu+++y7Dhw+nefPmhIWFMWDAAD777LMGTOs9nDku33zzDYMGDSIqKoqQkBA6d+7M008/3YBpvYuznzMVvv32W/z9/enZs2f9BvQklht6++23rYCAAOvf//63lZ6ebt11111Wo0aNrJ07d1b7+u3bt1uhoaHWXXfdZaWnp1v//ve/rYCAAGvRokUNnNyzOXtcduzYYd15553Wf/7zH6tnz57WXXfd1bCBvYizx+auu+6yHn/8cev777+3Nm/ebE2dOtUKCAiw1q5d28DJPZuzx2Xt2rXWm2++aW3YsMHasWOH9frrr1uhoaHWiy++2MDJPZ+zx6bC4cOHrXbt2lnJyclWjx49GiasB3DLMtKvXz9r0qRJVfZ17tzZeuCBB6p9/X333Wd17ty5yr5bb73VOuecc+otozdy9ricaMiQISoj9ehMjk2FLl26WNOnT6/raF6tLo7LpZdeal1zzTV1Hc3r1fbYjBs3znrooYesRx55RGXECW53maakpITU1FSSk5Or7E9OTmblypXVvmfVqlUnvX7EiBGsWbOG0tLSesvqTWpzXKRh1MWxcTgc5OfnExkZWR8RvVJdHJe0tDRWrlzJkCFD6iOi16rtsXn11VfZtm0bjzzySH1H9Di1umuvSTk5OZSXl590Y77o6OiTbshXISsrq9rXl5WVkZOTQ0xMTL3l9Ra1OS7SMOri2Dz55JMUFhZy5ZVX1kdEr3Qmx6V169bs37+fsrIyHn30UW666ab6jOp1anNstmzZwgMPPMCKFSvw93e7j1bj3Pa/mI+PT5Vty7JO2vdbr69uv5wZZ4+LNJzaHpu33nqLRx99lA8++IAWLVrUVzyvVZvjsmLFCgoKCvjuu+944IEH6NChA1dffXV9xvRKNT025eXljB8/nunTp9OxY8eGiudR3K6MNGvWDD8/v5PaaXZ29kkttkLLli2rfb2/vz9RUVH1ltWb1Oa4SMM4k2OzYMECbrzxRhYuXMgFF1xQnzG9zpkcl4SEBAC6d+/Ovn37ePTRR1VG6pCzxyY/P581a9aQlpbGHXfcAdiXNi3Lwt/fn6VLlzJs2LAGye6u3G7MSGBgIElJSaSkpFTZn5KSwsCBA6t9z4ABA056/dKlS+nTpw8BAQH1ltWb1Oa4SMOo7bF56623uO6663jzzTcZPXp0fcf0OnX1/4xlWRQXF9d1PK/m7LEJCwtj/fr1rFu3rvIxadIkOnXqxLp16+jfv39DRXdf5sbO1l7FlKu5c+da6enp1uTJk61GjRpZv/zyi2VZlvXAAw9YEyZMqHx9xdTeu+++20pPT7fmzp2rqb31wNnjYlmWlZaWZqWlpVlJSUnW+PHjrbS0NGvjxo0m4ns0Z4/Nm2++afn7+1vPP/+8lZmZWfk4fPiwqR/BIzl7XGbNmmV9+OGH1ubNm63Nmzdbr7zyihUWFmZNmzbN1I/gsWrz99mJNJvGOW5ZRizLsp5//nkrPj7eCgwMtHr37m0tW7as8mvXXnutNWTIkCqv//rrr61evXpZgYGBVtu2ba05c+Y0cGLv4OxxAU56xMfHN2xoL+HMsRkyZEi1x+baa69t+OAezpnj8uyzz1pdu3a1QkNDrbCwMKtXr17W7NmzrfLycgPJPZ+zf5+dSGXEOT6WdWwkp4iIiIgBbjdmRERERDyLyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFEqIyIiImKUyoiIiIgYpTIiIiIiRqmMiIiIiFH/Dz3L1//uAOo3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.016796066997483282 0.07346432855368562\n",
      "90 0.07909204873483928 0.2751986895385197\n",
      "95 0.09532450370252392 0.31491465589104833\n",
      "99 0.1320168295307611 0.3700949495105308\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 0.012160265192768834 0.07145898533012404\n",
      "90 0.047915725836543216 0.2727736298325866\n",
      "95 0.053717750535610495 0.3153216864822403\n",
      "99 0.06597230544940674 0.36794891171362076\n"
     ]
    }
   ],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 5000/5000 [05:13<00:00, 15.93it/s]\n"
     ]
    }
   ],
   "source": [
    "a = []\n",
    "for i in tqdm(range(1000)):\n",
    "    torch.manual_seed(i)\n",
    "    latent_dim = 32\n",
    "    z = torch.randn((1, latent_dim)).to(device)\n",
    "    size = decoder(z)\n",
    "    size = size.squeeze().detach().to('cpu').numpy()\n",
    "    size[size < 1e-3] = 0\n",
    "    size /= size.sum()\n",
    "\n",
    "    dis = []\n",
    "    for j in range(1000):\n",
    "        loss = JSD(size, sizedata[j])\n",
    "        dis.append(loss)\n",
    "\n",
    "    pair = np.argmin(dis)\n",
    "    a.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 0.37469881772994995 0.36950908958911893 11.19027829170227\n",
      "200 0.36915746331214905 0.35975850641727447 21.188878059387207\n",
      "300 0.35969191789627075 0.35673402369022367 32.09577918052673\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# dataset.extend([tune_dataset[i] for i in ran_index])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m2000\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 13\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     14\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     15\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m plot_every \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, optimizer)\u001b[0m\n\u001b[1;32m      4\u001b[0m sum_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m seq_tensor, size_tensor, target_tensor \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m----> 6\u001b[0m     seq_tensor \u001b[39m=\u001b[39m inputTensor(seq_tensor)\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     size_tensor \u001b[39m=\u001b[39m size_tensor\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m     target_tensor \u001b[39m=\u001b[39m target_tensor\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[12], line 14\u001b[0m, in \u001b[0;36minputTensor\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(lines\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     13\u001b[0m         size \u001b[39m=\u001b[39m lines[line][i]\n\u001b[0;32m---> 14\u001b[0m         tensor[i][line][size] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "# optimizer = torch.optim.Adam([{'params': s2h.parameters()}], lr=lr)\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "\n",
    "s_time = time.time()\n",
    "plot_every = 100\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i)\n",
    "    ran_index = np.random.permutation(len(tune_dataset))[0:1000]\n",
    "    # dataset.extend([tune_dataset[i] for i in ran_index])\n",
    "    dataloader = DataLoader(dataset, batch_size=2000, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, time.time() - s_time)\n",
    "        if avg_loss / plot_every < 0.1:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1000/1000 [00:58<00:00, 17.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "a = set()\n",
    "for i in tqdm(range(1000)):\n",
    "    torch.manual_seed(i)\n",
    "    latent_dim = 32\n",
    "    z = torch.randn((1, latent_dim)).to(device)\n",
    "    size = decoder(z)\n",
    "    size = size.squeeze().detach().to('cpu').numpy()\n",
    "    size[size < 1e-3] = 0\n",
    "    size /= size.sum()\n",
    "\n",
    "    dis = []\n",
    "    for j in range(1000):\n",
    "        loss = JSD(size, sizedata[j])\n",
    "        dis.append(loss)\n",
    "        # if loss < 0.02:\n",
    "        #     a.add(j)\n",
    "\n",
    "    pair = np.argmin(dis)\n",
    "    a.add(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsds = np.zeros((1000, 1000))\n",
    "for i in range(1000):\n",
    "    for j in range(1000):\n",
    "        jsds[i][j] = JSD(sizedata[i], sizedata[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.0712686 , 0.0989841 , 0.10461984, 0.10732551,\n",
       "       0.11061315, 0.11413909, 0.11538036, 0.11660952, 0.11782312,\n",
       "       0.12270552, 0.12357579, 0.12381709, 0.12831985, 0.12900488,\n",
       "       0.13021052, 0.13290488, 0.13328528, 0.13464494, 0.13588953,\n",
       "       0.13698097, 0.13924533, 0.14027519, 0.14082266, 0.14095486,\n",
       "       0.14102039, 0.14222849, 0.14290341, 0.14387662, 0.14660579,\n",
       "       0.14678407, 0.14694698, 0.14717077, 0.14719107, 0.14890525,\n",
       "       0.14939437, 0.14958512, 0.15188435, 0.1524103 , 0.15251156,\n",
       "       0.15369764, 0.1559504 , 0.15639397, 0.15688702, 0.15898742,\n",
       "       0.15899087, 0.16149691, 0.16396171, 0.16474164, 0.16477304,\n",
       "       0.1657369 , 0.16581361, 0.16621777, 0.16631866, 0.1666306 ,\n",
       "       0.16702055, 0.16783559, 0.16789516, 0.16937711, 0.16999767,\n",
       "       0.17006917, 0.17089419, 0.17237322, 0.17269899, 0.17295262,\n",
       "       0.17311279, 0.1738042 , 0.17434601, 0.17441435, 0.17492409,\n",
       "       0.17498089, 0.17590966, 0.17673751, 0.1767769 , 0.17685239,\n",
       "       0.17733793, 0.17733859, 0.17755393, 0.177951  , 0.17813919,\n",
       "       0.17815045, 0.17871073, 0.17879594, 0.17892494, 0.1791654 ,\n",
       "       0.17921196, 0.1801971 , 0.1807614 , 0.18117546, 0.18125231,\n",
       "       0.18135442, 0.18198672, 0.18222288, 0.18241376, 0.18258302,\n",
       "       0.18262841, 0.18266575, 0.18293546, 0.18320284, 0.18338127,\n",
       "       0.18383586, 0.18386672, 0.18406265, 0.18417186, 0.18449631,\n",
       "       0.18469741, 0.18472404, 0.18474315, 0.18486096, 0.18635224,\n",
       "       0.18651748, 0.18662395, 0.18693884, 0.18698377, 0.18719315,\n",
       "       0.18781946, 0.18836521, 0.18884797, 0.18956284, 0.18983969,\n",
       "       0.19001956, 0.19022332, 0.1902604 , 0.19076237, 0.19097689,\n",
       "       0.19151861, 0.19152719, 0.19177246, 0.19188523, 0.19231322,\n",
       "       0.19295623, 0.1931194 , 0.19357893, 0.19446891, 0.19458126,\n",
       "       0.19465321, 0.19476943, 0.19499326, 0.19504991, 0.19534656,\n",
       "       0.19608496, 0.19671116, 0.19693814, 0.19783898, 0.19786838,\n",
       "       0.1982728 , 0.19864062, 0.19908045, 0.19972837, 0.20060773,\n",
       "       0.20075074, 0.20101976, 0.20118712, 0.20120148, 0.20132543,\n",
       "       0.20145173, 0.20146317, 0.20179804, 0.20192126, 0.20241434,\n",
       "       0.20271094, 0.20293146, 0.20404812, 0.20447014, 0.20474019,\n",
       "       0.20492326, 0.20495016, 0.20538361, 0.20572343, 0.20576312,\n",
       "       0.20612296, 0.20626037, 0.20639919, 0.20696949, 0.20826728,\n",
       "       0.20885927, 0.20887153, 0.20897336, 0.20911203, 0.20918792,\n",
       "       0.20986857, 0.21055266, 0.21120023, 0.21134245, 0.21140708,\n",
       "       0.21195211, 0.21231916, 0.2123584 , 0.21285352, 0.21290893,\n",
       "       0.21292164, 0.21301341, 0.21313637, 0.21361757, 0.21449909,\n",
       "       0.21477319, 0.21501816, 0.21519042, 0.21579753, 0.21616203,\n",
       "       0.21639014, 0.21642339, 0.21683652, 0.21722668, 0.21743179,\n",
       "       0.21770664, 0.21771587, 0.21818101, 0.2184033 , 0.21898291,\n",
       "       0.21900266, 0.21918248, 0.21927371, 0.21934558, 0.21940255,\n",
       "       0.21944778, 0.21947962, 0.21966281, 0.21973549, 0.21975252,\n",
       "       0.21984796, 0.22036319, 0.22090047, 0.22147298, 0.22156073,\n",
       "       0.22157982, 0.22160772, 0.2217407 , 0.22227462, 0.2225324 ,\n",
       "       0.22270144, 0.22272389, 0.22278591, 0.22326229, 0.22348136,\n",
       "       0.22352013, 0.22385144, 0.22414126, 0.2246308 , 0.2248496 ,\n",
       "       0.22529469, 0.22537828, 0.22555107, 0.22572806, 0.22654558,\n",
       "       0.22682547, 0.22714561, 0.22725781, 0.22762842, 0.22779162,\n",
       "       0.22790799, 0.22804214, 0.22859007, 0.22862069, 0.22929094,\n",
       "       0.22962914, 0.22984653, 0.229852  , 0.23023409, 0.23025379,\n",
       "       0.23072017, 0.23144365, 0.23192409, 0.23249705, 0.23277592,\n",
       "       0.23339237, 0.23361231, 0.23404151, 0.23500077, 0.23502402,\n",
       "       0.23528629, 0.23531764, 0.23537534, 0.23545058, 0.23553714,\n",
       "       0.23563498, 0.23594436, 0.23602324, 0.23612106, 0.23627042,\n",
       "       0.23637232, 0.23641386, 0.23655684, 0.23687559, 0.23708677,\n",
       "       0.23755224, 0.23795358, 0.23853469, 0.23857294, 0.23888587,\n",
       "       0.23897496, 0.23925227, 0.23958056, 0.2398841 , 0.24003631,\n",
       "       0.2403348 , 0.24041492, 0.24134958, 0.24139177, 0.24148694,\n",
       "       0.24198383, 0.24254673, 0.24316905, 0.24340846, 0.24362422,\n",
       "       0.24400348, 0.24474908, 0.24526318, 0.24555859, 0.24594612,\n",
       "       0.24610359, 0.24627118, 0.24662194, 0.24665233, 0.24693703,\n",
       "       0.24764772, 0.24771226, 0.24774597, 0.24781387, 0.2481245 ,\n",
       "       0.24854935, 0.24855978, 0.24890758, 0.24895393, 0.24916782,\n",
       "       0.25007749, 0.25015382, 0.2503157 , 0.25059996, 0.25081337,\n",
       "       0.25185066, 0.25229848, 0.25245055, 0.25289129, 0.25331318,\n",
       "       0.2533505 , 0.25387384, 0.25513259, 0.25525203, 0.25658397,\n",
       "       0.25737137, 0.25754914, 0.25771241, 0.25822603, 0.2583451 ,\n",
       "       0.25841318, 0.25947204, 0.25995359, 0.26002243, 0.26028342,\n",
       "       0.2603924 , 0.26097775, 0.26111144, 0.26114015, 0.26182433,\n",
       "       0.26185643, 0.26188322, 0.26223239, 0.26225009, 0.26235135,\n",
       "       0.26273467, 0.26360623, 0.26368101, 0.26409175, 0.26454406,\n",
       "       0.26492847, 0.26572468, 0.2659745 , 0.26600219, 0.26604093,\n",
       "       0.26605977, 0.26621498, 0.26650083, 0.26679597, 0.26723988,\n",
       "       0.26749053, 0.26758702, 0.26797904, 0.26821143, 0.26922938,\n",
       "       0.2704383 , 0.27076334, 0.27088524, 0.27114124, 0.27173614,\n",
       "       0.27216456, 0.27220683, 0.27262086, 0.2730809 , 0.27316495,\n",
       "       0.27450374, 0.27507727, 0.27579355, 0.2764037 , 0.27649103,\n",
       "       0.27655353, 0.27664809, 0.2766998 , 0.27776084, 0.27813467,\n",
       "       0.27830197, 0.27896472, 0.27904399, 0.27916736, 0.27925291,\n",
       "       0.27948919, 0.27952152, 0.27956051, 0.27958542, 0.27991394,\n",
       "       0.28011877, 0.28037961, 0.2804614 , 0.28073998, 0.28117339,\n",
       "       0.28124103, 0.28125505, 0.28157578, 0.28256693, 0.2828647 ,\n",
       "       0.28289178, 0.28305584, 0.28308632, 0.28308888, 0.28324722,\n",
       "       0.28330103, 0.28443007, 0.2855556 , 0.28612781, 0.28697198,\n",
       "       0.28733192, 0.28735517, 0.28749486, 0.28882455, 0.28893915,\n",
       "       0.28907958, 0.28969924, 0.28980439, 0.29037346, 0.29133943,\n",
       "       0.29138968, 0.29182541, 0.29194244, 0.29200094, 0.29218193,\n",
       "       0.29344498, 0.29582257, 0.29653179, 0.29702478, 0.29855564,\n",
       "       0.29856078, 0.29924234, 0.29950011, 0.30010905, 0.30033352,\n",
       "       0.30214963, 0.3033076 , 0.30453139, 0.30488249, 0.30516044,\n",
       "       0.30526441, 0.30598692, 0.30701168, 0.30769269, 0.3083303 ,\n",
       "       0.3087238 , 0.31053665, 0.31084658, 0.31132074, 0.31251387,\n",
       "       0.31274526, 0.31293362, 0.313071  , 0.31308757, 0.31324137,\n",
       "       0.31340172, 0.31362299, 0.31385183, 0.31410506, 0.31470971,\n",
       "       0.31474685, 0.31606627, 0.31628787, 0.3165189 , 0.31691141,\n",
       "       0.31711742, 0.31787109, 0.3191704 , 0.31955154, 0.32019762,\n",
       "       0.32047015, 0.32234559, 0.32263437, 0.32388264, 0.3240724 ,\n",
       "       0.32415959, 0.32491949, 0.32504911, 0.32576048, 0.32628987,\n",
       "       0.32675987, 0.32676961, 0.32695558, 0.32826221, 0.32881858,\n",
       "       0.33030781, 0.33097694, 0.33278704, 0.33485632, 0.33657012,\n",
       "       0.3366979 , 0.33770067, 0.33772824, 0.33937936, 0.34059703,\n",
       "       0.34201449, 0.34201449, 0.34258309, 0.34270976, 0.3435679 ,\n",
       "       0.34365086, 0.34368412, 0.34396319, 0.34424276, 0.34725629,\n",
       "       0.34745442, 0.34759679, 0.34797266, 0.34931396, 0.34974604,\n",
       "       0.35014401, 0.3523915 , 0.35342043, 0.35475613, 0.35480829,\n",
       "       0.35692294, 0.35778698, 0.35883807, 0.35956611, 0.36076733,\n",
       "       0.36090711, 0.36223054, 0.36277884, 0.36427886, 0.36464182,\n",
       "       0.3656838 , 0.36915682, 0.36916669, 0.36944556, 0.37045361,\n",
       "       0.37124232, 0.37222947, 0.37246051, 0.37473295, 0.37590078,\n",
       "       0.37647556, 0.3785312 , 0.38074558, 0.38074558, 0.38106817,\n",
       "       0.38294032, 0.38330713, 0.38645282, 0.38658861, 0.38739757,\n",
       "       0.38914022, 0.38947831, 0.39065371, 0.39065731, 0.39248109,\n",
       "       0.39301651, 0.39325365, 0.39468744, 0.39596497, 0.39868444,\n",
       "       0.40145191, 0.40210522, 0.40219783, 0.40222876, 0.40311735,\n",
       "       0.40368127, 0.40388573, 0.40417469, 0.40662853, 0.40806479,\n",
       "       0.40812735, 0.41164539, 0.41339782, 0.41340748, 0.41632306,\n",
       "       0.4172043 , 0.417269  , 0.41786134, 0.41823751, 0.41832528,\n",
       "       0.42135339, 0.42250736, 0.42340714, 0.42341431, 0.42345112,\n",
       "       0.42492069, 0.4252607 , 0.4253492 , 0.42558176, 0.42681452,\n",
       "       0.42734174, 0.4285238 , 0.42865908, 0.42908997, 0.43231994,\n",
       "       0.43236087, 0.43311036, 0.43364912, 0.43403486, 0.43457784,\n",
       "       0.43641591, 0.4364635 , 0.43674932, 0.43693445, 0.43717427,\n",
       "       0.43814126, 0.44163962, 0.44238783, 0.44615488, 0.44630847,\n",
       "       0.44636155, 0.44997723, 0.45197698, 0.45461751, 0.45573847,\n",
       "       0.45637225, 0.45665096, 0.45674449, 0.45801141, 0.45866295,\n",
       "       0.45938689, 0.46274876, 0.46289106, 0.46463032, 0.46532701,\n",
       "       0.46698491, 0.46900063, 0.46927868, 0.47050006, 0.4706744 ,\n",
       "       0.47099214, 0.47231024, 0.47316277, 0.47362045, 0.47380178,\n",
       "       0.47483461, 0.47538298, 0.47605786, 0.48345764, 0.48379543,\n",
       "       0.48572486, 0.48608424, 0.48656386, 0.48779508, 0.49040046,\n",
       "       0.49433991, 0.49658484, 0.49799957, 0.50100009, 0.50169463,\n",
       "       0.50197682, 0.50275894, 0.50316597, 0.50599772, 0.50763714,\n",
       "       0.50873996, 0.51766124, 0.51896299, 0.51931578, 0.51946329,\n",
       "       0.51964744, 0.52061773, 0.52078469, 0.52420036, 0.52473401,\n",
       "       0.52769723, 0.52816925, 0.52826627, 0.53435258, 0.53471712,\n",
       "       0.53503891, 0.53516557, 0.53526988, 0.5395895 , 0.5430481 ,\n",
       "       0.54345403, 0.54364195, 0.54394195, 0.54446072, 0.54819886,\n",
       "       0.55221941, 0.55425822, 0.55651156, 0.55713981, 0.55713981,\n",
       "       0.55713981, 0.55713981, 0.55937648, 0.55953154, 0.56022074,\n",
       "       0.562201  , 0.56262987, 0.56497805, 0.56712355, 0.56773685,\n",
       "       0.5684331 , 0.56902858, 0.56975874, 0.57052667, 0.57177206,\n",
       "       0.57296696, 0.57387289, 0.57434217, 0.57459712, 0.57498937,\n",
       "       0.57632497, 0.5772049 , 0.57811433, 0.57915652, 0.57924155,\n",
       "       0.57938496, 0.57960526, 0.58077804, 0.58155701, 0.58184202,\n",
       "       0.58194423, 0.58285732, 0.58353303, 0.58411188, 0.58426232,\n",
       "       0.58483549, 0.58489462, 0.58492763, 0.58557324, 0.58574965,\n",
       "       0.58662816, 0.58669104, 0.58686393, 0.58711208, 0.58724735,\n",
       "       0.58745714, 0.58916123, 0.58924739, 0.58953028, 0.58969996,\n",
       "       0.58979823, 0.58993588, 0.59016007, 0.59024723, 0.59056136,\n",
       "       0.59058079, 0.59100415, 0.59132933, 0.59136644, 0.59168155,\n",
       "       0.59208726, 0.5925581 , 0.59265375, 0.59297742, 0.59301719,\n",
       "       0.59333787, 0.59344549, 0.59348494, 0.59350135, 0.59391524,\n",
       "       0.59420711, 0.5943282 , 0.59439991, 0.59445306, 0.59449285,\n",
       "       0.59468332, 0.59478413, 0.59484802, 0.59535395, 0.5953704 ,\n",
       "       0.59546682, 0.59558256, 0.59559649, 0.59586556, 0.59587736,\n",
       "       0.59600706, 0.59626496, 0.59642563, 0.59651898, 0.59654944,\n",
       "       0.59659429, 0.59670746, 0.59722868, 0.59724953, 0.59735165,\n",
       "       0.59744122, 0.59767431, 0.59778276, 0.59827992, 0.59835819,\n",
       "       0.59845644, 0.59858812, 0.59861667, 0.5989453 , 0.59898782,\n",
       "       0.59904065, 0.59920902, 0.59925823, 0.59949493, 0.59960255,\n",
       "       0.59964764, 0.59970026, 0.59994921, 0.59999061, 0.60070822,\n",
       "       0.60083072, 0.60087235, 0.60116484, 0.60151962, 0.60182405,\n",
       "       0.60216109, 0.60231715, 0.60238584, 0.60267546, 0.60295766,\n",
       "       0.60302033, 0.60311346, 0.60317113, 0.60338865, 0.60359637,\n",
       "       0.60414658, 0.60417474, 0.60423159, 0.60464779, 0.60470478,\n",
       "       0.60472254, 0.6050119 , 0.60508936, 0.60517311, 0.60524052,\n",
       "       0.6052662 , 0.60539681, 0.60547162, 0.60595083, 0.60606526,\n",
       "       0.60609718, 0.60610258, 0.60631853, 0.60660426, 0.60693419,\n",
       "       0.60702112, 0.60702152, 0.60712814, 0.60724399, 0.60742562,\n",
       "       0.60751242, 0.60754729, 0.60760677, 0.60804985, 0.6080941 ,\n",
       "       0.60815761, 0.60817031, 0.6085903 , 0.60871943, 0.60883582,\n",
       "       0.60884227, 0.6090395 , 0.60924093, 0.60940234, 0.60944426,\n",
       "       0.60945427, 0.60987094, 0.61002459, 0.61008427, 0.61013957,\n",
       "       0.6102212 , 0.61042352, 0.61044386, 0.61071951, 0.61072962,\n",
       "       0.61079096, 0.61082276, 0.61087635, 0.61105787, 0.61108179,\n",
       "       0.61114502, 0.61121769, 0.61130169, 0.6113294 , 0.61137894,\n",
       "       0.61137894, 0.61154942, 0.61156151, 0.6116976 , 0.61196253,\n",
       "       0.61205161, 0.61243292, 0.61247024, 0.61265191, 0.61270809,\n",
       "       0.61281137, 0.61282159, 0.61330038, 0.61337133, 0.6136343 ,\n",
       "       0.61373917, 0.61374855, 0.6137777 , 0.61380172, 0.61398216,\n",
       "       0.61519818, 0.61596933, 0.616133  , 0.61614625, 0.6161936 ,\n",
       "       0.61635543, 0.61645556, 0.61646733, 0.61654916, 0.61666782,\n",
       "       0.61669966, 0.61679626, 0.61700956, 0.61767956, 0.61784814,\n",
       "       0.61809157, 0.61846965, 0.61853303, 0.61866373, 0.61912186,\n",
       "       0.61927461, 0.6194242 , 0.61952805, 0.61957847, 0.6201609 ,\n",
       "       0.62041038, 0.62113907, 0.62120027, 0.62157555, 0.6215841 ,\n",
       "       0.62171428, 0.6219509 , 0.62259515, 0.62341043, 0.62368792,\n",
       "       0.62405424, 0.62499239, 0.62502632, 0.62516169, 0.62542184,\n",
       "       0.62552526, 0.62670995, 0.62710415, 0.62732843, 0.62752052,\n",
       "       0.62753248, 0.62826179, 0.62885748, 0.63008957, 0.63024161,\n",
       "       0.63371014, 0.63424071, 0.63438122, 0.63438122, 0.63438122,\n",
       "       0.63438122, 0.63438122, 0.63438122, 0.63438122, 0.63445863,\n",
       "       0.63450407, 0.63461787, 0.63470187, 0.63496908, 0.63889871,\n",
       "       0.64016471, 0.64182081, 0.64714655, 0.65172356, 0.65782506,\n",
       "       0.66201054, 0.66255408, 0.66631046, 0.66790865, 0.6697703 ,\n",
       "       0.6697703 , 0.68314195, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.69314718,\n",
       "       0.69314718, 0.69314718, 0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(jsds[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   3,   1,   1,  95,   1,   1,   1,   1,   1,   2,   1,  96,\n",
       "         1,   1,   1,   4,   1,   2,   1,   1,   1,   1,   2,   1,   3,\n",
       "         1,   1,   1,   2,   3,   1,   1,   2,   4,   2,   1,   1,   1,\n",
       "         2,   1,   2,   1,   1,   1,   1,   1,   1,   1,   4,   3,   1,\n",
       "         3,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n",
       "         1,   1,   6,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   1,   6,\n",
       "         2,   1,   1,   4,   1,   2,   1,   1,   1,   1,   1,   1,   4,\n",
       "         1,   1,   1,   1,   1,   1,   1,  76,   2,   1,   1,  29,   1,\n",
       "         1,   4,   5,   1,   1,   1,   1,   1,   2,   1,   1,   5,   7,\n",
       "         4,   1,   2,   1,   5,   1,   4,   1,   1,   1,   1,  20,   7,\n",
       "         5,   1,   1,   1,   1,   1,   1,   1,   5,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   4,   2,   1,   1,   1,   1,   1,   2,\n",
       "         1,   1,   1,   1,   1,   1,   2,   1,   1,   2,   1,   1,   1,\n",
       "         1,   1,   4,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,\n",
       "         1,   1,   2,   1,   1,   1,   1,   2,   1,   3,   1,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   4,\n",
       "         2,   1,   1,   3,   1,   1,   1,   1,   1,   1,   1,   1,   6,\n",
       "         2,   1,   1,   1,   1,   3,   1,   1,   1,   1,   1,   2,   1,\n",
       "         1,   1,   1,   1,   3,   1,   1,   1,   1,   8,   1,   2,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   1,\n",
       "         1,   1,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         6,   1,   1,   6,   1,   1,   1,   1,   8,   1,   1,   1,   1,\n",
       "         1,   1,   1,   2,   1,   1,   1,   2,   1,   1,   1,   1,   1,\n",
       "        63,  47,   1,   2,   1,   1,   6,   1,   1,   1,   1,   1,   1,\n",
       "         3,   1,   1,   3,   1,   1,   1,   1,   1,   1,   3,   1,   3,\n",
       "         1,   1,   1,   3,   2,   2,   1,   2,   1,   1,   1,   1,   1,\n",
       "         3,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,   2,\n",
       "         3,   1,   3,   1,   1,   1,   1,   5,   1,   1,   2,   4,   1,\n",
       "         1,   1,   1,   1,   1,   3,   1,   2,   1,   1,   1,   1,   1,\n",
       "         2,   1,   1,   1,   1,   1,   6,   1,   6,   1,   1,   1,   1,\n",
       "         1,   1,   1,   4,   1,   1,   1,   1,   1,   1,  73,  95,   1,\n",
       "         1,   5,   1,   1,   1,   1,  80,   1,   1,   1,   1,   1,   1,\n",
       "        45,  96,   1,   1,   1,   1,   1,   1,   4,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   8,  84,   8,   8,   8,   8,  96,   8,   1,\n",
       "         1,   1,   1,   1,   1,   1,   1,   1,   1,   2,   1,   1,   1,\n",
       "         2,   1,   2,   1,   1,   1,   1,   1,   6,   1,   1,   1,   1,\n",
       "         1,   1,   2,   2,   1,   1,   1,   4,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   7,   1,   2,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,  96,   4,   1,   4,   1,   1,  79,   1,   1,\n",
       "         1,   1,   1,   1,   2,   1,   1,   1,  70,  98,   1,   1,   1,\n",
       "         5,   2,  96,   1,   1,   1,  65,  48,   2,   1,  87,   1,   4,\n",
       "         1,   1,   1,   1,   1,  91,  67,  85,  86,   1,   3,   1,   2,\n",
       "         1,   1,   2,   3,   1,   1,   1,   1,   1,  85,  98,   7,  96,\n",
       "         1,   1,   1,   1,   1,   1,  94,  93,  38,  84,   5,   1,   1,\n",
       "         1,   1,  29, 100,   1,   1,   1,  96,  97,  79,  75,   1,   1,\n",
       "         2,   1,  95,  84,   1,   7,   1,  90,   1,   1,   1,  79,   3,\n",
       "         1,   6,   2,   1,   1,   1,   1,   1,   4,   1,   1,   1,   1,\n",
       "         1,   3,  96,  96,  91,  86,   2,   1,  98,  51,   1,   1,  71,\n",
       "        96,   4,   1,  74,  65,  96,   1,  96,   1,  51,  99,  95,  89,\n",
       "        86,  96,  80,  86,   1,   1,   1,  97,  86,   1,   1,   1,  98,\n",
       "         1,   4,  79,   1,   1,   1,   3,  98,  92,   1,   8,   1,   2,\n",
       "         1,   1,   4,   1,   1,  96,   1,   1,  90,   1,   1,  96,  75,\n",
       "        98,  78,   1,   1,  94,  75,  96,  90,   1,   1,  95,  86,   6,\n",
       "         1,   1,  83,  95,   1,   1,   1,   1,   1,   1,  62,  98,  75,\n",
       "         1,   1,  63,  79,  96,  91,  98,   6,   1,   1,   1,   1,   3,\n",
       "         7,   2,  96,  88,  66,  96,  88,  96,  72,  94,  54,  67,  84,\n",
       "        62,  98,  84,  54,  85,   2,  95,  61,   1,  96,  78,  95,  49,\n",
       "         1,   5,  62,  96,  96,  84,  90,  96,   1,  96,  84,  96,  83,\n",
       "         1,  98,  86,  91,  89,   1,   1,   1,   1,   1,   2,  98,  88,\n",
       "         1,   1,   1,  94,  86,  76,  96,  98,  75,  92,   1,  94,   1,\n",
       "         1,   5,   1,   1,   1,   6,  52,  96,  91,  96,  96,  34,   1,\n",
       "         1,   1,   1,   1,   1,  97,  51,   1,   1,   1,   1,   1,  74,\n",
       "        99,  88,  87,   2,  70,   1,  96,   1,   1,  84,  97,  93,  90,\n",
       "        98,  98,  92,  85,   2,   1,   1,   3,  81,  96,   1,   1,  98,\n",
       "        90,  92,  99,   1,   1,  98,  77,   1,   1,  92,   1,  82,   1,\n",
       "        97,  81,  91,  98,  98,  93,   1,   1,  62,  97,   1,   1,   1,\n",
       "         1,   6,   1,   1,   1,   1,   1,   1,   1,   1,   3,   1,   2,\n",
       "         3,   1,   1,   1,   1,  26,  86,  96,  84,   3,   1,   1,   6,\n",
       "         1,   1,   1,   1, 100,  83,   2,   1,   1,   1,   1,   1,   1,\n",
       "         1,  92,  98,  98,  71,   1,   1,   1,   2,   1,   1,  97,  93,\n",
       "         1,   1,   1,   1,   1,   1,  92,  98,   1,   2,   1,   1,   1,\n",
       "         1,  89,  94,   1,   1,  96,  94,   1,   1,   1,   1,   1,   1,\n",
       "         1,   5,   2,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1,\n",
       "         1,   1,   1,   1,   1,   2,   1,   1,   1,   1,   1,   1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(jsds < 0.01).sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
