{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 1024\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306934"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.2455940991640091 0.27368770691752436 89551 [0.0001, 0.0001] 28.481378316879272\n",
      "2000 0.31438690423965454 0.2890303600430489 90551 [0.0001, 0.0001] 60.24504494667053\n",
      "3000 0.31526094675064087 0.2871412740200758 91551 [0.0001, 0.0001] 90.5380790233612\n",
      "4000 0.2966729402542114 0.28826461495459077 92551 [0.0001, 0.0001] 122.17063426971436\n",
      "5000 0.27312296628952026 0.3031864850819111 93551 [0.0001, 0.0001] 153.8219175338745\n",
      "6000 0.324411004781723 0.30074237543344495 94551 [0.0001, 0.0001] 185.50472331047058\n",
      "7000 0.29798492789268494 0.29885714343190195 95551 [0.0001, 0.0001] 217.22019386291504\n",
      "8000 0.27968648076057434 0.29683683860301974 96551 [0.0001, 0.0001] 248.84166836738586\n",
      "9000 0.28931739926338196 0.29559150913357735 97551 [0.0001, 0.0001] 280.6368374824524\n",
      "10000 0.3145560324192047 0.29539152133464813 98551 [0.0001, 0.0001] 311.6103343963623\n",
      "11000 0.28427955508232117 0.2939231134355068 99551 [0.0001, 0.0001] 343.81417298316956\n",
      "12000 0.2863001227378845 0.29375478067994115 100551 [0.0001, 0.0001] 375.52176094055176\n",
      "13000 0.275190532207489 0.29233861368894576 101551 [0.0001, 0.0001] 407.18247270584106\n",
      "14000 0.2889230251312256 0.29064783222973345 102551 [0.0001, 0.0001] 437.38989090919495\n",
      "15000 0.29327136278152466 0.29120895837247374 103551 [0.0001, 0.0001] 469.01065158843994\n",
      "16000 0.27729612588882446 0.2899595104008913 104551 [0.0001, 0.0001] 500.6664283275604\n",
      "17000 0.29268747568130493 0.28960611191391944 105551 [0.0001, 0.0001] 532.4654388427734\n",
      "18000 0.3011850416660309 0.2881419765204191 106551 [0.0001, 0.0001] 564.2741372585297\n",
      "19000 0.28677767515182495 0.2880587183237076 107551 [0.0001, 0.0001] 596.0934007167816\n",
      "20000 0.27395352721214294 0.28734403622150423 108551 [0.0001, 0.0001] 627.7728419303894\n",
      "21000 0.275851309299469 0.28504669725894927 109551 [0.0001, 0.0001] 659.6086716651917\n",
      "22000 0.2914777994155884 0.28703529098629954 110551 [0.0001, 0.0001] 691.3980324268341\n",
      "23000 0.26293593645095825 0.284748713940382 111551 [0.0001, 0.0001] 723.1525967121124\n",
      "24000 0.3010149300098419 0.28421824355423453 112551 [0.0001, 0.0001] 754.8551716804504\n",
      "25000 0.29772573709487915 0.2841250868588686 113551 [0.0001, 0.0001] 786.5576543807983\n",
      "26000 0.2839232385158539 0.2835038654357195 114551 [0.0001, 0.0001] 818.2742025852203\n",
      "27000 0.2828552722930908 0.28246318390965464 115551 [0.0001, 0.0001] 849.8567688465118\n",
      "28000 0.2905674874782562 0.28200641137361526 116551 [0.0001, 0.0001] 881.5650038719177\n",
      "29000 0.27241480350494385 0.281644584402442 117551 [0.0001, 0.0001] 912.8527519702911\n",
      "30000 0.2918684780597687 0.2810440246760845 118551 [0.0001, 0.0001] 944.5827264785767\n",
      "31000 0.27400946617126465 0.279892604470253 119551 [0.0001, 0.0001] 976.2160596847534\n",
      "32000 0.2643452286720276 0.27938905458152297 120551 [0.0001, 0.0001] 1007.9487693309784\n",
      "33000 0.27000606060028076 0.27937275560200214 121551 [0.0001, 0.0001] 1039.6576359272003\n",
      "34000 0.2851085364818573 0.27685499599575997 122551 [0.0001, 0.0001] 1071.35498213768\n",
      "35000 0.27066102623939514 0.27834742119908334 123551 [0.0001, 0.0001] 1102.6014280319214\n",
      "36000 0.28575584292411804 0.27752482675015927 124551 [0.0001, 0.0001] 1133.7537925243378\n",
      "37000 0.28286877274513245 0.27677248980104924 125551 [0.0001, 0.0001] 1165.455394744873\n",
      "38000 0.27607640624046326 0.2768384042978287 126551 [0.0001, 0.0001] 1196.5729961395264\n",
      "39000 0.30537107586860657 0.2755525769144297 127551 [0.0001, 0.0001] 1228.1569564342499\n",
      "40000 0.28869926929473877 0.2743932411074638 128551 [0.0001, 0.0001] 1259.965488433838\n",
      "41000 0.27515968680381775 0.27434290397167205 129551 [0.0001, 0.0001] 1291.6866662502289\n",
      "42000 0.2589115798473358 0.2754002095609903 130551 [0.0001, 0.0001] 1323.388920545578\n",
      "43000 0.27005496621131897 0.27364218820631503 131551 [0.0001, 0.0001] 1355.1545386314392\n",
      "44000 0.2650368809700012 0.2739435300976038 132551 [0.0001, 0.0001] 1386.7500221729279\n",
      "45000 0.25103479623794556 0.2731026736497879 133551 [0.0001, 0.0001] 1418.092936038971\n",
      "46000 0.2797176241874695 0.27347011628746987 134551 [0.0001, 0.0001] 1448.5632574558258\n",
      "47000 0.26380017399787903 0.2723865954130888 135551 [0.0001, 0.0001] 1479.924602508545\n",
      "48000 0.2617611587047577 0.27194864831864834 136551 [0.0001, 0.0001] 1509.311198234558\n",
      "49000 0.26640525460243225 0.271317805275321 137551 [0.0001, 0.0001] 1538.481373310089\n",
      "50000 0.281343013048172 0.27067872846126556 138551 [0.0001, 0.0001] 1569.9532487392426\n",
      "51000 0.26395878195762634 0.2709006008952856 139551 [0.0001, 0.0001] 1599.3847649097443\n",
      "52000 0.2754786014556885 0.2704520844370127 140551 [0.0001, 0.0001] 1630.7412321567535\n",
      "53000 0.250186026096344 0.2689974847137928 141551 [0.0001, 0.0001] 1662.2697813510895\n",
      "54000 0.28872206807136536 0.26964954487979415 142551 [0.0001, 0.0001] 1693.6007688045502\n",
      "55000 0.26571235060691833 0.2694270914494991 143551 [0.0001, 0.0001] 1724.9341604709625\n",
      "56000 0.2558484673500061 0.26888530457019805 144551 [0.0001, 0.0001] 1756.4138894081116\n",
      "57000 0.28316158056259155 0.2680725035965443 145551 [0.0001, 0.0001] 1787.7725744247437\n",
      "58000 0.27478092908859253 0.26873748199641706 146551 [0.0001, 0.0001] 1819.300916671753\n",
      "59000 0.25778627395629883 0.2670785448104143 147551 [0.0001, 0.0001] 1850.718185186386\n",
      "60000 0.29244428873062134 0.2685454272478819 148551 [0.0001, 0.0001] 1882.1968593597412\n",
      "61000 0.26595309376716614 0.26789297695457936 149551 [0.0001, 0.0001] 1913.5864465236664\n",
      "62000 0.26815083622932434 0.2670634091347456 150551 [0.0001, 0.0001] 1945.3728535175323\n",
      "63000 0.2642018795013428 0.26601729659736156 151551 [0.0001, 0.0001] 1977.042935848236\n",
      "64000 0.2581470012664795 0.2666071922034025 152551 [0.0001, 0.0001] 2008.6420550346375\n",
      "65000 0.2667488753795624 0.26558909015357496 153551 [0.0001, 0.0001] 2040.380882024765\n",
      "66000 0.2596338391304016 0.2659331038743257 154551 [0.0001, 0.0001] 2072.28439617157\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 15\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mNLLLoss()(output[i], target_tensor[i])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     sum_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m/\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m seq_tensor\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m sum_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[1;32m    158\u001b[0m          grads,\n\u001b[1;32m    159\u001b[0m          exp_avgs,\n\u001b[1;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[1;32m    162\u001b[0m          state_steps,\n\u001b[1;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m func(params,\n\u001b[1;32m    214\u001b[0m      grads,\n\u001b[1;32m    215\u001b[0m      exp_avgs,\n\u001b[1;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    218\u001b[0m      state_steps,\n\u001b[1;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/optim/adam.py:262\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n\u001b[1;32m    261\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m    263\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-0914.pth')\n",
    "        torch.save(s2h, 'model/s2h-0914.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-0914.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7, 4] False\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 11, 10, 7, 7, 4, 6, 7, 6, 6, 4, 13, 10, 8, 8] False\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 6, 7, 11, 10, 4, 10, 4, 11, 4, 4, 13, 7, 13, 16, 4] False\n",
      "[8, 4, 4, 12, 10, 7, 6, 4, 4, 11, 13, 7, 4, 8, 4, 8] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6, 4] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 7, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6] False\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 13, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 4, 4, 8, 13, 7, 4, 4, 4, 15] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 4, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 9, 10, 8, 4] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 8, 4, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] False\n",
      "[8, 11, 4, 4, 13, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8] False\n",
      "[8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4, 8] False\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 13, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] False\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 11, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4] False\n",
      "[8, 4, 6, 8, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] False\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 11, 6, 4, 13, 6, 6, 4, 13, 10, 8, 8, 7, 11, 7, 4] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 11, 11, 13, 2, 11] False\n",
      "[8, 6, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13] False\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8] False\n",
      "[8, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 11, 7, 7, 11, 11, 9, 10, 8, 4, 7, 7, 10, 4, 9, 7] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 4, 13, 7, 13, 16, 4, 4] False\n",
      "[8, 4, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 9, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 6, 8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0029549398469369426 0.018603947682234022 0.0736751094920089\n",
      "1 0.001041595105076414 0.013417267618765303 0.1042314247342685\n",
      "2 0.002364721259777069 0.015521618008335687 0.09041966129037701\n",
      "3 0.0023653360529131244 0.01183452670383181 0.08037825775341918\n",
      "4 0.003258671367803783 0.01880890221517193 0.11541280624370863\n",
      "5 0.03402248341366561 0.05763509427598876 0.05843249684209055\n",
      "6 0.0036487291428318304 0.02238899716632021 0.12974426772181177\n",
      "7 0.0031495603661086747 0.027654901894427836 0.13241771341810435\n",
      "8 0.0022023242022836263 0.020781951538936447 0.13076504713529877\n",
      "9 0.003654301141898871 0.02123955032269649 0.09984615243879238\n",
      "10 0.003244414783476127 0.020854137144992797 0.09568657866273808\n",
      "11 0.0024077990372957194 0.02997117301431226 0.12444833584924142\n",
      "12 0.00384253023874395 0.022636262757333925 0.09530537767837935\n",
      "13 0.0025367823938274798 0.01765128020078547 0.1058306659955253\n",
      "14 0.003776342677191403 0.025362899887202237 0.09919430254377648\n",
      "15 0.004131620115085607 0.025400963498013875 0.10472203100377064\n",
      "16 0.0032369338431650196 0.016329230311943138 0.08211342620477911\n",
      "17 0.001955987921781777 0.030650715009016748 0.1338076640505814\n",
      "18 0.004586428450650064 0.021625941256490377 0.10245314514844706\n",
      "19 0.0034332601947018883 0.019353418835362488 0.10725497113284456\n",
      "20 0.0035546786166102008 0.032065619547912746 0.11493331334406842\n",
      "21 0.002616921299385744 0.019704241846001676 0.10996115827061187\n",
      "22 0.004653416816876234 0.02372949132392399 0.11219160432682712\n",
      "23 0.004485605324114579 0.027391653916253544 0.12260792078861099\n",
      "24 0.004082580074248917 0.027321537124365428 0.10043666212992552\n",
      "25 0.0021261878480271007 0.021037686524279512 0.12676180410838037\n",
      "26 0.0037748866811138683 0.02442285552667315 0.09891848859700221\n",
      "27 0.004901240227057329 0.02712204591037143 0.0730846074721768\n",
      "28 0.001996811544043399 0.01574809773443116 0.1104124405284788\n",
      "29 0.0025198472685871625 0.02375661538259365 0.13541102398212612\n",
      "30 0.002268263867463877 0.011449945207662564 0.08280698660367523\n",
      "31 0.0018340021633819496 0.02225234170318082 0.04868861446856464\n",
      "32 0.0028137416710504494 0.018868314770601358 0.11417219516820398\n",
      "33 0.004305482827320134 0.028792795263763847 0.09203515388689569\n",
      "34 0.0029645784213501144 0.01886425360497865 0.1139012105484031\n",
      "35 0.003045711197645139 0.02789856297254459 0.09665539912796595\n",
      "36 0.0035673132801314193 0.022992480369803052 0.10807572046766079\n",
      "37 0.004064331090505864 0.031385474707510026 0.08587515511754218\n",
      "38 0.003654077825914189 0.019716438520901255 0.07490635155715515\n",
      "39 0.0035143554695079766 0.030058772602403517 0.11519057774560024\n",
      "40 0.0033265364018639177 0.02750306830764121 0.10995695858419105\n",
      "41 0.003529939154761962 0.02756918904334621 0.09332418849834113\n",
      "42 0.002959719145019072 0.013930114246107453 0.08675856182622263\n",
      "43 0.004238656913324223 0.022830511668860368 0.09813521422555554\n",
      "44 0.00362925610416157 0.028590106816072186 0.10673038793264576\n",
      "45 0.004744194470224114 0.028918320428545294 0.10984403945089452\n",
      "46 0.003658922148710844 0.021315692619973584 0.10131811258050497\n",
      "47 0.0031500275057307096 0.01984579197214649 0.09352312731515616\n",
      "48 0.003273906640640422 0.015563658948494959 0.11032567787826482\n",
      "49 0.004286967850952814 0.02788668981188614 0.08237362303622925\n",
      "50 0.0021770847051530064 0.015113206424918927 0.09591794513505242\n",
      "51 0.003342670708309418 0.027499927314914165 0.12283278093789532\n",
      "52 0.00445355576266047 0.030849478886477613 0.10168760146969924\n",
      "53 0.004101863535256076 0.022540146081383335 0.08920788958351944\n",
      "54 0.0021252991959431816 0.018856988055974945 0.11421734937305407\n",
      "55 0.0031577912119031878 0.019706113913345355 0.11864348112353379\n",
      "56 0.0017394697661055486 0.020756159824008455 0.10315916506533149\n",
      "57 0.0044280026560470235 0.018348086571158308 0.08492055734442056\n",
      "58 0.0036271941765715363 0.028106460598640894 0.13174019180173754\n",
      "59 0.004820871096475911 0.024069232919748506 0.0829196744059342\n",
      "60 0.00420801489813522 0.03154715780593971 0.12849005789897305\n",
      "61 0.003538478766792927 0.019616204618774995 0.07898722130023553\n",
      "62 0.0028877894716188048 0.020004740651559168 0.09239453235357423\n",
      "63 0.0013738964942609732 0.01852416467987702 0.1055014573275081\n",
      "64 0.0027697194231047755 0.012116277994673467 0.05288173229587856\n",
      "65 0.0041566225568216 0.031169906370959517 0.08960023315334897\n",
      "66 0.0043875365030305175 0.026434942520733317 0.09539199109774618\n",
      "67 0.0041984240669044395 0.023812435192275716 0.08555761419509013\n",
      "68 0.0037585944415869693 0.027176151974876668 0.11256503973884295\n",
      "69 0.0023476384652448006 0.014996809800631424 0.09782239060365072\n",
      "70 0.0033643138311440753 0.016764061578118654 0.09645210707340438\n",
      "71 0.004425706051393231 0.024632415542034845 0.09208198734110978\n",
      "72 0.003285417401258538 0.023454503866187214 0.08774217028977943\n",
      "73 0.0044787178504136355 0.03339423757541078 0.11424021784363969\n",
      "74 0.003770728671699691 0.04102585024022982 0.11092948548726224\n",
      "75 0.003268496732833564 0.01838774061538923 0.11699496933927611\n",
      "76 0.003175867230263401 0.01580708573482218 0.08164650808916038\n",
      "77 0.0035525588233299666 0.015872380692356435 0.10565417187332546\n",
      "78 0.004545263913133818 0.022215535047108544 0.10872402078336675\n",
      "79 0.0030414218481194413 0.02436257474887486 0.1110879621573403\n",
      "80 0.003241182113452099 0.026552293714815337 0.1301729265588225\n",
      "81 0.004669778005674246 0.025016828332393583 0.09893653787270035\n",
      "82 0.003055016262324345 0.019608078338159855 0.1239971912601549\n",
      "83 0.0033626246680854913 0.021470195663780795 0.10021740963118375\n",
      "84 0.00441166433408669 0.023564382628771356 0.0967919793390512\n",
      "85 0.0030262703341927145 0.01607966685509313 0.08823484783360139\n",
      "86 0.0034193063032329067 0.02388033328421722 0.12232333015293939\n",
      "87 0.0034438559867087915 0.02333159972748769 0.1056548716485837\n",
      "88 0.00477691320823782 0.02501636807097958 0.09942935378279542\n",
      "89 0.0035667095989105717 0.03563300511533071 0.1108539252846088\n",
      "90 0.004440705941415653 0.022138113013170452 0.08669679087887328\n",
      "91 0.003844421502822896 0.025171349714327093 0.1022206765504856\n",
      "92 0.0032764425731227932 0.014821416114734181 0.06611252741223159\n",
      "93 0.002952406445252491 0.020808627097247602 0.08674519970533176\n",
      "94 0.0027150803116068913 0.0206964111146739 0.08863416448218087\n",
      "95 0.0042166363795192105 0.024414814361251397 0.1165412214199228\n",
      "96 0.0034279799321510172 0.02360246402903421 0.10214393376593517\n",
      "97 0.0033754099371149297 0.020600120345680876 0.110467308637513\n",
      "98 0.003880894091339751 0.02316298292349255 0.11332896683555777\n",
      "99 0.0020378185606218548 0.01571847561981176 0.10379974340790225\n",
      "100 0.004635327795523963 0.023852705841357704 0.14448147730158845\n",
      "101 0.002871239010013917 0.020626616778734223 0.10413461725153723\n",
      "102 0.0033068381305274477 0.01544330693457193 0.08852600013635087\n",
      "103 0.0031665402569633115 0.036181088881511766 0.1233876304530812\n",
      "104 0.002075561701223046 0.025227718946565048 0.10170668247132991\n",
      "105 0.0038724254267150813 0.02399750607196922 0.08884725333651065\n",
      "106 0.0029791229319338867 0.02653466154439945 0.12243350086754246\n",
      "107 0.0037534593188883847 0.02622566713761402 0.12065236187576686\n",
      "108 0.004758450347213378 0.026186033413074912 0.1142268612149248\n",
      "109 0.0045314491134156145 0.020685716481982697 0.10615126756144289\n",
      "110 0.0020134689486412645 0.013320279635030605 0.10128003927610213\n",
      "111 0.0031324082090865924 0.021727914985679334 0.07256352029682542\n",
      "112 0.0040513950298945115 0.01705945094235347 0.1300038142473806\n",
      "113 0.003723058860986616 0.04417576875842258 0.12638875080914658\n",
      "114 0.004475566877310765 0.03784634887867663 0.13460038879236425\n",
      "115 0.0041170474061194146 0.022635174570663268 0.10652496228845723\n",
      "116 0.0015684966550412384 0.026563603079989645 0.11258895932137199\n",
      "117 0.0032384893511144793 0.01809399395224792 0.09995493399080008\n",
      "118 0.002054035984171863 0.01723335845528733 0.08121152664316994\n",
      "119 0.002725109816711503 0.02878815070096021 0.138250433309481\n",
      "120 0.003746675987041477 0.020616656421991057 0.10294816240320928\n",
      "121 0.0028382062899894 0.02246549553278996 0.1007969202849352\n",
      "122 0.003891271402587904 0.02144560218419066 0.07073966493734425\n",
      "123 0.002673989051686528 0.023276994060071984 0.07782755384879206\n",
      "124 0.0020878266560325452 0.011927304111807207 0.06420187588269041\n",
      "125 0.004352590104692943 0.03175398160377067 0.10392707744431326\n",
      "126 0.00288642988145286 0.022616106213076463 0.1163588196417264\n",
      "127 0.0036190534374295784 0.02193491530613714 0.08360067206775501\n",
      "128 0.004840940835935084 0.03766955304521476 0.12199576533198289\n",
      "129 0.004131278779338907 0.016933200587147618 0.0880989591230215\n",
      "130 0.0030890647756794906 0.022138996328574197 0.10037983821453339\n",
      "131 0.0014941629531196716 0.019548193034117325 0.12054625282329907\n",
      "132 0.002477486191707256 0.014327654990053139 0.09297148849780391\n",
      "133 0.003463011830493776 0.030670203042934406 0.10515749732217408\n",
      "134 0.0033754068785299537 0.020933805931618046 0.10716850891620353\n",
      "135 0.003148143218694959 0.02071199816754473 0.0951284705418828\n",
      "136 0.0031595034118161508 0.02954977134645287 0.1065321767746078\n",
      "137 0.002590349885577976 0.0173598149634767 0.10609606676928945\n",
      "138 0.0023168600922809125 0.013131503634404532 0.10434777736504153\n",
      "139 0.00452726873181076 0.02775039518632548 0.08968829479249424\n",
      "140 0.00459207136769035 0.029356390675382694 0.10255466969416974\n",
      "141 0.00331651744591507 0.021220116284702932 0.11259642168878181\n",
      "142 0.003167244139784014 0.023577900377841576 0.09193579208817079\n",
      "143 0.002926044999804066 0.0205067303692825 0.12254235741535972\n",
      "144 0.0038355893781716437 0.02516177499818035 0.1003388212676855\n",
      "145 0.0023976718843442095 0.018616333671021307 0.11541129152639154\n",
      "146 0.003608353831263872 0.01689503199753062 0.09738797675314378\n",
      "147 0.003055893708758375 0.020198304529419037 0.13323951883987312\n",
      "148 0.002915046296355671 0.014980613921958996 0.10378567504699937\n",
      "149 0.003295783619354318 0.01949535945720464 0.0822411000210086\n",
      "150 0.0030634365758947816 0.019473438573026962 0.09900538210233237\n",
      "151 0.00444827325206826 0.02148144125383315 0.09353343984330284\n",
      "152 0.0026112386416616935 0.023856908523297218 0.11500151086600185\n",
      "153 0.00480414016687206 0.029043617394351042 0.11219027926767523\n",
      "154 0.0016800670874529307 0.015388617728913565 0.11623202066715323\n",
      "155 0.0021976159014170512 0.014955378385848237 0.08157529893210924\n",
      "156 0.004021457763086748 0.0246569909913948 0.10734611857972207\n",
      "157 0.0029645437447395817 0.022670931906844993 0.10634811159356042\n",
      "158 0.004299810344376204 0.02033177566253702 0.08620933821924906\n",
      "159 0.004226170000917568 0.025001305386356303 0.11268500790480665\n",
      "160 0.0030905835212407568 0.015733708709618734 0.11145145246753675\n",
      "161 0.003901625228336884 0.026330937366068712 0.09443454648062712\n",
      "162 0.0018878246385690238 0.019181800205270983 0.10938622962885272\n",
      "163 0.0039394935178569054 0.02950734800000319 0.1440204289680475\n",
      "164 0.00221723531523754 0.02024514215402183 0.12467210253729477\n",
      "165 0.00414261286002272 0.02132949903925818 0.14427100591157815\n",
      "166 0.003457406045494547 0.029116399967956876 0.09392474762174316\n",
      "167 0.00401222122728343 0.03239534183132814 0.13077286677526326\n",
      "168 0.002767888131246438 0.015496875091579853 0.09132100516134187\n",
      "169 0.0026675443420722325 0.01739080045066544 0.09208460480028274\n",
      "170 0.00361078649286177 0.017171010163311816 0.11461953957463303\n",
      "171 0.0024628902565046983 0.01925273461442011 0.0897149320723227\n",
      "172 0.0021156416803121128 0.018454023754042023 0.0992667521589993\n",
      "173 0.004681166285379542 0.021376569384253388 0.09854930548778615\n",
      "174 0.003325316967828141 0.022586277896025554 0.11053039788971583\n",
      "175 0.003197267235008133 0.01934306725246943 0.08001278208611441\n",
      "176 0.0033482645630988103 0.01863772967261807 0.07054170276594847\n",
      "177 0.004288585899691048 0.022902442784498968 0.08820582594460868\n",
      "178 0.004430997182506416 0.02852435860523493 0.1120705454007988\n",
      "179 0.002500816594410974 0.01537079221284853 0.1080118015119227\n",
      "180 0.0027134999146543617 0.027027617408376753 0.09521186218857755\n",
      "181 0.004945373102634736 0.01925825168652793 0.10944759510605569\n",
      "182 0.003275505940399237 0.018065599077844698 0.07998195382849213\n",
      "183 0.002520933191080367 0.015573387332792312 0.1127836210992387\n",
      "184 0.004321620790243176 0.023177592205461253 0.09874968390305146\n",
      "185 0.0015664617682862448 0.019502258958790566 0.10865305935371966\n",
      "186 0.004285317024493387 0.02714004445106586 0.10143916904734847\n",
      "187 0.0032478009872431825 0.02158516443909884 0.10462669284669618\n",
      "188 0.0034912478567703273 0.01780022042622408 0.09161891849059485\n",
      "189 0.004290759458928837 0.01973307572126306 0.0932623413023961\n",
      "190 0.004220910935447541 0.025680057747664728 0.11695209486798021\n",
      "191 0.002309820963511573 0.021958290603604005 0.0899953611319127\n",
      "192 0.0031862472190055287 0.025600188912661026 0.10359719357423562\n",
      "193 0.0038016752992414238 0.021877147345147886 0.10875106523139749\n",
      "194 0.004874532605097146 0.020636178105041307 0.11883963837841602\n",
      "195 0.003848116601836793 0.03090019460039152 0.10649152646989012\n",
      "196 0.0030907795591214593 0.019573862012436074 0.11608650708681037\n",
      "197 0.003149647007994084 0.016528293178988067 0.08968739191306596\n",
      "198 0.004593266775217774 0.03546956560428325 0.1162532807086241\n",
      "199 0.0038170192203743677 0.019190439028453417 0.10121332713942374\n",
      "200 0.002275925882278191 0.019469597647084058 0.12367583698126107\n",
      "201 0.0046118625246702695 0.026252283253061566 0.09467294757995162\n",
      "202 0.0026645590075141364 0.019788547840394757 0.1010967865078123\n",
      "203 0.002310664283292835 0.02066431389344769 0.08921113159783448\n",
      "204 0.0044649831186667505 0.024998876826382688 0.11280835998862546\n",
      "205 0.0039615230062497436 0.03832811974777739 0.11463749546633542\n",
      "206 0.0041402791911947165 0.028646372877871494 0.12143289849881374\n",
      "207 0.00334190444986701 0.018006915772428696 0.09998269234763446\n",
      "208 0.003321216826273684 0.02009102184984268 0.11435944006259378\n",
      "209 0.004943296720462384 0.02393748845371168 0.09602613714513193\n",
      "210 0.0035279910677373013 0.0278034258227812 0.12544362373967938\n",
      "211 0.0035208014578463334 0.027245433178941604 0.11274196769023648\n",
      "212 0.004786893164993302 0.0447849908254699 0.12281199556711783\n",
      "213 0.0029779206816393598 0.012129212207993303 0.10624420725688354\n",
      "214 0.0037598028697740384 0.02781778987597868 0.12010287172712719\n",
      "215 0.002846674885278709 0.035437739417281976 0.13836969896731957\n",
      "216 0.0038578898037128348 0.02734381896947158 0.1024581709198151\n",
      "217 0.0034061145485702734 0.02115504625125414 0.09998809311885062\n",
      "218 0.0024835438223295286 0.026729961155985554 0.13390407879601338\n",
      "219 0.0038108541160502283 0.025686107387251817 0.11472133098868727\n",
      "220 0.0038440810080018657 0.019708360322302592 0.09895392217448609\n",
      "221 0.0009738667704014949 0.01654627012881054 0.10640683235965775\n",
      "222 0.004379315033919097 0.030780704541981556 0.10627207389423671\n",
      "223 0.0033571113707807493 0.017694704778114752 0.07761982928911652\n",
      "224 0.003238171418886761 0.030014999353173685 0.1259548317234055\n",
      "225 0.0040182516621236255 0.02553740186705459 0.12008233799634718\n",
      "226 0.0034999812705534465 0.02086068276778805 0.12334909677179276\n",
      "227 0.0032773729288016465 0.017982415752936434 0.08277873521613509\n",
      "228 0.0021881801584065935 0.01659381527118229 0.1074607599637005\n",
      "229 0.00439833078888871 0.02640434791654215 0.11329580867178979\n",
      "230 0.0029998309589616967 0.023247567364767144 0.09502743041254137\n",
      "231 0.004387810476721175 0.028151768805008588 0.1100406056566126\n",
      "232 0.00296920579837698 0.012132273188483606 0.07378898969495787\n",
      "233 0.0021010308196936795 0.016193994189977225 0.10757247886877383\n",
      "234 0.004090545645923078 0.02441089623115191 0.07664730343177653\n",
      "235 0.004057236143468694 0.026780172998671914 0.10928412620186051\n",
      "236 0.003530412556841075 0.02707249723892877 0.1274398264630219\n",
      "237 0.004430065869379574 0.029632399821623367 0.1112194297036622\n",
      "238 0.003181730919322401 0.02231446982217996 0.13184994220593665\n",
      "239 0.0020874846044890982 0.013961300266989208 0.06549787907242777\n",
      "240 0.003682361078533176 0.016309564109711513 0.07349260550374623\n",
      "241 0.001868345177021571 0.014548938796539096 0.08724867129597366\n",
      "242 0.003787157264779497 0.020034072062329468 0.11167167413143889\n",
      "243 0.003680479402054968 0.023152945456816267 0.09631207971556355\n",
      "244 0.004197153448022659 0.038081408010970846 0.1387953048045815\n",
      "245 0.0026085903268695363 0.01569153210218044 0.10998327640011821\n",
      "246 0.0022109852210032074 0.01775390956665912 0.09456617890302374\n",
      "247 0.0026013919401044814 0.020059611298560792 0.09462625217493123\n",
      "248 0.0029148682874149123 0.025402642799387774 0.10256951011243054\n",
      "249 0.004596082418241144 0.024454381011349037 0.1430524619896873\n",
      "250 0.0020849436005844317 0.019750124082377543 0.08545959387910901\n",
      "251 0.003477843910307959 0.029849153108588315 0.10820142694631929\n",
      "252 0.003561924711612168 0.017099226046309875 0.09613016236929053\n",
      "253 0.0035453289378727295 0.023699049997690035 0.12274072617518009\n",
      "254 0.004585034719742696 0.027277154403393372 0.0994207192438596\n",
      "255 0.003079761424282873 0.024922748497207547 0.08524788952982967\n",
      "256 0.003366485797148202 0.024144280343961606 0.11072588310758426\n",
      "257 0.003135416913630555 0.028553207599861276 0.09329608279269769\n",
      "258 0.004494462547337905 0.02581416477187362 0.10509251387409\n",
      "259 0.0033539253293524943 0.012754799342664934 0.07691303424205301\n",
      "260 0.0022539008824682648 0.017054557001125722 0.09345545288802995\n",
      "261 0.00251656314217741 0.01501674420081622 0.10745140759019495\n",
      "262 0.0034544919520714916 0.02727066751167101 0.1130272518137353\n",
      "263 0.004216980607533861 0.01757418148010306 0.08271304463510257\n",
      "264 0.003007966290633984 0.016066807565039636 0.09222857944314894\n",
      "265 0.004845471377699563 0.02155509350829659 0.10329143206573264\n",
      "266 0.0024884820113046553 0.017610530260260847 0.13007728955365466\n",
      "267 0.002091528519017726 0.019077999085579094 0.11464407579127359\n",
      "268 0.003463794175957745 0.020076270051039936 0.11950230743571029\n",
      "269 0.0024042590032906453 0.020207895439656533 0.09542130064176199\n",
      "270 0.0049286167203920605 0.02056451101031427 0.12690448473814386\n",
      "271 0.004979873525141233 0.03681906122270492 0.10088684964069353\n",
      "272 0.0033024306218508047 0.016999957805519515 0.1089261638961837\n",
      "273 0.004003771144305091 0.021080772639684588 0.10829117481273301\n",
      "274 0.004039655950302471 0.022992344964078825 0.11654364943341772\n",
      "275 0.004903813933062552 0.027581701635719134 0.13290840841595275\n",
      "276 0.00278359148884835 0.019744897445799332 0.10128128990100498\n",
      "277 0.003691637831044926 0.03180523391189713 0.1144734273391986\n",
      "278 0.003588316712666361 0.020216956033089692 0.11272571035625664\n",
      "279 0.002140740416823189 0.02191113707769079 0.10202936021783252\n",
      "280 0.004976876660784165 0.020968178204226053 0.06709648055331469\n",
      "281 0.00439876084691951 0.021796953774012313 0.09662446900181479\n",
      "282 0.0018879610132940682 0.014798304490255758 0.07806396636244352\n",
      "283 0.004043541954407655 0.028410931889854642 0.1394120361031364\n",
      "284 0.003204198919458466 0.01705630199791345 0.12168406666096307\n",
      "285 0.002558750664607027 0.01375810706397217 0.09811081316973778\n",
      "286 0.0018993771455432587 0.012914105511660516 0.11626471840295458\n",
      "287 0.0036441836832257586 0.022273110040705403 0.094140735102795\n",
      "288 0.0036852556848596236 0.025256291605413744 0.10111169362392305\n",
      "289 0.003973450329640254 0.029985894913908892 0.10919202339480627\n",
      "290 0.004512694448057256 0.03286111667451912 0.11654741664970196\n",
      "291 0.0035865848821257456 0.01853137604475226 0.08498675612554116\n",
      "292 0.002834419969096562 0.02003560183327923 0.10178029982010695\n",
      "293 0.004158186728759228 0.02390357874843823 0.1180559330720654\n",
      "294 0.004771219183207055 0.027178873084574114 0.13152546467307263\n",
      "295 0.0034810822269789998 0.017999699732793858 0.07984582092451412\n",
      "296 0.004997850860981042 0.03619194815708403 0.10648201583182101\n",
      "297 0.004497527421553746 0.023087945087807933 0.1128804951819379\n",
      "298 0.0035416123190480852 0.027171544306923864 0.11368192613963335\n",
      "299 0.0034671507129219645 0.02034511097137066 0.09668639999077855\n",
      "300 0.0020287245326328874 0.014366844986899737 0.0915433971078762\n",
      "301 0.004375229151269075 0.03123702188965085 0.13246504561010486\n",
      "302 0.004003131677586502 0.024810035604791017 0.12498732577704316\n",
      "303 0.003455844949767986 0.019713600731966367 0.09507353458820654\n",
      "304 0.004634661881328633 0.031676467779485025 0.13238764947680393\n",
      "305 0.0031667199958402533 0.021327586396581045 0.09012742277807681\n",
      "306 0.002028849032548439 0.016369107960572938 0.0683508570279329\n",
      "307 0.004999347211728087 0.033140255411717084 0.1436357003664342\n",
      "308 0.0023513666070621017 0.022365827727322235 0.09576265028290376\n",
      "309 0.004357939486711059 0.024430125827267772 0.11441936907527628\n",
      "310 0.0025381056928474367 0.01924378008334448 0.1049541110679303\n",
      "311 0.0024009154390000906 0.013971361950723234 0.10009854398900911\n",
      "312 0.004374105518036818 0.02150502008319008 0.10955659359844727\n",
      "313 0.0046355260509597495 0.024998785653534236 0.12132492066986807\n",
      "314 0.003429559309511681 0.015618148645118983 0.07148901873030042\n",
      "315 0.003428523369005049 0.015535093217495568 0.06927549919376928\n",
      "316 0.0029106125895178615 0.023903451010494597 0.1147118638132368\n",
      "317 0.002273857633235587 0.013807119436788425 0.0877212047374474\n",
      "318 0.004284726477865345 0.03377083498795526 0.12225630745561429\n",
      "319 0.0033105222057740586 0.027504968324010712 0.10855176717383548\n",
      "320 0.003911825137553884 0.025571254954319438 0.13093308663705333\n",
      "321 0.002372944831528765 0.021072978160116086 0.12422431344518942\n",
      "322 0.004669425569923006 0.04894577696434827 0.13135990129504913\n",
      "323 0.004154201755911158 0.017506397773781397 0.08503624870604853\n",
      "324 0.004073177759125709 0.03236896855430203 0.0981362084624664\n",
      "325 0.004016624228361007 0.036465801991805984 0.11676154579567163\n",
      "326 0.0032984627940806496 0.021902887137454412 0.08140185639989239\n",
      "327 0.0021963814573952413 0.02351449336079127 0.10662404061483288\n",
      "328 0.004328600096573701 0.025718598689087782 0.12404605929236051\n",
      "329 0.003399733080276533 0.021320834426396856 0.1050079049419437\n",
      "330 0.0033259706694812197 0.025151049058297677 0.11990135887943743\n",
      "331 0.004278475335795734 0.01759652528318522 0.055482626761304915\n",
      "332 0.0035829269046708727 0.018058933348382433 0.0942073085627522\n",
      "333 0.003947755620423596 0.02893898753772526 0.11702801978224653\n",
      "334 0.004670839847494571 0.026574103330446797 0.10623527524751736\n",
      "335 0.003193649258680321 0.02157579125649718 0.07282936070676962\n",
      "336 0.004244591724423503 0.022845968704135237 0.08924078695376134\n",
      "337 0.003550603708385088 0.019981384370398054 0.09308216116097355\n",
      "338 0.002475783873643592 0.017924858635106507 0.10701007560302356\n",
      "339 0.0015006893617555608 0.012965590254537036 0.09815231091976596\n",
      "340 0.003952663025885513 0.02707344243714717 0.10339901561539537\n",
      "341 0.002530644215058891 0.022417556187087292 0.09594779338813501\n",
      "342 0.004373742953923585 0.03536566117924369 0.10568851179763789\n",
      "343 0.0029755608937623978 0.019633955154892066 0.09577675463089452\n",
      "344 0.004838219612750077 0.02184666011310611 0.0957896992397349\n",
      "345 0.003042084991742565 0.021343818417913038 0.09659245342178277\n",
      "346 0.0035887180986430046 0.023230814675973314 0.10829266534861523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, seq_len, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m inputTensor(np\u001b[39m.\u001b[39marray([[size]]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39;49m, hn)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     out_\u001b[39m=\u001b[39mout\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m out \u001b[39m=\u001b[39m o(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m out_\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
