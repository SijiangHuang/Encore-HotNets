{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 8192*2\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            # self.ots.append(\n",
    "            #     nn.Sequential(\n",
    "            #         nn.LayerNorm(hidden_size))\n",
    "            # )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%2==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%2==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%2\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 6, 48).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 6).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34225694, 2645504, 36871198)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5157940"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.967997777777775"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "584003 584002\n"
     ]
    }
   ],
   "source": [
    "gru=torch.load('model/gru-09202.pth')\n",
    "s2h=torch.load('model/s2h-09202.pth')\n",
    "save_dict = torch.load(\"model/save_dict-09202.pth\")\n",
    "optimizer.load_state_dict(save_dict['optimizer'])\n",
    "scheduler._step_count = save_dict[\"_step_count\"]\n",
    "scheduler.last_epoch = save_dict[\"last_epoch\"]\n",
    "print(save_dict[\"_step_count\"],save_dict[\"last_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：34225694\n",
      "总参数数量和：2645504\n",
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 2.2872114181518555 2.2932925255298615 585004 [0.0001, 0.0001] 28.783653736114502\n",
      "2000 2.241100549697876 2.290548651218414 586004 [0.0001, 0.0001] 59.67343282699585\n",
      "3000 2.2956578731536865 2.2915248136520385 587004 [0.0001, 0.0001] 89.66933536529541\n",
      "4000 2.329803466796875 2.2909572265148164 588004 [0.0001, 0.0001] 120.57687640190125\n",
      "5000 2.2635064125061035 2.2908501532077787 589004 [0.0001, 0.0001] 151.69206428527832\n",
      "6000 2.2731082439422607 2.290872532606125 590004 [0.0001, 0.0001] 182.61566925048828\n",
      "7000 2.281688690185547 2.2932064027786256 591004 [0.0001, 0.0001] 213.1979615688324\n",
      "8000 2.2952065467834473 2.2909260592460634 592004 [0.0001, 0.0001] 242.72117471694946\n",
      "9000 2.3253519535064697 2.2916927652359007 593004 [0.0001, 0.0001] 273.84122037887573\n",
      "10000 2.2552120685577393 2.290374804496765 594004 [0.0001, 0.0001] 303.93332076072693\n",
      "11000 2.263457775115967 2.2898257277011873 595004 [0.0001, 0.0001] 334.86946535110474\n",
      "12000 2.3284270763397217 2.292487079143524 596004 [0.0001, 0.0001] 365.7641956806183\n",
      "13000 2.2686893939971924 2.290745753288269 597004 [0.0001, 0.0001] 396.66605615615845\n",
      "14000 2.2947423458099365 2.2907689979076387 598004 [0.0001, 0.0001] 426.3145229816437\n",
      "15000 2.279240369796753 2.2908649311065674 599004 [0.0001, 0.0001] 456.8022549152374\n",
      "16000 2.3083574771881104 2.2914686393737793 600004 [0.0001, 0.0001] 486.8221924304962\n",
      "17000 2.3153164386749268 2.2897822580337523 601004 [0.0001, 0.0001] 517.3889083862305\n",
      "18000 2.2929468154907227 2.290519291639328 602004 [0.0001, 0.0001] 546.7505490779877\n",
      "19000 2.3241147994995117 2.2889163496494294 603004 [0.0001, 0.0001] 577.1169147491455\n",
      "20000 2.2668395042419434 2.2901585443019865 604004 [0.0001, 0.0001] 607.433023929596\n",
      "21000 2.2902424335479736 2.289548197746277 605004 [0.0001, 0.0001] 636.4542648792267\n",
      "22000 2.3313825130462646 2.2910813422203065 606004 [0.0001, 0.0001] 665.9677357673645\n",
      "23000 2.275540828704834 2.2919960281848906 607004 [0.0001, 0.0001] 696.2180433273315\n",
      "24000 2.2591116428375244 2.289665066242218 608004 [0.0001, 0.0001] 727.1054515838623\n",
      "25000 2.3164281845092773 2.291701790332794 609004 [0.0001, 0.0001] 756.6635918617249\n",
      "26000 2.3397929668426514 2.2921944472789764 610004 [0.0001, 0.0001] 785.9749119281769\n",
      "27000 2.332677125930786 2.2909859132766726 611004 [0.0001, 0.0001] 815.9457890987396\n",
      "28000 2.2497990131378174 2.290195086479187 612004 [0.0001, 0.0001] 846.1519119739532\n",
      "29000 2.323208808898926 2.2914611120223998 613004 [0.0001, 0.0001] 876.2977867126465\n",
      "30000 2.32292103767395 2.290519055366516 614004 [0.0001, 0.0001] 905.7969517707825\n",
      "31000 2.3145716190338135 2.290824711561203 615004 [0.0001, 0.0001] 936.5934808254242\n",
      "32000 2.2868456840515137 2.2896141884326933 616004 [0.0001, 0.0001] 967.1906261444092\n",
      "33000 2.329115629196167 2.2896930360794068 617004 [0.0001, 0.0001] 997.6529657840729\n",
      "34000 2.227120876312256 2.289707380771637 618004 [0.0001, 0.0001] 1028.3014359474182\n",
      "35000 2.3199262619018555 2.290758792877197 619004 [0.0001, 0.0001] 1058.2552440166473\n",
      "36000 2.330805778503418 2.2894875447750094 620004 [0.0001, 0.0001] 1087.8494143486023\n",
      "37000 2.2698588371276855 2.2901942491531373 621004 [0.0001, 0.0001] 1118.5387501716614\n",
      "38000 2.270050048828125 2.289157639980316 622004 [0.0001, 0.0001] 1147.9571890830994\n",
      "39000 2.280578374862671 2.291983018159866 623004 [0.0001, 0.0001] 1177.7732963562012\n",
      "40000 2.3023977279663086 2.2911929779052733 624004 [0.0001, 0.0001] 1208.3529930114746\n",
      "41000 2.2603936195373535 2.2903557751178742 625004 [0.0001, 0.0001] 1238.7749457359314\n",
      "42000 2.273322582244873 2.29017095541954 626004 [0.0001, 0.0001] 1269.2811794281006\n",
      "43000 2.2973361015319824 2.2894860994815827 627004 [0.0001, 0.0001] 1298.0161066055298\n",
      "44000 2.3103182315826416 2.290940949201584 628004 [0.0001, 0.0001] 1327.67284989357\n",
      "45000 2.3100876808166504 2.2910459065437316 629004 [0.0001, 0.0001] 1358.1855416297913\n",
      "46000 2.2857649326324463 2.2933661148548126 630004 [0.0001, 0.0001] 1388.5950946807861\n",
      "47000 2.2942280769348145 2.2908199908733367 631004 [0.0001, 0.0001] 1418.9698958396912\n",
      "48000 2.3100643157958984 2.291877225637436 632004 [0.0001, 0.0001] 1449.4775993824005\n",
      "49000 2.318897008895874 2.2927926931381224 633004 [0.0001, 0.0001] 1480.1989359855652\n",
      "50000 2.3031399250030518 2.292238831996918 634004 [0.0001, 0.0001] 1511.0637636184692\n",
      "51000 2.3353829383850098 2.292026351213455 635004 [0.0001, 0.0001] 1542.0022869110107\n",
      "52000 2.316624879837036 2.292732047796249 636004 [0.0001, 0.0001] 1572.965455532074\n",
      "53000 2.29072904586792 2.2917667284011842 637004 [0.0001, 0.0001] 1602.1473898887634\n",
      "54000 2.2418360710144043 2.2898004314899443 638004 [0.0001, 0.0001] 1631.369915485382\n",
      "55000 2.306483268737793 2.290374372720718 639004 [0.0001, 0.0001] 1660.8769209384918\n",
      "56000 2.302903890609741 2.291287971496582 640004 [0.0001, 0.0001] 1691.5813937187195\n",
      "57000 2.290318489074707 2.2904389894008634 641004 [0.0001, 0.0001] 1722.488359451294\n",
      "58000 2.305619716644287 2.2901775534152984 642004 [0.0001, 0.0001] 1752.6890354156494\n",
      "59000 2.240483283996582 2.290528804779053 643004 [0.0001, 0.0001] 1783.4358263015747\n",
      "60000 2.3223373889923096 2.291122765541077 644004 [0.0001, 0.0001] 1813.7533116340637\n",
      "61000 2.311577320098877 2.2902102196216583 645004 [0.0001, 0.0001] 1844.9471826553345\n",
      "62000 2.2630419731140137 2.290743888139725 646004 [0.0001, 0.0001] 1876.020215511322\n",
      "63000 2.2919459342956543 2.2902050375938416 647004 [0.0001, 0.0001] 1907.1411652565002\n",
      "64000 2.2828354835510254 2.2923498673439027 648004 [0.0001, 0.0001] 1937.0716013908386\n",
      "65000 2.303372383117676 2.290076421737671 649004 [0.0001, 0.0001] 1966.462174654007\n",
      "66000 2.280153512954712 2.2908551123142242 650004 [0.0001, 0.0001] 1995.73286318779\n",
      "67000 2.26872181892395 2.2906325862407684 651004 [0.0001, 0.0001] 2025.1258642673492\n",
      "68000 2.3065950870513916 2.291729732275009 652004 [0.0001, 0.0001] 2055.8093123435974\n",
      "69000 2.27710223197937 2.2916444540023804 653004 [0.0001, 0.0001] 2086.1660599708557\n",
      "70000 2.2880640029907227 2.2908453876972197 654004 [0.0001, 0.0001] 2117.077059984207\n",
      "71000 2.2875187397003174 2.2905805213451385 655004 [0.0001, 0.0001] 2147.981400489807\n",
      "72000 2.3063864707946777 2.292720376729965 656004 [0.0001, 0.0001] 2179.095629930496\n",
      "73000 2.2745025157928467 2.2912904496192934 657004 [0.0001, 0.0001] 2209.717032432556\n",
      "74000 2.270803451538086 2.291591496229172 658004 [0.0001, 0.0001] 2240.3409311771393\n",
      "75000 2.332157850265503 2.2909055485725403 659004 [0.0001, 0.0001] 2270.8228085041046\n",
      "76000 2.2163498401641846 2.290668621778488 660004 [0.0001, 0.0001] 2300.3073494434357\n",
      "77000 2.2725021839141846 2.290997968196869 661004 [0.0001, 0.0001] 2330.647223711014\n",
      "78000 2.262148857116699 2.291016142845154 662004 [0.0001, 0.0001] 2361.333718061447\n",
      "79000 2.2678587436676025 2.290741465806961 663004 [0.0001, 0.0001] 2392.139340877533\n",
      "80000 2.3330085277557373 2.2915562002658842 664004 [0.0001, 0.0001] 2422.552951335907\n",
      "81000 2.3032896518707275 2.2901966280937196 665004 [0.0001, 0.0001] 2452.9653210639954\n",
      "82000 2.282989025115967 2.29071183514595 666004 [0.0001, 0.0001] 2483.531863927841\n",
      "83000 2.2860140800476074 2.2912050013542173 667004 [0.0001, 0.0001] 2514.04909491539\n",
      "84000 2.3079259395599365 2.290838654279709 668004 [0.0001, 0.0001] 2544.4690239429474\n",
      "85000 2.2907490730285645 2.2907756581306455 669004 [0.0001, 0.0001] 2575.250059366226\n",
      "86000 2.3092427253723145 2.2914204547405244 670004 [0.0001, 0.0001] 2605.0925154685974\n",
      "87000 2.298887252807617 2.289907907485962 671004 [0.0001, 0.0001] 2635.23472738266\n",
      "88000 2.3230597972869873 2.290991066932678 672004 [0.0001, 0.0001] 2664.7107877731323\n",
      "89000 2.299855947494507 2.2924524059295655 673004 [0.0001, 0.0001] 2695.1406393051147\n",
      "90000 2.3366944789886475 2.2934039804935455 674004 [0.0001, 0.0001] 2726.1082515716553\n",
      "91000 2.33431339263916 2.29024587726593 675004 [0.0001, 0.0001] 2757.0681388378143\n",
      "92000 2.2513649463653564 2.2910369141101836 676004 [0.0001, 0.0001] 2787.7166435718536\n",
      "93000 2.2493863105773926 2.289237103462219 677004 [0.0001, 0.0001] 2818.2006294727325\n",
      "94000 2.37397837638855 2.2916663143634795 678004 [0.0001, 0.0001] 2849.016786336899\n",
      "95000 2.2735531330108643 2.2921497976779937 679004 [0.0001, 0.0001] 2879.8014907836914\n",
      "96000 2.300593137741089 2.292085617542267 680004 [0.0001, 0.0001] 2911.012024641037\n",
      "97000 2.3009397983551025 2.2911267731189726 681004 [0.0001, 0.0001] 2941.612612247467\n",
      "98000 2.212362766265869 2.291060533761978 682004 [0.0001, 0.0001] 2972.0133426189423\n",
      "99000 2.217808246612549 2.2899610221385958 683004 [0.0001, 0.0001] 3001.4749925136566\n",
      "100000 2.2066774368286133 2.291662845134735 684004 [0.0001, 0.0001] 3032.2721948623657\n",
      "101000 2.282336711883545 2.2914082345962523 685004 [0.0001, 0.0001] 3061.011255264282\n",
      "102000 2.320537567138672 2.2894557058811187 686004 [0.0001, 0.0001] 3089.815407514572\n",
      "103000 2.2581658363342285 2.2913099830150605 687004 [0.0001, 0.0001] 3120.0856478214264\n",
      "104000 2.330343246459961 2.2916056575775148 688004 [0.0001, 0.0001] 3150.788881778717\n",
      "105000 2.2503700256347656 2.2916277108192444 689004 [0.0001, 0.0001] 3181.1869060993195\n",
      "106000 2.240957021713257 2.2907678916454315 690004 [0.0001, 0.0001] 3211.4610579013824\n",
      "107000 2.314091444015503 2.2909614140987395 691004 [0.0001, 0.0001] 3242.3604865074158\n",
      "108000 2.2627594470977783 2.2927463855743406 692004 [0.0001, 0.0001] 3273.2458231449127\n",
      "109000 2.2398948669433594 2.2917135899066925 693004 [0.0001, 0.0001] 3304.108025789261\n",
      "110000 2.2961783409118652 2.292201200723648 694004 [0.0001, 0.0001] 3334.936787366867\n",
      "111000 2.2970173358917236 2.291830704450607 695004 [0.0001, 0.0001] 3365.761693239212\n",
      "112000 2.307443618774414 2.2899765558242797 696004 [0.0001, 0.0001] 3396.701378107071\n",
      "113000 2.2807846069335938 2.290912709951401 697004 [0.0001, 0.0001] 3425.8932914733887\n",
      "114000 2.341494560241699 2.291265023231506 698004 [0.0001, 0.0001] 3455.576375722885\n",
      "115000 2.2608554363250732 2.2921271302700044 699004 [0.0001, 0.0001] 3485.4883885383606\n",
      "116000 2.3381338119506836 2.2895994951725007 700004 [0.0001, 0.0001] 3516.3200788497925\n",
      "117000 2.3314640522003174 2.291715441465378 701004 [0.0001, 0.0001] 3547.4213886260986\n",
      "118000 2.343644142150879 2.2903092091083526 702004 [0.0001, 0.0001] 3578.3125846385956\n",
      "119000 2.3229238986968994 2.2922990708351136 703004 [0.0001, 0.0001] 3608.9976677894592\n",
      "120000 2.2844748497009277 2.290946264743805 704004 [0.0001, 0.0001] 3639.989613056183\n",
      "121000 2.353053569793701 2.290760754108429 705004 [0.0001, 0.0001] 3671.079171180725\n",
      "122000 2.2693889141082764 2.2894835221767424 706004 [0.0001, 0.0001] 3701.987648963928\n",
      "123000 2.2740964889526367 2.2917816722393036 707004 [0.0001, 0.0001] 3732.1300389766693\n",
      "124000 2.273102283477783 2.290069976806641 708004 [0.0001, 0.0001] 3761.759546995163\n",
      "125000 2.28177547454834 2.2926055059432984 709004 [0.0001, 0.0001] 3791.933253765106\n",
      "126000 2.2876698970794678 2.2900146617889403 710004 [0.0001, 0.0001] 3822.569185733795\n",
      "127000 2.292017936706543 2.2903727967739105 711004 [0.0001, 0.0001] 3853.36452794075\n",
      "128000 2.286074638366699 2.290976301193237 712004 [0.0001, 0.0001] 3884.5092527866364\n",
      "129000 2.2868518829345703 2.2916214385032654 713004 [0.0001, 0.0001] 3915.6575310230255\n",
      "130000 2.268909454345703 2.291856587648392 714004 [0.0001, 0.0001] 3946.6711661815643\n",
      "131000 2.3342785835266113 2.29093994307518 715004 [0.0001, 0.0001] 3977.653425216675\n",
      "132000 2.22867751121521 2.2912747881412505 716004 [0.0001, 0.0001] 4008.0732657909393\n",
      "133000 2.3113176822662354 2.290540607213974 717004 [0.0001, 0.0001] 4038.986572742462\n",
      "134000 2.285118341445923 2.2901264774799346 718004 [0.0001, 0.0001] 4069.425402402878\n",
      "135000 2.2743735313415527 2.2920142464637756 719004 [0.0001, 0.0001] 4100.2793436050415\n",
      "136000 2.3169925212860107 2.2920297844409943 720004 [0.0001, 0.0001] 4129.6432003974915\n",
      "137000 2.278095245361328 2.2902359476089478 721004 [0.0001, 0.0001] 4160.561929941177\n",
      "138000 2.310382127761841 2.2915306913852693 722004 [0.0001, 0.0001] 4191.757882118225\n",
      "139000 2.248784303665161 2.2916250138282774 723004 [0.0001, 0.0001] 4222.147466421127\n",
      "140000 2.3003478050231934 2.2897100214958193 724004 [0.0001, 0.0001] 4253.09264421463\n",
      "141000 2.2720677852630615 2.290044531583786 725004 [0.0001, 0.0001] 4283.773551940918\n",
      "142000 2.293715000152588 2.2904746489524843 726004 [0.0001, 0.0001] 4314.925632238388\n",
      "143000 2.2894067764282227 2.2924709882736205 727004 [0.0001, 0.0001] 4344.940087556839\n",
      "144000 2.301769971847534 2.2908045468330385 728004 [0.0001, 0.0001] 4375.733023881912\n",
      "145000 2.2903757095336914 2.2917847719192506 729004 [0.0001, 0.0001] 4405.517512559891\n",
      "146000 2.3167471885681152 2.290414328813553 730004 [0.0001, 0.0001] 4435.998912096024\n",
      "147000 2.2975826263427734 2.291464503765106 731004 [0.0001, 0.0001] 4466.961256027222\n",
      "148000 2.29557728767395 2.293560744047165 732004 [0.0001, 0.0001] 4497.508499145508\n",
      "149000 2.273423910140991 2.2897161343097685 733004 [0.0001, 0.0001] 4527.684170722961\n",
      "150000 2.255497455596924 2.2923610899448397 734004 [0.0001, 0.0001] 4558.360868692398\n",
      "151000 2.305807113647461 2.2905426120758055 735004 [0.0001, 0.0001] 4588.865224838257\n",
      "152000 2.3109865188598633 2.2898723435401918 736004 [0.0001, 0.0001] 4619.072956800461\n",
      "153000 2.2789769172668457 2.291559329986572 737004 [0.0001, 0.0001] 4649.58106470108\n",
      "154000 2.319950819015503 2.290816477060318 738004 [0.0001, 0.0001] 4679.381560325623\n",
      "155000 2.3268401622772217 2.2903486201763155 739004 [0.0001, 0.0001] 4710.296943902969\n",
      "156000 2.2699859142303467 2.2925298087596895 740004 [0.0001, 0.0001] 4740.6052005290985\n",
      "157000 2.2886781692504883 2.2918248682022093 741004 [0.0001, 0.0001] 4770.76079916954\n",
      "158000 2.2582755088806152 2.292679636478424 742004 [0.0001, 0.0001] 4800.0830950737\n",
      "159000 2.300157308578491 2.289975328683853 743004 [0.0001, 0.0001] 4830.393155574799\n",
      "160000 2.3351705074310303 2.2932152671813966 744004 [0.0001, 0.0001] 4859.791933059692\n",
      "161000 2.326589345932007 2.292459505081177 745004 [0.0001, 0.0001] 4890.272910118103\n",
      "162000 2.3143115043640137 2.29184068441391 746004 [0.0001, 0.0001] 4920.616621255875\n",
      "163000 2.302216053009033 2.2893998024463653 747004 [0.0001, 0.0001] 4950.956968784332\n",
      "164000 2.256328821182251 2.2915308010578155 748004 [0.0001, 0.0001] 4980.171636581421\n",
      "165000 2.33449125289917 2.2905117719173433 749004 [0.0001, 0.0001] 5009.6966235637665\n",
      "166000 2.33720064163208 2.292060273647308 750004 [0.0001, 0.0001] 5039.366459131241\n",
      "167000 2.2744767665863037 2.2900256497859957 751004 [0.0001, 0.0001] 5070.009050369263\n",
      "168000 2.2986955642700195 2.2909357542991637 752004 [0.0001, 0.0001] 5100.516612768173\n",
      "169000 2.315305709838867 2.291239921331406 753004 [0.0001, 0.0001] 5130.524698257446\n",
      "170000 2.2995526790618896 2.2903422672748563 754004 [0.0001, 0.0001] 5160.363061904907\n",
      "171000 2.3486545085906982 2.292674209356308 755004 [0.0001, 0.0001] 5190.581093549728\n",
      "172000 2.2800328731536865 2.29116925573349 756004 [0.0001, 0.0001] 5221.002896785736\n",
      "173000 2.253681182861328 2.2911248350143434 757004 [0.0001, 0.0001] 5251.670972824097\n",
      "174000 2.2419164180755615 2.2915785517692564 758004 [0.0001, 0.0001] 5281.720021247864\n",
      "175000 2.2915139198303223 2.291254410743713 759004 [0.0001, 0.0001] 5312.173112869263\n",
      "176000 2.301753520965576 2.2915840404033663 760004 [0.0001, 0.0001] 5342.4004991054535\n",
      "177000 2.309866428375244 2.2903071970939637 761004 [0.0001, 0.0001] 5371.116926908493\n",
      "178000 2.268869638442993 2.290624870300293 762004 [0.0001, 0.0001] 5400.269658327103\n",
      "179000 2.319772481918335 2.2921955966949463 763004 [0.0001, 0.0001] 5430.24111700058\n",
      "180000 2.30440092086792 2.2917195391654968 764004 [0.0001, 0.0001] 5459.439775943756\n",
      "181000 2.2782013416290283 2.2904604868888856 765004 [0.0001, 0.0001] 5490.109667301178\n",
      "182000 2.2724483013153076 2.290781804561615 766004 [0.0001, 0.0001] 5519.967266798019\n",
      "183000 2.247969150543213 2.290636307001114 767004 [0.0001, 0.0001] 5549.307080745697\n",
      "184000 2.294750928878784 2.2919404680728914 768004 [0.0001, 0.0001] 5580.400921821594\n",
      "185000 2.273862838745117 2.2892182676792143 769004 [0.0001, 0.0001] 5611.185215473175\n",
      "186000 2.304551601409912 2.2910615682601927 770004 [0.0001, 0.0001] 5640.558531522751\n",
      "187000 2.2640113830566406 2.290289138317108 771004 [0.0001, 0.0001] 5670.689670324326\n",
      "188000 2.32423996925354 2.2890146594047547 772004 [0.0001, 0.0001] 5700.330240249634\n",
      "189000 2.280287981033325 2.2905127575397493 773004 [0.0001, 0.0001] 5731.33882689476\n",
      "190000 2.266204595565796 2.2916753675937653 774004 [0.0001, 0.0001] 5762.276785373688\n",
      "191000 2.324244976043701 2.290498254776001 775004 [0.0001, 0.0001] 5792.422897815704\n",
      "192000 2.2727253437042236 2.2889181230068205 776004 [0.0001, 0.0001] 5822.617087364197\n",
      "193000 2.276376962661743 2.2909795744419097 777004 [0.0001, 0.0001] 5853.06923532486\n",
      "194000 2.3067657947540283 2.291171847343445 778004 [0.0001, 0.0001] 5883.602409124374\n",
      "195000 2.2337090969085693 2.290181082725525 779004 [0.0001, 0.0001] 5914.172971248627\n",
      "196000 2.3366403579711914 2.289536690950394 780004 [0.0001, 0.0001] 5944.697450876236\n",
      "197000 2.2834203243255615 2.292313911676407 781004 [0.0001, 0.0001] 5975.24825501442\n",
      "198000 2.3029696941375732 2.2906432731151583 782004 [0.0001, 0.0001] 6004.0862674713135\n",
      "199000 2.345289707183838 2.2914321780204774 783004 [0.0001, 0.0001] 6034.7365798950195\n",
      "200000 2.2857038974761963 2.2905822651386263 784004 [0.0001, 0.0001] 6065.285166501999\n",
      "201000 2.300926685333252 2.2912056601047515 785004 [0.0001, 0.0001] 6095.881325721741\n",
      "202000 2.2804582118988037 2.2922275245189665 786004 [0.0001, 0.0001] 6126.274102687836\n",
      "203000 2.243042230606079 2.2895620594024657 787004 [0.0001, 0.0001] 6156.676303386688\n",
      "204000 2.2923803329467773 2.29118759226799 788004 [0.0001, 0.0001] 6187.723296403885\n",
      "205000 2.281369686126709 2.2905996594429014 789004 [0.0001, 0.0001] 6218.353080034256\n",
      "206000 2.278667688369751 2.2898545763492586 790004 [0.0001, 0.0001] 6249.233639717102\n",
      "207000 2.216212749481201 2.2894174404144287 791004 [0.0001, 0.0001] 6280.229432582855\n",
      "208000 2.2856152057647705 2.2913258254528044 792004 [0.0001, 0.0001] 6309.818558692932\n",
      "209000 2.25650691986084 2.29005232834816 793004 [0.0001, 0.0001] 6339.57076716423\n",
      "210000 2.279857635498047 2.2910242047309874 794004 [0.0001, 0.0001] 6370.376518726349\n",
      "211000 2.2438552379608154 2.2919628665447234 795004 [0.0001, 0.0001] 6400.833595275879\n",
      "212000 2.2680258750915527 2.291082615852356 796004 [0.0001, 0.0001] 6431.433669805527\n",
      "213000 2.268979549407959 2.290082323074341 797004 [0.0001, 0.0001] 6461.696675777435\n",
      "214000 2.255274772644043 2.2920333416461944 798004 [0.0001, 0.0001] 6492.192960262299\n",
      "215000 2.29291033744812 2.2918442280292513 799004 [0.0001, 0.0001] 6522.721626758575\n",
      "216000 2.3289921283721924 2.2910855526924134 800004 [1e-05, 1e-05] 6552.824520111084\n",
      "217000 2.3096375465393066 2.290849157333374 801004 [1e-05, 1e-05] 6583.429101467133\n",
      "218000 2.3011677265167236 2.2905175473690034 802004 [1e-05, 1e-05] 6612.929501533508\n"
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-09202.pth')\n",
    "        torch.save(s2h, 'model/s2h-09202.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-09202.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(500):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
