{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import datetime\n",
    "import time\n",
    "from sklearn import metrics\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, Tensor\n",
    "from typing import List\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from utils.dataset import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "pairs = 4096*2\n",
    "pairdata, freqpairs, n_size, n_interval = get_fb_data(pairs)\n",
    "sizedata = get_data(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_set = defaultdict(list)\n",
    "target_set = defaultdict(list)\n",
    "size_set = {}\n",
    "seq_len = 16\n",
    "\n",
    "for pair in range(pairs):\n",
    "    size_index = pairdata[freqpairs[pair]].size_index.values\n",
    "    target_index = np.concatenate((size_index[1:], size_index[0:1]))\n",
    "    for i in range(len(size_index) - seq_len):\n",
    "        seq_set[pair].append(size_index[i:i+seq_len])\n",
    "        target_set[pair].append(target_index[i:i+seq_len])\n",
    "        size_set[pair] = sizedata[pair]\n",
    "    seq_set[pair] = np.array(seq_set[pair])\n",
    "    target_set[pair] = np.array(target_set[pair])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dataset(seed, batch=32):\n",
    "    np.random.seed(seed)\n",
    "    dataset = []\n",
    "    ps = [np.random.randint(pairs) for i in range(batch)]\n",
    "    for pair in ps:\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size_set[pair], target_set[pair][ran_index]])\n",
    "    return dataset\n",
    "\n",
    "def inputTensor(lines):\n",
    "    tensor = torch.zeros(lines.shape[1], lines.shape[0], n_size, dtype=torch.long)\n",
    "    for line in range(lines.shape[0]):\n",
    "        for i in range(lines.shape[1]):\n",
    "            size = lines[line][i]\n",
    "            tensor[i][line][size] = 1\n",
    "    return tensor\n",
    "\n",
    "dataset = sample_dataset(0)\n",
    "dataloader = DataLoader(dataset[:32], batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeToHidden(nn.Module):\n",
    "    def __init__(self, input_size,  n_deep, hidden_size, n_layer):\n",
    "        super(SizeToHidden, self).__init__()\n",
    "        self.lins = nn.ModuleList()\n",
    "        self.n_layer = n_layer\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_net = nn.Sequential(\n",
    "                    nn.Linear(input_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "        for _ in range(n_deep):\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.lins.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, out_features=hidden_size * n_layer)\n",
    "\n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        x = self.in_net(x)\n",
    "        i = 0\n",
    "        for lin in self.lins:\n",
    "            if i % 3 == 0:\n",
    "                x_ = x\n",
    "            x = lin(x)\n",
    "            if i % 3 == 1:\n",
    "                x = x + x_\n",
    "            i = (i+1)%3\n",
    "        x = self.output(x)\n",
    "        x = x.view(-1, self.n_layer, self.hidden_size)\n",
    "        x = x.permute(1, 0, 2).contiguous()\n",
    "        # x = self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layer, n_deep):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layer)\n",
    "        \n",
    "        self.o2o = nn.Linear(hidden_size+n_size, hidden_size)\n",
    "        self.ots = nn.ModuleList()\n",
    "        for _ in range(n_deep):\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(hidden_size, out_features=hidden_size),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            self.ots.append(\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(hidden_size))\n",
    "            )\n",
    "        self.h2o = nn.Linear(hidden_size, n_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.norm_1 = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        out = self.norm(out)\n",
    "        i=0\n",
    "        for o in self.ots:\n",
    "            if i%3==0:\n",
    "                out_=out\n",
    "            out = o(out)\n",
    "            if i%3==1:\n",
    "                out = out + out_\n",
    "            i = (i+1)%3\n",
    "        out = self.norm_1(out)\n",
    "        out = self.h2o(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        out = self.softmax(out)\n",
    "        # print(out.shape)torch.Size([8, 9000, 30])\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 512\n",
    "gru = GRU(n_size, hidden_size, 2, 24).to(device)\n",
    "s2h = SizeToHidden(n_size, 2, hidden_size, 2).to(device)\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[8e5,16e5,24e5,32e5],gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_size = 128\n",
    "# gru = GRU(n_size, hidden_size, 1).to(device)\n",
    "# s2h = SizeToHidden(n_size, [64, 128], hidden_size, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15339038, 1594880, 16933918)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total parameters and trainable parameters of gru and s2h\n",
    "def get_params(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params, trainable_params\n",
    "get_params(gru)[0], get_params(s2h)[0], get_params(gru)[0] + get_params(s2h)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2509618"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_flow = 0\n",
    "for i in range(pairs):\n",
    "    sum_flow += len(pairdata[freqpairs[i]])\n",
    "sum_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.815464444444444"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_params(gru)[0] + get_params(s2h)[0]) / 900000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date = 'final'\n",
    "# gru = torch.load('model/{date}/gru.pth'.format(date=date))\n",
    "# s2h = torch.load('model/{date}/s2h.pth'.format(date=date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "811003 811002\n"
     ]
    }
   ],
   "source": [
    "gru=torch.load('model/gru-0914.pth')\n",
    "s2h=torch.load('model/s2h-0914.pth')\n",
    "save_dict = torch.load(\"model/save_dict-0914.pth\")\n",
    "optimizer.load_state_dict(save_dict['optimizer'])\n",
    "scheduler._step_count = save_dict[\"_step_count\"]\n",
    "scheduler.last_epoch = save_dict[\"last_epoch\"]\n",
    "print(save_dict[\"_step_count\"],save_dict[\"last_epoch\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, optimizer):\n",
    "    gru.train()\n",
    "    s2h.train()\n",
    "    sum_loss = 0\n",
    "    for seq_tensor, size_tensor, target_tensor in dataloader:\n",
    "        seq_tensor = inputTensor(seq_tensor).float().to(device)\n",
    "        size_tensor = size_tensor.float().to(device)\n",
    "        target_tensor = target_tensor.T.long().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, hn = gru(seq_tensor, s2h(size_tensor))\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            loss += nn.NLLLoss()(output[i], target_tensor[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item() / seq_tensor.shape[0] * seq_tensor.shape[1]\n",
    "    return sum_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总参数数量和：15339038\n",
      "总参数数量和：1594880\n",
      "1e-05\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = gru\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "model = s2h\n",
    "params = list(model.parameters())\n",
    "k = 0\n",
    "for i in params:\n",
    "    l = 1\n",
    "    # print(\"该层的结构：\" + str(list(i.size())))\n",
    "    for j in i.size():\n",
    "        l *= j\n",
    "    # print(\"该层参数和：\" + str(l))\n",
    "    k = k + l\n",
    "print(\"总参数数量和：\" + str(k))\n",
    "print(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.5783600211143494 0.5965690571367741 812004 [1e-05, 1e-05] 29.688807249069214\n",
      "2000 0.589902937412262 0.5965112702846527 813004 [1e-05, 1e-05] 52.887463092803955\n",
      "3000 0.6609735488891602 0.5953420588374138 814004 [1e-05, 1e-05] 75.74798631668091\n",
      "4000 0.5708510279655457 0.5963286204338074 815004 [1e-05, 1e-05] 98.89389610290527\n",
      "5000 0.5507369041442871 0.5951172886490822 816004 [1e-05, 1e-05] 121.62399768829346\n",
      "6000 0.6801292300224304 0.5960663321614266 817004 [1e-05, 1e-05] 144.74698781967163\n",
      "7000 0.602750837802887 0.5944200973510743 818004 [1e-05, 1e-05] 167.1187551021576\n",
      "8000 0.5433800220489502 0.59459560328722 819004 [1e-05, 1e-05] 189.3117220401764\n",
      "9000 0.6033045053482056 0.5960682321190834 820004 [1e-05, 1e-05] 212.51857948303223\n",
      "10000 0.5750752687454224 0.5966349368095398 821004 [1e-05, 1e-05] 234.8323187828064\n",
      "11000 0.6020897626876831 0.5955727699398995 822004 [1e-05, 1e-05] 257.58552622795105\n",
      "12000 0.6074000597000122 0.5949631782174111 823004 [1e-05, 1e-05] 280.4334034919739\n",
      "13000 0.6408208012580872 0.5950076180696487 824004 [1e-05, 1e-05] 303.3269820213318\n",
      "14000 0.6877577900886536 0.5972267029285431 825004 [1e-05, 1e-05] 326.2089548110962\n",
      "15000 0.5771198272705078 0.5961156769394874 826004 [1e-05, 1e-05] 349.1370151042938\n",
      "16000 0.6160417199134827 0.5954210901260376 827004 [1e-05, 1e-05] 371.6629316806793\n",
      "17000 0.5986939668655396 0.5950003384947776 828004 [1e-05, 1e-05] 394.5344948768616\n",
      "18000 0.6511167287826538 0.5939967170059681 829004 [1e-05, 1e-05] 417.34917664527893\n",
      "19000 0.6520320773124695 0.5954551562666893 830004 [1e-05, 1e-05] 440.2952950000763\n",
      "20000 0.5660642981529236 0.5957609928846359 831004 [1e-05, 1e-05] 463.1439640522003\n",
      "21000 0.546110987663269 0.5955271325111389 832004 [1e-05, 1e-05] 485.86076736450195\n",
      "22000 0.5657362341880798 0.5959909274578095 833004 [1e-05, 1e-05] 508.77146005630493\n",
      "23000 0.6278414130210876 0.5961961440443992 834004 [1e-05, 1e-05] 531.6129314899445\n",
      "24000 0.555452823638916 0.5960323629975319 835004 [1e-05, 1e-05] 554.63458776474\n",
      "25000 0.6241830587387085 0.5954230777919293 836004 [1e-05, 1e-05] 577.7990102767944\n",
      "26000 0.5295476913452148 0.5938318390846252 837004 [1e-05, 1e-05] 600.6199064254761\n",
      "27000 0.6121094822883606 0.5961561949253082 838004 [1e-05, 1e-05] 623.4815249443054\n",
      "28000 0.5033946633338928 0.5956806299090386 839004 [1e-05, 1e-05] 646.5816478729248\n",
      "29000 0.6092543601989746 0.5952335646152497 840004 [1e-05, 1e-05] 669.6465222835541\n",
      "30000 0.6271778345108032 0.5951254327297211 841004 [1e-05, 1e-05] 692.7425508499146\n",
      "31000 0.6118763089179993 0.5942232666015625 842004 [1e-05, 1e-05] 715.296445608139\n",
      "32000 0.5968329310417175 0.5947961465120316 843004 [1e-05, 1e-05] 737.6046073436737\n",
      "33000 0.6652401089668274 0.5944650473892689 844004 [1e-05, 1e-05] 760.4472548961639\n",
      "34000 0.5864191651344299 0.5951124625205994 845004 [1e-05, 1e-05] 783.4196922779083\n",
      "35000 0.6365917921066284 0.5953328037261962 846004 [1e-05, 1e-05] 806.2980325222015\n",
      "36000 0.5996631979942322 0.5946165044307709 847004 [1e-05, 1e-05] 828.2447807788849\n",
      "37000 0.6100322008132935 0.5935737147927285 848004 [1e-05, 1e-05] 850.239705324173\n",
      "38000 0.5763891339302063 0.5958572925329209 849004 [1e-05, 1e-05] 873.3426749706268\n",
      "39000 0.5829723477363586 0.595667409479618 850004 [1e-05, 1e-05] 896.1752722263336\n",
      "40000 0.5975940823554993 0.5942780874967575 851004 [1e-05, 1e-05] 919.0757901668549\n",
      "41000 0.5608127117156982 0.5962061690688133 852004 [1e-05, 1e-05] 942.1310555934906\n",
      "42000 0.5862641930580139 0.5950119979083538 853004 [1e-05, 1e-05] 965.0812299251556\n",
      "43000 0.6021072864532471 0.5946943213343621 854004 [1e-05, 1e-05] 987.7832851409912\n",
      "44000 0.6157671213150024 0.5966808366179466 855004 [1e-05, 1e-05] 1010.1771142482758\n",
      "45000 0.5944502353668213 0.5945269973874092 856004 [1e-05, 1e-05] 1033.110773563385\n",
      "46000 0.5416873693466187 0.5948357694745063 857004 [1e-05, 1e-05] 1056.0715720653534\n",
      "47000 0.6097061038017273 0.5977501013875007 858004 [1e-05, 1e-05] 1078.5114123821259\n",
      "48000 0.565946102142334 0.594670333981514 859004 [1e-05, 1e-05] 1101.4349901676178\n",
      "49000 0.5767679214477539 0.5949318348765373 860004 [1e-05, 1e-05] 1124.1685166358948\n",
      "50000 0.5546234250068665 0.5942910745739937 861004 [1e-05, 1e-05] 1147.227322101593\n",
      "51000 0.6023555994033813 0.5950714967846871 862004 [1e-05, 1e-05] 1170.390081167221\n",
      "52000 0.6412824392318726 0.5953564227819442 863004 [1e-05, 1e-05] 1193.6268289089203\n",
      "53000 0.6082960963249207 0.5945849306583405 864004 [1e-05, 1e-05] 1216.803010225296\n",
      "54000 0.6076212525367737 0.5960283427834511 865004 [1e-05, 1e-05] 1239.8871204853058\n",
      "55000 0.6401095986366272 0.5921628514528274 866004 [1e-05, 1e-05] 1262.7268977165222\n",
      "56000 0.5891398191452026 0.5928083459734916 867004 [1e-05, 1e-05] 1284.9762139320374\n",
      "57000 0.5474449992179871 0.5940343348383903 868004 [1e-05, 1e-05] 1307.1205713748932\n",
      "58000 0.625002384185791 0.5954097216129303 869004 [1e-05, 1e-05] 1329.6805753707886\n",
      "59000 0.6094816327095032 0.5939136207699776 870004 [1e-05, 1e-05] 1352.9430329799652\n",
      "60000 0.5756435990333557 0.5935517142415047 871004 [1e-05, 1e-05] 1376.1515464782715\n",
      "61000 0.5849568843841553 0.5942549412548542 872004 [1e-05, 1e-05] 1399.2605955600739\n",
      "62000 0.5934614539146423 0.594342318803072 873004 [1e-05, 1e-05] 1421.7862741947174\n",
      "63000 0.6378989815711975 0.5934744594693184 874004 [1e-05, 1e-05] 1444.4082522392273\n",
      "64000 0.659060001373291 0.592878810942173 875004 [1e-05, 1e-05] 1467.2982988357544\n",
      "65000 0.6530255079269409 0.5942152377367019 876004 [1e-05, 1e-05] 1490.4387879371643\n",
      "66000 0.5328420400619507 0.5947469956874848 877004 [1e-05, 1e-05] 1513.368860244751\n",
      "67000 0.6084328293800354 0.5956687243580818 878004 [1e-05, 1e-05] 1535.7590005397797\n",
      "68000 0.6434991955757141 0.5941964603066444 879004 [1e-05, 1e-05] 1558.2055160999298\n",
      "69000 0.5517702102661133 0.5945280771255493 880004 [1e-05, 1e-05] 1580.84974360466\n",
      "70000 0.5687812566757202 0.5925734956860542 881004 [1e-05, 1e-05] 1604.1095232963562\n",
      "71000 0.5980982184410095 0.5932323905825615 882004 [1e-05, 1e-05] 1627.29438829422\n",
      "72000 0.6113020181655884 0.5944350258708 883004 [1e-05, 1e-05] 1650.4762506484985\n",
      "73000 0.605706512928009 0.5937406362295151 884004 [1e-05, 1e-05] 1673.5602939128876\n",
      "74000 0.6268801689147949 0.5944886692762374 885004 [1e-05, 1e-05] 1696.6087093353271\n",
      "75000 0.5625223517417908 0.5934899290204048 886004 [1e-05, 1e-05] 1719.1239421367645\n",
      "76000 0.617461621761322 0.5951290725469589 887004 [1e-05, 1e-05] 1741.612457036972\n",
      "77000 0.6011346578598022 0.5919967004060745 888004 [1e-05, 1e-05] 1764.5935761928558\n",
      "78000 0.5743263959884644 0.5937246127128601 889004 [1e-05, 1e-05] 1787.7076098918915\n",
      "79000 0.5877494812011719 0.5923917725086212 890004 [1e-05, 1e-05] 1810.7907721996307\n",
      "80000 0.5907788276672363 0.5929545886516571 891004 [1e-05, 1e-05] 1833.7322795391083\n",
      "81000 0.6153693199157715 0.5938134937584401 892004 [1e-05, 1e-05] 1856.4020743370056\n",
      "82000 0.6149413585662842 0.5935058497190475 893004 [1e-05, 1e-05] 1879.290376663208\n",
      "83000 0.6083738803863525 0.5940170782208443 894004 [1e-05, 1e-05] 1902.1778481006622\n",
      "84000 0.5448693037033081 0.593444180905819 895004 [1e-05, 1e-05] 1925.1383261680603\n",
      "85000 0.585866391658783 0.5914852396845818 896004 [1e-05, 1e-05] 1948.0698111057281\n",
      "86000 0.5739829540252686 0.5918461292982101 897004 [1e-05, 1e-05] 1971.1135671138763\n",
      "87000 0.5856043696403503 0.5912972318530083 898004 [1e-05, 1e-05] 1993.447693824768\n",
      "88000 0.5974072217941284 0.5924951149225235 899004 [1e-05, 1e-05] 2016.2439622879028\n",
      "89000 0.605906069278717 0.5934591841101646 900004 [1e-05, 1e-05] 2038.842872619629\n",
      "90000 0.5887883901596069 0.5920692920088768 901004 [1e-05, 1e-05] 2062.165026664734\n",
      "91000 0.5767881870269775 0.5926705416440964 902004 [1e-05, 1e-05] 2085.4118604660034\n",
      "92000 0.5886071920394897 0.5928156502842903 903004 [1e-05, 1e-05] 2108.738461256027\n",
      "93000 0.5538831949234009 0.5930138577818871 904004 [1e-05, 1e-05] 2131.9163210392\n",
      "94000 0.6654155254364014 0.5927703448534012 905004 [1e-05, 1e-05] 2155.1336839199066\n",
      "95000 0.5883046388626099 0.5925606004595757 906004 [1e-05, 1e-05] 2178.063170671463\n",
      "96000 0.626347541809082 0.592258808016777 907004 [1e-05, 1e-05] 2200.887094259262\n",
      "97000 0.5722662210464478 0.5928704025149345 908004 [1e-05, 1e-05] 2223.4428973197937\n",
      "98000 0.5602831244468689 0.5927670876383782 909004 [1e-05, 1e-05] 2246.2714383602142\n",
      "99000 0.5619145035743713 0.5907642012238502 910004 [1e-05, 1e-05] 2269.1685531139374\n",
      "100000 0.5625648498535156 0.59232468354702 911004 [1e-05, 1e-05] 2291.8674881458282\n",
      "101000 0.5525505542755127 0.5923991860747337 912004 [1e-05, 1e-05] 2314.8838114738464\n",
      "102000 0.5654924511909485 0.5937751492261887 913004 [1e-05, 1e-05] 2337.7344720363617\n",
      "103000 0.6152640581130981 0.5923882366418839 914004 [1e-05, 1e-05] 2360.1791412830353\n",
      "104000 0.5884802341461182 0.5912416897416115 915004 [1e-05, 1e-05] 2382.9589915275574\n",
      "105000 0.5817825794219971 0.5930643711686134 916004 [1e-05, 1e-05] 2406.04532122612\n",
      "106000 0.5812848806381226 0.5927455929517746 917004 [1e-05, 1e-05] 2429.2627346515656\n",
      "107000 0.57399582862854 0.5910454847812653 918004 [1e-05, 1e-05] 2452.359130859375\n",
      "108000 0.6367223262786865 0.5911491849422454 919004 [1e-05, 1e-05] 2475.4701368808746\n",
      "109000 0.5402447581291199 0.5915798159241676 920004 [1e-05, 1e-05] 2498.56769490242\n",
      "110000 0.5555782318115234 0.5924043931961059 921004 [1e-05, 1e-05] 2521.721504688263\n",
      "111000 0.5655044913291931 0.5918004841208458 922004 [1e-05, 1e-05] 2544.338109731674\n",
      "112000 0.5697173476219177 0.5927007312178612 923004 [1e-05, 1e-05] 2567.2362525463104\n",
      "113000 0.5704451203346252 0.5914458052515984 924004 [1e-05, 1e-05] 2590.3314805030823\n",
      "114000 0.6238893270492554 0.5919823952317238 925004 [1e-05, 1e-05] 2613.6156902313232\n",
      "115000 0.6531063318252563 0.5911294615268707 926004 [1e-05, 1e-05] 2636.6444652080536\n",
      "116000 0.6121131181716919 0.5910231434702873 927004 [1e-05, 1e-05] 2659.6947014331818\n",
      "117000 0.5857092142105103 0.5925920863747597 928004 [1e-05, 1e-05] 2682.9219143390656\n",
      "118000 0.5708415508270264 0.5917252398133278 929004 [1e-05, 1e-05] 2706.430874109268\n",
      "119000 0.5933545827865601 0.591935012280941 930004 [1e-05, 1e-05] 2729.272656917572\n",
      "120000 0.6273118853569031 0.592095584988594 931004 [1e-05, 1e-05] 2751.291925430298\n",
      "121000 0.5877078175544739 0.5932131068408489 932004 [1e-05, 1e-05] 2773.304805994034\n",
      "122000 0.5783349871635437 0.5914442427754402 933004 [1e-05, 1e-05] 2796.171007871628\n",
      "123000 0.5651649236679077 0.5915285869836807 934004 [1e-05, 1e-05] 2819.168215036392\n",
      "124000 0.6140841245651245 0.5923349150419235 935004 [1e-05, 1e-05] 2842.0702424049377\n",
      "125000 0.5830803513526917 0.5920758057832718 936004 [1e-05, 1e-05] 2865.184491634369\n",
      "126000 0.6293948292732239 0.5918197543919086 937004 [1e-05, 1e-05] 2888.372185945511\n",
      "127000 0.5667549967765808 0.5920864344835282 938004 [1e-05, 1e-05] 2911.487356185913\n",
      "128000 0.5819072723388672 0.5926479885280133 939004 [1e-05, 1e-05] 2934.336127758026\n",
      "129000 0.6398321986198425 0.5911683596372604 940004 [1e-05, 1e-05] 2956.7721474170685\n",
      "130000 0.587179958820343 0.5921574243307114 941004 [1e-05, 1e-05] 2979.179217815399\n",
      "131000 0.6354783773422241 0.5924383249878883 942004 [1e-05, 1e-05] 3001.628631591797\n",
      "132000 0.537905752658844 0.5933398850560189 943004 [1e-05, 1e-05] 3024.0401225090027\n",
      "133000 0.6311736702919006 0.5903070277571678 944004 [1e-05, 1e-05] 3046.580781698227\n",
      "134000 0.5909618139266968 0.5930634427070618 945004 [1e-05, 1e-05] 3069.3404400348663\n",
      "135000 0.584644615650177 0.5938006603717804 946004 [1e-05, 1e-05] 3092.5470254421234\n",
      "136000 0.5745049118995667 0.5918933040499688 947004 [1e-05, 1e-05] 3115.702158689499\n",
      "137000 0.5346349477767944 0.5926718166470528 948004 [1e-05, 1e-05] 3138.883688688278\n",
      "138000 0.5596684813499451 0.5937998731136322 949004 [1e-05, 1e-05] 3161.92777633667\n",
      "139000 0.5525740385055542 0.5930699613392353 950004 [1e-05, 1e-05] 3183.9072942733765\n",
      "140000 0.5961889028549194 0.5917121989130973 951004 [1e-05, 1e-05] 3206.7787566184998\n",
      "141000 0.6070603728294373 0.5929004889130592 952004 [1e-05, 1e-05] 3229.375317335129\n",
      "142000 0.575524628162384 0.5920916482806206 953004 [1e-05, 1e-05] 3251.608769416809\n",
      "143000 0.5994857549667358 0.5920061312615872 954004 [1e-05, 1e-05] 3274.088874101639\n",
      "144000 0.6054381728172302 0.5922708446979523 955004 [1e-05, 1e-05] 3296.48459482193\n",
      "145000 0.592897355556488 0.590399873971939 956004 [1e-05, 1e-05] 3318.8568840026855\n",
      "146000 0.6021236777305603 0.5930389222502709 957004 [1e-05, 1e-05] 3342.0100495815277\n",
      "147000 0.58270663022995 0.5919906956553459 958004 [1e-05, 1e-05] 3365.1810219287872\n",
      "148000 0.6071690917015076 0.5928666285872459 959004 [1e-05, 1e-05] 3388.3626034259796\n",
      "149000 0.620112955570221 0.5908220580816269 960004 [1e-05, 1e-05] 3410.8732092380524\n",
      "150000 0.5828040242195129 0.5922961192727089 961004 [1e-05, 1e-05] 3434.010078191757\n",
      "151000 0.5979531407356262 0.5919069597125053 962004 [1e-05, 1e-05] 3457.251664161682\n",
      "152000 0.5886504054069519 0.5931947814226151 963004 [1e-05, 1e-05] 3479.60293674469\n",
      "153000 0.5742657780647278 0.591055314719677 964004 [1e-05, 1e-05] 3502.337539434433\n",
      "154000 0.5974507927894592 0.5902064223885536 965004 [1e-05, 1e-05] 3525.501921415329\n",
      "155000 0.5782136917114258 0.5937488825321198 966004 [1e-05, 1e-05] 3548.336215019226\n",
      "156000 0.5771688222885132 0.5927708721160889 967004 [1e-05, 1e-05] 3571.275254011154\n",
      "157000 0.631689727306366 0.5920358691215515 968004 [1e-05, 1e-05] 3594.4130709171295\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m dataset \u001b[39m=\u001b[39m sample_dataset(i,batch\u001b[39m=\u001b[39mbatch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset[:], batch_size\u001b[39m=\u001b[39mbatch, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m train(dataloader, optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()    \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m avg_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# lr = 5e-4\n",
    "# optimizer = torch.optim.Adam([{'params': gru.parameters()}, {'params': s2h.parameters()}], lr=lr)\n",
    "batch=64\n",
    "s_time = time.time()\n",
    "plot_every = 1000\n",
    "avg_loss = 0\n",
    "for i in range(1000001):\n",
    "    dataset = sample_dataset(i,batch=batch)\n",
    "    dataloader = DataLoader(dataset[:], batch_size=batch, shuffle=True)\n",
    "    loss = train(dataloader, optimizer)\n",
    "    scheduler.step()    \n",
    "    avg_loss += loss\n",
    "    if i and i % plot_every == 0:\n",
    "        print(i, loss, avg_loss / plot_every, scheduler._step_count, scheduler.get_last_lr(),time.time() - s_time)\n",
    "        torch.save(gru, 'model/gru-0914.pth')\n",
    "        torch.save(s2h, 'model/s2h-0914.pth')\n",
    "        save_dict = {'epoch':i,\"optimizer\":optimizer.state_dict(),\"_step_count\":scheduler._step_count,\"last_epoch\":scheduler.last_epoch}\n",
    "        torch.save(save_dict,\"model/save_dict-0914.pth\")\n",
    "        if avg_loss / plot_every < 0.18:\n",
    "            print(i, avg_loss / plot_every)\n",
    "            break\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=2).to(device)\n",
    "def sample(size_data, seq_length, start_size=8):\n",
    "    gru.eval()\n",
    "    s2h.eval()\n",
    "    with torch.no_grad():  # no need to track history in sampling\n",
    "        size_tensor = torch.tensor(size_data, dtype=torch.float).to(device)\n",
    "        hn = s2h(size_tensor)\n",
    "        output_seq = [start_size]\n",
    "        size = start_size\n",
    "        for _ in range(seq_length - 1):\n",
    "            input = inputTensor(np.array([[size]])).to(device)\n",
    "            input = input.float()\n",
    "            output, hn = gru(input, hn)\n",
    "            output = softmax(output)\n",
    "            p_size = output.detach().cpu().numpy().squeeze()\n",
    "            size = np.random.choice(n_size, p=p_size)\n",
    "            output_seq.append(size)\n",
    "        return output_seq\n",
    "\n",
    "def is_subarray(arr1, arr2):\n",
    "    arr1 = np.array(arr1)\n",
    "    arr2 = np.array(arr2)\n",
    "    for i in range(len(arr1) - len(arr2) + 1):\n",
    "        if np.array_equal(arr1[i:i+len(arr2)], arr2):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7, 4] False\n",
      "[8, 4, 10, 4, 13, 4, 8, 4, 11, 2, 4, 8, 6, 6, 11, 8] True\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 11, 10, 7, 7, 4, 6, 7, 6, 6, 4, 13, 10, 8, 8] False\n",
      "[8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4] True\n",
      "[8, 6, 7, 11, 10, 4, 10, 4, 11, 4, 4, 13, 7, 13, 16, 4] False\n",
      "[8, 4, 4, 12, 10, 7, 6, 4, 4, 11, 13, 7, 4, 8, 4, 8] False\n",
      "[8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10, 7, 6, 4] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 7, 11, 8, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6] False\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 4, 8, 11, 13, 4, 8, 4, 4, 7, 13, 4, 7, 8, 6, 10] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11] True\n",
      "[8, 13, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 6, 8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11] False\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 4, 4, 8, 13, 7, 4, 4, 4, 15] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 4, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 9, 10, 8, 4] False\n",
      "[8, 8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4] False\n",
      "[8, 8, 4, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 8, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] False\n",
      "[8, 11, 4, 4, 13, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8] False\n",
      "[8, 13, 4, 13, 6, 13, 7, 13, 6, 7, 7, 4, 11, 8, 4, 8] False\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 13, 10, 7, 4, 4, 7, 7, 13, 13, 4, 16, 4, 4, 12, 10] False\n",
      "[8, 6, 6, 11, 11, 8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13, 7] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 7, 9, 8, 8, 4, 13, 4, 8, 11, 13, 4, 13, 6, 13, 7] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] True\n",
      "[8, 11, 4, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4] False\n",
      "[8, 4, 6, 8, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10, 13] False\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 8, 4, 8, 11, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3] False\n",
      "[8, 13, 7, 4, 4, 4, 15, 4, 11, 13, 4, 7, 11, 13, 4, 4] True\n",
      "[8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6, 8, 6, 11] True\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 11, 6, 4, 13, 6, 6, 4, 13, 10, 8, 8, 7, 11, 7, 4] False\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 4, 6] True\n",
      "[8, 4, 9, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7] True\n",
      "[8, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8, 4, 13, 4, 8, 11] False\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 11, 11, 13, 2, 11] False\n",
      "[8, 6, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 6, 8, 6, 11, 13, 8, 4, 9, 11, 4, 4, 11, 4, 4, 13] False\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6, 6, 11, 11, 8, 11] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7] True\n",
      "[8, 6, 6, 11, 8, 8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7] True\n",
      "[8, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8] False\n",
      "[8, 11, 4, 4, 11, 4, 4, 13, 4, 7, 8, 6, 10, 7, 4, 4] False\n",
      "[8, 4, 7, 7, 10, 4, 9, 7, 8, 11, 8, 8, 4, 8, 11, 8] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9, 8, 8] True\n",
      "[8, 8, 9, 11, 9, 6, 13, 4, 4, 7, 7, 8, 4, 7, 13, 10] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 11, 7, 7, 11, 11, 9, 10, 8, 4, 7, 7, 10, 4, 9, 7] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 6, 8, 4, 6, 8, 6, 11] False\n",
      "[8, 4, 13, 8, 4, 13, 1, 6, 4, 4, 13, 7, 13, 16, 4, 4] False\n",
      "[8, 4, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11] False\n",
      "[8, 4, 8, 8, 9, 4, 11, 4, 8, 8, 9, 11, 9, 6, 13, 4] True\n",
      "[8, 6, 10, 11, 13, 7, 7, 4, 4, 13, 13, 8, 4, 10, 4, 13] False\n",
      "[8, 7, 11, 7, 4, 15, 4, 10, 4, 11, 6, 8, 6, 4, 13, 9] True\n",
      "[8, 6, 7, 13, 7, 13, 15, 5, 8, 4, 13, 8, 4, 13, 1, 6] True\n",
      "[8, 7, 9, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11] False\n",
      "[8, 4, 7, 13, 10, 13, 7, 4, 4, 8, 13, 7, 4, 4, 4, 15] True\n",
      "[8, 13, 4, 7, 16, 13, 7, 6, 7, 3, 7, 11, 10, 4, 10, 4] True\n",
      "[8, 11, 10, 4, 4, 6, 7, 11, 11, 13, 2, 11, 13, 11, 11, 9] True\n",
      "[8, 8, 11, 7, 7, 11, 11, 8, 8, 6, 7, 13, 7, 13, 15, 5] True\n",
      "[8, 13, 7, 8, 13, 8, 6, 4, 6, 4, 4, 4, 4, 6, 8, 6] True\n",
      "[8, 6, 8, 6, 4, 13, 9, 4, 4, 10, 8, 7, 8, 8, 7, 9] False\n"
     ]
    }
   ],
   "source": [
    "pair = 0\n",
    "start_size = 8\n",
    "for i in range(100):\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "    a = sample(sizedata[pair], 16, start_size)\n",
    "    print(a, is_subarray(size_index, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "s2s_pair, size_trans = get_trans(pairdata, freqpairs, 'size_index', n_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plf_dict={}\n",
    "# print(size_trans[0])\n",
    "# plf_dict[str(sizedata[0])]=size_trans[0]\n",
    "\n",
    "# print(sizedata[0])\n",
    "minp=100\n",
    "maxp=0\n",
    "ans=0\n",
    "for i in range(1000):\n",
    "    for j in range(i):\n",
    "        if i==j:\n",
    "            continue\n",
    "        minp=min(np.sum(np.abs(sizedata[i]-sizedata[j])),minp)\n",
    "        maxp=max(maxp,np.sum(np.abs(sizedata[i]-sizedata[j])))\n",
    "        if(np.sum(np.square(sizedata[i]-sizedata[j]))<0.001):\n",
    "            ans+=1\n",
    "print(maxp,minp,ans)\n",
    "# for i in range(1000):\n",
    "#     try:\n",
    "#         plf_dict[str(sizedata[i])]\n",
    "#     # if sizedata[i] in plf_dict.keys():\n",
    "#         temp = plf_dict[str(sizedata[i])]\n",
    "#         temp += size_trans[i]\n",
    "#         temp/=2\n",
    "#         plf_dict[str(sizedata[i])]=temp\n",
    "#         print(\"1\\n\")\n",
    "#     except:\n",
    "#         plf_dict[str(sizedata[i])]=size_trans[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "encore_seq = np.zeros((pairs, 1000))\n",
    "he_seq = np.zeros((pairs, 1000))\n",
    "\n",
    "for pair in tqdm(range(pairs)):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    size_seq = []\n",
    "    while len(size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq.append(size)\n",
    "    size_seq = np.array(size_seq)[0:1000]\n",
    "    he_seq[pair] = size_seq\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(n_size), p=sizedata[pair])\n",
    "        size_seq = [start_size]\n",
    "        while len(size_seq) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq += list(new_size[:])\n",
    "                # start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "            start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        \n",
    "        \n",
    "        size_seq = np.array(size_seq)\n",
    "        values, counts = np.unique(size_seq, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq[:-1] * n_size + size_seq[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            # print(pair, seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "            break\n",
    "\n",
    "    encore_seq[pair] = size_seq[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = pairdata[freqpairs[pair]]['size_index'].values\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        grams[i][pair][values] = counts\n",
    "    grams[i] /= grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "he_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    he_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = he_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        he_grams[i][pair][values] = counts\n",
    "    he_grams[i] /= he_grams[i].sum(axis=1).reshape(-1, 1)\n",
    "\n",
    "encore_grams = {}\n",
    "for i in [2, 3, 4]:\n",
    "    encore_grams[i] = np.zeros((pairs, n_size ** i))\n",
    "    for pair in tqdm(range(pairs)):\n",
    "        sizeindex = encore_seq[pair].astype(int)\n",
    "        l = len(sizeindex) - i + 1\n",
    "        feature = np.zeros(l, dtype=int)\n",
    "        for j in range(i):\n",
    "            feature += sizeindex[j:j+l] * n_size ** (i - j - 1)\n",
    "        values, counts = np.unique(feature, return_counts=True)\n",
    "        encore_grams[i][pair][values] = counts\n",
    "    encore_grams[i] /= encore_grams[i].sum(axis=1).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_jsds, encore_jsds = {}, {}\n",
    "for i in range(2, 5):\n",
    "    he_jsds[i] = []    \n",
    "    encore_jsds[i] = []    \n",
    "    for pair in range(pairs):\n",
    "        he_jsds[i].append(JSD(grams[i][pair], he_grams[i][pair]))\n",
    "        encore_jsds[i].append(JSD(grams[i][pair], encore_grams[i][pair]))\n",
    "    he_jsds[i] = np.array(he_jsds[i])\n",
    "    encore_jsds[i] = np.array(encore_jsds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(2, 5):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.subplots_adjust(left=0.18, top=0.95, bottom=0.24, right=0.98)\n",
    "    ax = plt.subplot(111)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    values, bins = np.histogram(encore_jsds[i], bins=np.arange(0, np.max(encore_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='CornFlowerBlue', label='Encore-Sequential')\n",
    "    values, bins = np.histogram(he_jsds[i], bins=np.arange(0, np.max(he_jsds[i]) + 0.01, 0.01))\n",
    "    cdf = np.cumsum(values) / np.sum(values)\n",
    "    plt.plot(bins[:-1], cdf, linewidth=2, color='IndianRed', label='Common Practice')\n",
    "    plt.ylim(0, 1.05)\n",
    "    # plt.legend(fontsize=20, frameon=False, loc=(0.45, 0.2))\n",
    "    plt.ylabel('CDF', fontsize=20)\n",
    "    plt.xlabel('JSD', fontsize=20)\n",
    "    plt.grid(linestyle='-.')\n",
    "    plt.xticks(fontsize=20)\n",
    "    plt.yticks(fontsize=20)\n",
    "\n",
    "    if i == 2:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.125, 0.84, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    elif i == 3:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.22, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    else:\n",
    "        bbox_props = dict(boxstyle=\"larrow\", fc=\"none\", ec=\"red\", lw=2)\n",
    "        t = ax.text(0.3, 0.8, \"Better\", ha=\"center\", va=\"center\", rotation=0,\n",
    "                    size=18,\n",
    "                    bbox=bbox_props)\n",
    "    plt.legend(fontsize=20, frameon=False, loc=(0.185, -0.025), handletextpad=0.5)\n",
    "\n",
    "    plt.savefig('figure/{i}-gram-jsd.pdf'.format(i=i), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.004082541521851113 0.0337065305030811 0.0736751094920089\n",
      "1 0.004693377259405371 0.08172096425303191 0.1042314247342685\n",
      "2 0.003623476205794533 0.049978884267248794 0.07722916947355046\n",
      "3 0.00490398523215316 0.048527761870073 0.08086079321732315\n",
      "4 0.0028337228124704437 0.05505688584281922 0.1126146170283329\n",
      "5 0.017480166412742065 0.038624228303103345 0.05843249684209055\n",
      "6 0.004230395405295247 0.0635676077243882 0.12974426772181177\n",
      "7 0.0029529465927298952 0.06221945814779238 0.12212496883223506\n",
      "8 0.004024165490224982 0.06014040007310334 0.11672103015604808\n",
      "9 0.00432188690151564 0.0361440380656507 0.09830841721535631\n",
      "10 0.003431598644190163 0.04402259634315614 0.08451325312891833\n",
      "11 0.008530985452907468 0.0742274829189594 0.12403968853068947\n",
      "12 0.0037716664489650968 0.05081919799406754 0.10689129894673811\n",
      "13 0.00448878391159363 0.05478143959800651 0.10637437761040613\n",
      "14 0.009004225946108866 0.05093128224363404 0.09999558257104302\n",
      "15 0.0048795425030875965 0.06131469119904691 0.10472203100377064\n",
      "16 0.0063304804738862205 0.04181317324863333 0.09012752599759008\n",
      "17 0.008705481069172469 0.08542392171321181 0.11524447608156743\n",
      "18 0.0038009667981175905 0.049812078562506336 0.10537829564423173\n",
      "19 0.004890016296689132 0.03718833468513082 0.11007005863028065\n",
      "20 0.003243589791021965 0.06571079584348133 0.0992531175510176\n",
      "21 0.013049188142013383 0.07853267093466146 0.10026866857609482\n",
      "22 0.012159656329214093 0.056887123671955954 0.0912383806024383\n",
      "23 0.004407997917941619 0.08316172199950064 0.12603446948128155\n",
      "24 0.004733139843233586 0.05370741512986503 0.10043666212992552\n",
      "25 0.010666653417939532 0.07410823547630765 0.12682986798650778\n",
      "26 0.004755511812704291 0.04154048756032047 0.09891848859700221\n",
      "27 0.004884105627205843 0.05280452932600795 0.0660232934997412\n",
      "28 0.004012946350770392 0.05764017977103557 0.10568594630988733\n",
      "29 0.003827466377509529 0.07071472937045656 0.13826907750683662\n",
      "30 0.006807367525633531 0.057152674618022495 0.09035059999987288\n",
      "31 0.0028148368982076526 0.023535134387510055 0.04868861446856464\n",
      "32 0.006841247615137776 0.06072735726352822 0.10461376791490962\n",
      "33 0.0049770701469997805 0.050540696292396534 0.08397606629308829\n",
      "34 0.004822201954132148 0.05549803995100361 0.11506215387051524\n",
      "35 0.016996859283083622 0.06916526692419721 0.08351573042379531\n",
      "36 0.011462658804395547 0.08060003027063459 0.10451082864691\n",
      "37 0.003853750454021158 0.05011232470039451 0.08568096205518708\n",
      "38 0.002409540279453882 0.0331513028146021 0.07490635155715515\n",
      "39 0.004890703980273924 0.08571738227969017 0.11519057774560024\n",
      "40 0.008319402447590744 0.058359519272859345 0.10117835158983615\n",
      "41 0.003342560599710804 0.058828385452012875 0.08413754987357422\n",
      "42 0.0038586435902852496 0.04002205297136312 0.08396054638410728\n",
      "43 0.003937649148828503 0.07399004575464371 0.09211931971990357\n",
      "44 0.0047640641150525145 0.04447562607083595 0.102618795260109\n",
      "45 0.010731605158181983 0.05987550819968124 0.0942617808089699\n",
      "46 0.010057454049228046 0.05351440977493972 0.09409308715180492\n",
      "47 0.004817406136103515 0.03648291455244415 0.08239539380723146\n",
      "48 0.010609061406061287 0.08600338749238702 0.09706201021618441\n",
      "49 0.003790656043284385 0.04549181652455006 0.08237362303622925\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m size_seq_gen \u001b[39m=\u001b[39m [start_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(size_seq_gen) \u001b[39m<\u001b[39m \u001b[39m1000\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     new_size \u001b[39m=\u001b[39m sample(size_data, seq_len, start_size\u001b[39m=\u001b[39;49mstart_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     size_seq_gen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(new_size[\u001b[39m1\u001b[39m:])\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m inputTensor(np\u001b[39m.\u001b[39marray([[size]]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m output, hn \u001b[39m=\u001b[39m gru(\u001b[39minput\u001b[39;49m, hn)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m output \u001b[39m=\u001b[39m softmax(output)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m p_size \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39msqueeze()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/root/plf/hotNets-Encore/gru.ipynb 单元格 26\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     out_\u001b[39m=\u001b[39mout\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m out \u001b[39m=\u001b[39m o(out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mif\u001b[39;00m i\u001b[39m%\u001b[39m\u001b[39m3\u001b[39m\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B166.111.68.231/root/plf/hotNets-Encore/gru.ipynb#X34sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     out \u001b[39m=\u001b[39m out \u001b[39m+\u001b[39m out_\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp/lib/python3.10/site-packages/torch/nn/functional.py:1457\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   1456\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1457\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m   1458\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_jsds, he_jsds,min_jsds = [], [],[]\n",
    "for pair in range(pairs):\n",
    "    size_data = sizedata[pair]\n",
    "    size_index = np.concatenate((pairdata[freqpairs[pair]].size_index.values, pairdata[freqpairs[pair]].size_index.values[0:1]))\n",
    "\n",
    "    for seed in range(10):\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = [start_size]\n",
    "        while len(size_seq_gen) < 1000:\n",
    "            new_size = sample(size_data, seq_len, start_size=start_size)\n",
    "            # if set(new_size).issubset(np.unique(pairdata[freqpairs[pair]].size_index.values)):\n",
    "            size_seq_gen += list(new_size[1:])\n",
    "            start_size = new_size[-1]\n",
    "                # if seed > 10:\n",
    "                #     start_size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        size_seq_gen = np.array(size_seq_gen)\n",
    "        values, counts = np.unique(size_seq_gen, return_counts=True)\n",
    "        new_size = np.zeros(n_size, dtype=float)\n",
    "        new_size[values] = counts\n",
    "        new_size /= new_size.sum()\n",
    "\n",
    "        rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "        values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "        rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "        rnn_s2s_pair[values] = counts\n",
    "        rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "        \n",
    "        # print(seed, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "        if JSD(new_size, sizedata[pair]) < 0.005:\n",
    "            break\n",
    "\n",
    "    rnn_s2s_pair = [size_seq_gen[:-1] * n_size + size_seq_gen[1:]]  \n",
    "    values, counts = np.unique(rnn_s2s_pair, return_counts=True)\n",
    "    rnn_s2s_pair = np.zeros(n_size ** 2, dtype=float)\n",
    "    rnn_s2s_pair[values] = counts\n",
    "    rnn_s2s_pair /= rnn_s2s_pair.sum()\n",
    "\n",
    "    he_size_seq = []\n",
    "    while len(he_size_seq) < 1000:\n",
    "        size = np.random.choice(np.arange(30), p=sizedata[pair])\n",
    "        he_size_seq.append(size)\n",
    "    he_size_seq = np.array(he_size_seq)\n",
    "\n",
    "    he_s2s_pair = [he_size_seq[:-1] * n_size + he_size_seq[1:]]  \n",
    "    values, counts = np.unique(he_s2s_pair, return_counts=True)\n",
    "    he_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    he_s2s_pair[values] = counts\n",
    "    he_s2s_pair /= he_s2s_pair.sum()\n",
    "    \n",
    "    min_size_seq = []\n",
    "    min_size_seq.append(np.random.choice(n_size, p=size_data))\n",
    "    while len(min_size_seq) < 1000:\n",
    "        min_size_seq.append(np.random.choice(n_size, p=size_trans[pair][min_size_seq[-1]]))\n",
    "    min_size_seq = np.array(min_size_seq)\n",
    "        \n",
    "    min_s2s_pair = [min_size_seq[:-1] * n_size + min_size_seq[1:]]  \n",
    "    values, counts = np.unique(min_s2s_pair, return_counts=True)\n",
    "    min_s2s_pair = np.zeros(n_size * n_size, dtype=float)\n",
    "    min_s2s_pair[values] = counts\n",
    "    min_s2s_pair /= min_s2s_pair.sum()\n",
    "\n",
    "    print(pair, JSD(new_size, sizedata[pair]), JSD(rnn_s2s_pair, s2s_pair[pair]), JSD(he_s2s_pair, s2s_pair[pair]))\n",
    "    rnn_jsds.append(JSD(rnn_s2s_pair, s2s_pair[pair]))\n",
    "    min_jsds.append(JSD(min_s2s_pair, s2s_pair[pair]))\n",
    "    he_jsds.append(JSD(he_s2s_pair, s2s_pair[pair]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 1.05)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZFklEQVR4nO3deXhU5fnG8W8y2VcgZIWw77InsilVEVFcutmKomIVVMSqgNWK/qxbK9qqpS6gglRRRFyqtZYqKIrIooCASNj3JSEkgezbzJzfH8NEQgLMJDM5M5P7c125cjKckXuOyeThPe/7PkGGYRiIiIiImCTY7AAiIiLSvKkYEREREVOpGBERERFTqRgRERERU6kYEREREVOpGBERERFTqRgRERERU6kYEREREVOFmB3AFXa7ncOHDxMbG0tQUJDZcURERMQFhmFQXFxMWloawcGnH//wi2Lk8OHDpKenmx1DREREGuDAgQO0bdv2tH/uF8VIbGws4HgxcXFxJqcRERERVxQVFZGenl7ze/x0/KIYcd6aiYuLUzEiIiLiZ842xUITWEVERMRUKkZERETEVCpGRERExFQqRkRERMRUKkZERETEVCpGRERExFQqRkRERMRUKkZERETEVCpGRERExFQqRkRERMRUbhcjX3/9NVdddRVpaWkEBQXx0UcfnfU5y5YtIyMjg4iICDp16sTLL7/ckKwiIiISgNwuRkpLS+nXrx8vvviiS+fv2bOHyy+/nOHDh7N+/XoefPBB7r77bj744AO3w4qIiEjgcbtR3ujRoxk9erTL57/88su0a9eOGTNmANCzZ0/Wrl3LM888w9VXX+3uXy9NoNxaTmRIpNkxRPxSeZWNyDCL2TFE/IrX54ysWrWKUaNG1Xrs0ksvZe3atVRXV9f7nMrKSoqKimp9SNM4VnGMqz++mtc2vYZhGGbHEfEr5VU2fvvKSp74JAurzW52HJHTqy6HA2vg21fhwztg5lA49L1pcdweGXFXTk4OycnJtR5LTk7GarWSl5dHampqnedMnz6dxx57zNvR5BTV9mqmfjWVA8UH+GDHB4zpPoaYsBizY4n4BcMwuO/9jfx4qIjDxyu4dXgnUuIjzI4lAtZKOLIZDq8/8bEBcrPAsNU+7/D30GagKRG9XowABAUF1fra+S/uUx93mjZtGlOnTq35uqioiPT0dO8FFACe+vYp1h5ZS3RoNC+MeEGFiIgbXvpyJ5/8kE1IcBAv35ChQkTMYauG3C0nFR7rHYWIvZ47EdGJkDYQ0gY4Ptqe2/R5T/B6MZKSkkJOTk6tx3JzcwkJCSEhIaHe54SHhxMeHu7taHKShVsX8u72dwkiiKeHP03nFp3NjiTiNxZvzuGZxdsBeOKXvRnUsZXJiaRZsFkhb3vtwiNnE9gq654b2eqnosP5EZcGpxkUaGpeL0aGDh3Kf/7zn1qPLV68mMzMTEJDQ73914sLvsv+jqe+ewqAewbewwXpF5icSMR/bM0pYsrCDQDcNLQ91w1qZ24gCUx2G+TvrH2rJecHqC6re25EPKT2r114tGjnM4VHfdwuRkpKSti5c2fN13v27GHDhg20atWKdu3aMW3aNA4dOsS8efMAmDhxIi+++CJTp07l1ltvZdWqVbz22mssWLDAc69CGuxA8QHuXXYvVsPKFZ2u4Jbet5gdScRvFJRWceu8tZRW2RjWOYH/u7KX2ZEkENjtcGxP7RGP7I1QVVL33LBYSOt/4uNE4dGyo08XHvVxuxhZu3YtF110Uc3XzrkdN910E6+//jrZ2dns37+/5s87duzIokWLmDJlCi+99BJpaWk8//zzWtbrA0qrS7l76d0crzxO74TePDr00dPO4xGR2qptdibNX8eBgnLaJ0Tx0tiBhFq0qbW4yTDg+L7ahcfhjVBZWPfc0ChI7Vd7xKNVZwj2/++7IMMP1m8WFRURHx9PYWEhcXFxZscJCHbDzj1f3sNXB74iMTKRBVcsIDk6+azPExGH//toE2+t3k9MeAj/mjSMbsmxZkcSf5K/Cz57CA6shvJjdf88JAJS+tQuPFp3g2D/2sPG1d/fTbKaRnzPi+tf5KsDXxEWHMaMi2aoEBFxw5ur9/HW6v0EBcGMMf1ViIh78nfB61dAcbbja0sYJJ9Tu/BI7AGW5jOvUsVIM/Tpnk+ZvWk2AI8Oe5S+iX1NTiTiP1btyuexjzcDcN+l3RnZS4W8uKFgD7xxlaMQSewBv5wJyb0hpHmvIFUx0sxszt/MwyseBuDm3jdzVeerTE4k4j8OFJQxaf46rHaDn/dL444LtARe3HBsn6MQKTrkuOVy038gJsnsVD7B/2e9iMvyyvO4Z+k9VNgqGN5mOPcMuMfsSCJ+o6TSyoQ31nKsrJq+beP562/6asK3uO74AXjjSig8AAldVIicQsVIM1Flq+KeL+/hSNkROsZ35OmfPY3FzyZCiZjFbjeYsnAD244Ukxgbzqs3ZhIRqp8fcVHhIUchcnw/tOrkKERiU8xO5VNUjDQDhmHw+KrH+eHoD8SGxfLCiBeIDdOEOxFX/f3z7SzJOkJYSDCv3qit3sUNRYcdhcixvdCyA9z0iWPnU6lFxUgz8GbWm/x717+xBFl45oJnaB/X3uxIIn7jPxsP88JSx0aP03/VhwHtWpqcSPxGcY5jjkjBbscOqDd9AvFtzE7lk1SMBLgVh1bw7LpnAbjv3PsYljbM5EQi/uPHQ4Xc9/5GAG77WSeuzmhrciLxGyW5jkIkfyfEpzsKkRZq+Ho6KkYC2J7CPdy37D7shp1fd/01Y3uMNTuSiN/ILa7g1nlrqai2c2H3RP54WQ+zI4m/KDnqKETytkNcW8cckZYakT4TFSMBqqiqiLuX3k1xdTEDkgbw0OCHNPNfxEWVVhsT31xHdmEFnRKjef66AViC9fMjLijNh3m/gKNbITYNbvoYWnU0O5XPUzESgGx2G/cvu5+9RXtJiU7huQufI8wSZnYsEb9gGAYPffgj3+8/TlxECHPGZRIX0Xx2wpRGKCtwFCK5myEmxTEikqC9aFyhYiQAPbfuOVYcXkFkSCQvjHiB1pGtzY4k4jfmrtjL++sOEhwEL44dSKfEGLMjiT8oP+YoRI5sgugkRyHSuovZqfyGipEA89HOj5iXNQ+AJ857gh6tdJ9bxFVfbz/KX/6bBcBDV/TiZ90STU4kfqH8OMz7JeT8ANGJjkIksZvZqfyKipEAsiF3A4+vehyAif0mcmmHS01OJOI/dh8t4fdvf4/dgN9mtOWW8zqYHUn8QUUhvPVryN4AUQkw7mNI0j8C3aViJEDklOYw+cvJVNurubjdxdzR7w6zI4n4jcLyaibMW0tRhZWM9i358696a8K3nF1FEbx1NRxaB5EtHYVIci+zU/klFSMBoNxazj1f3kN+RT7dWnbjyfOfJDhI/2tFXGGzG9y9YD27j5aSGh/ByzdkEB6ird7lLCpLYP5v4eAaiGgB4/4NKb3NTuW39BvLzxmGwSMrHiErP4uW4S15fsTzRIVGmR1LxG88/elWlm0/SkRoMLPHZZIY27xbuYsLqkrh7WvgwGqIiIdxH0FqP7NT+TUVI35uzqY5/G/v/wgJCuG5C5+jTYy2GhZx1fvrDvLq17sBeOa3/ejdJt7kROLzqsrg7TGwbwWEx8GNH0LaALNT+T0VI35s6f6lPL/+eQAeHPIgmSmZJicS8R/f7z/Gg//aBMBdI7pwZV81L5OzqC6HBdfC3uUQFgs3/AvaZJidKiCoGPFTO47tYNryaQBc2/1aftvttyYnEvEf2YXl3P7mOqpsdkb1SmbKSC3DlLOoroB3xsKeZRAWAzd8AOnnmp0qYKgY8UPHKo5x19K7KLOWMShlEPcPut/sSCJ+o6Laxm3z1nG0uJIeKbH8fUx/grXVu5yJtRIWXg+7lkJoNFz/HrQbbHaqgKJixM9U26u5d9m9HCo5RNuYtjx7wbOEBmurahFXGIbB/e//wKZDhbSMCmX2uEyiw0PMjiW+zFoJC2+EnZ9DSCRc/y60V/dzT1Mx4mee/u5p1uSsISokihdGvECLiBZmRxLxGzO/2sXHGw8TEhzEzOszSG+llWdyBtYqeO9m2PEZhETA2IXQ4XyzUwUkFSN+5N1t77Jw20KCCOLpnz1Nl5bqeyDiqiVZR3hm8TYAHv35OQztnGByIvFptmp4/2bY9l+whMN1C6DTBWanClgqRvzEmpw1TP92OgB3D7ybC9MvNDeQiB/ZfqSYye+sxzDghiHtuGFIe7MjiS+zWeGD8bD1E7CEwbVvQ+cRZqcKaCpG/MDB4oNM/WoqVsPK6I6jGd97vNmRRPzGsdIqJryxltIqG0M6teKRq84xO5L4MpsVPrwNsv4NwaEwZj50HWl2qoCnYsTHlVaXctfSuzheeZxeCb14fNjj6pkh4qJqm51J879nf0EZ6a0imXl9BqEWve3Jadht8NEd8OMHJwqRN6HbKLNTNQv6qfRhdsPOg8sfZOfxnbSObM0/LvoHESERZscS8RtPfJLFqt35RIdZmDPuXFpFh5kdSXyV3Qb/vhM2vQvBIfDbf0L30WanajZUjPiwmRtmsvTAUkKDQ5lx0QxSolPMjiTiN+Z/u495q/YB8Pcx/emeEmtyIvFZdjt8fDdsXABBFvjNXOh5ldmpmhUVIz7q072f8soPrwDw6LBH6ZeoJkwirvp2dz6P/HszAH8Y1Y1R56iQl9Ow2+GTe2DDWxAUDFfPgV6/MDtVs6NixAdl5Wfx8DcPA/C7c37Hzzv/3OREIv7jQEEZd8z/Hqvd4Mq+qdx5kZbAy2kYBiy6F76f5yhEfvUq9P612amaJRUjPiavPI+7l95Nha2C89qcx+SBk82OJOI3Siut3DpvLQWlVfRuE8ffftNPE76lfoYBi+6DtXOBIPjlLOirHl9mUTHiQ6psVUz5cgpHyo7QIa4Df/3ZX7EEW8yOJeIX7HaDqe9uYGtOMa1jwnn1xkwiw/TzI/UwDPh0GqyZDQTBL16CfteanapZUzHiIwzD4InVT7Dh6AZiw2J5YcQLxIXFmR1LxG/M+GIHn20+QpglmFduzCCtRaTZkcQXGQYs/j/4dpbj658/DwOuNzeTqBjxFfO3zOejnR8RHBTMMz97hg7xHcyOJOI3/vtDNs9/sQOAv/yqNxntW5qcSHzW54/Cqhcdx1fOgIHjzEwjJ6gY8QF2w86LGxw/HPdm3MuwNuoIKeKqL7fmMmXhBgAmnN+R32ammxtIfNeaObBihuP48mcg82ZT48hPVIz4gIKKAkqrSwkOCua6nteZHUfEbyzbfpTb31pHlc3O6N4pPDC6h9mRxFftWQ7/+6PjeMTDMOhWc/NILSpGfMDhksMAJEYmEhocanIaEf/wzY48bpu3liqrnUvPSeb56wYQoq3epT7H9sK748Buhd6/geH3mp1ITqGfXB+QXZoNQFpMmslJRPzDyp15jH9jDZVWOyN7JvHCdQPVc0bqV1kMC66D8gJI7Q+/eBG03Nvn6KfXB2SXOIoRbfcucnard+cz/o21VFrtjOiRxEvXDyQsRG9lUg+7Hf51O+RmQUwyXPs2hGqVlS/ST7APOFzquE2TFq2REZEzWbO3gFteX0N5tY0LuiUy8/qBhIdoLxE5ja+ehG3/BUsYjJkP8W3MTiSnoWLEBzhHRnSbRuT01u0r4Hdzv6Osysbwrq155cYMIkJViMhp/PgBfP03x/FVz0P6uebmkTNSMeIDnHNGUqNTTU4i4pvW7z/GTXPXUFplY1jnBF69MVOFiJze4Q3w0Z2O42F3QX+tUvR1KkZ8QM1tGo2MiNSx8cBxxr32HSWVVoZ0asWcm7TNu5xB8RF4ZyxYy6HLSBj5mNmJxAUqRkxWUlVCcVUxoJERkVP9eKiQG1/7luJKK4M6tOK1m84lKizE7Fjiq6yVsPAGKDoECV3h6tdA/b38gooRkzlHReLD44kKjTI5jYjv2Hy4kOvnfEtRhZWM9i2Ze/O5RIerEJHTMAz4ZAoc/A4i4uG6dyCyhdmpxEUqRkxWM3lVK2lEamzJLuKGOd9SWF5N//QWvH7zucSoEJEzWT0LNsyHoGD4zT+hdRezE4kbVIyYTJNXRWrbllPM9XO+5VhZNf3axjNv/CBiI7QzsZzBzi9g8UOO41F/gS4Xm5tH3KZixGTO2zSpMSpGRHbmFnP9nNUUlFbRp00888YPJk6FiJxJ3k54/2Yw7ND/Bhhyh9mJpAFUjJjMeZtGIyPS3O06WsJ1s78lr6SKXqlxvDl+EPGRKkTkDMqPw4JroaIQ0gfDlc9pq3c/pWLEZFrWKwJ78kq57tXVHC2upEdKLPMnDKZFVJjZscSX2W3wwXjI3wFxbWDMWxASbnYqaSAVIybLKckBNIFVmq99+Y5CJLe4ku7JjkKkZbQKETmLzx+BnZ9DSKSj50xMktmJpBFUjJio2lbN0fKjgJrkSfN0oKCM615dTU5RBV2TYph/62ASYvSvWzmLDQtg5QuO41/OhLT+psaRxlMxYqKc0hwMDCIsEbSKaGV2HJEmdaCgjGtfXc3hwgo6JUYz/9bBtFYhImdzYA38527H8c/ug96/NjePeISKERM554ukRKcQpElX0owcOl7O2DmrOXS8nI6to1lw6xCSYiPMjiW+rvAQLLwebFXQ40q48EGzE4mHqBgxkXOPEU1eleYku7CcsbNXc6CgnPYJUSy4dQjJcSpE5Cyqyx09Z0qOQNI58KtXIFi/wgKF/k+aSMt6pbk5UlTB2Nnfsi+/jPRWkSy4dQgp8SpE5CwMA/79e8jeAJGt4Lq3ITzG7FTiQSpGTFSz4ZmKEWkGcosruG72avbkldKmhaMQSWsRaXYs8Qff/B1+fB+CQ+CaedCyg9mJxMNUjJiopi+NbtNIgDtaXMnY2d+y+6ijEHnntiG0banGkOKCbf+DLx53HI/+K3Qcbm4e8QoVIyZSXxppDvJLKrl+zmp25paQGh/B27cOJr2VChFxQe4W+GACYMC5E+Dc8WYnEi9pUDEyc+ZMOnbsSEREBBkZGSxfvvyM58+fP59+/foRFRVFamoqN998M/n5+Q0KHCjshl0TWCXgFZRWcf2cb9l+pITkuHDevnUI7ROizY4l/qCswLHVe1UJdBgOlz1ldiLxIreLkYULFzJ58mQeeugh1q9fz/Dhwxk9ejT79++v9/xvvvmGcePGMX78eDZv3sx7773HmjVrmDBhQqPD+7P88nyq7dUEBwWTGJVodhwRjzteVsUNc75la04xibGOQqRjaxUi4gJbNbw7Do7thRbt4bdvgEV9igKZ28XIc889x/jx45kwYQI9e/ZkxowZpKenM2vWrHrPX716NR06dODuu++mY8eOnH/++dx+++2sXbu20eH9mXPyalJUEqHB+iGTwFJYVs0Nr31LVnYRrWPCWXDrYDonavWDuOizB2HvcgiLgevegegEsxOJl7lVjFRVVbFu3TpGjRpV6/FRo0axcuXKep8zbNgwDh48yKJFizAMgyNHjvD+++9zxRVXnPbvqayspKioqNZHoKm5RaOeNBJgCsuruXHut/x4qIiE6DDevnUwXZJizY4l/mLtP+G7V4Eg+PVsSO5ldiJpAm4VI3l5edhsNpKTk2s9npycTE5OTr3PGTZsGPPnz2fMmDGEhYWRkpJCixYteOGFF07790yfPp34+Piaj/T0dHdi+oWaPUZiNHlVAkdxRTU3zf2OHw4W0jIqlPm3DqZbsgoRcdHeFbDoD47jEf8HPS43N480mQZNYD1163LDME67nXlWVhZ33303f/rTn1i3bh2ffvope/bsYeLEiaf970+bNo3CwsKajwMHDjQkpk87XKI9RiSwlFRa+d0/17DhwHFaRIUyf8IQeqTEmR1L/MWxffDujWC3wjm/huH3mp1ImlCIOye3bt0ai8VSZxQkNze3zmiJ0/Tp0znvvPO47777AOjbty/R0dEMHz6cP//5z6Sm1v1lHB4eTnh4YDfM0rJeCSSllVZu/ud3rNt3jLiIEN4aP5heaSpExEWVJY6t3svyIbU//OIlUL+uZsWtkZGwsDAyMjJYsmRJrceXLFnCsGHD6n1OWVkZwaf0D7BYLIBjRKW5ck5g1bJe8XdlVVZufn0Na/YeIzYihLcmDKZ3m3izY4m/sNvhw9vhyI8QnQTXvg1h2oemuXH7Ns3UqVOZM2cOc+fOZcuWLUyZMoX9+/fX3HaZNm0a48aNqzn/qquu4l//+hezZs1i9+7drFixgrvvvptBgwaRltZ8fxHnlDhGlzSBVfxZeZWN8a+v5bs9BcSGh/Dm+MH0bdvC7FjiT5Y9DVs/AUsYXDsf4tuYnUhM4NZtGoAxY8aQn5/P448/TnZ2Nr1792bRokW0b98egOzs7Fp7jvzud7+juLiYF198kXvvvZcWLVowYsQInn76ac+9Cj9TXFVMcXUxACnRKSanEWmYimobt725llW784kOs/D6LYPon97C7FjiTzZ/CMtObGZ21T8gfZC5ecQ0QYYf3CspKioiPj6ewsJC4uL8/z70toJt/OY/v6FFeAuWX3vm3WtFfNUf3tvI++sOEhVmYd4tg8js0MrsSOJPsjfCa5eCtRyG/h4u/YvZicQLXP39rd40JtDkVfF3izZl8/66gwQFwexxmSpExD0lubBgrKMQ6XwxjHzM7ERiMhUjJlBPGvFnOYUVPPjhJgDuuKAz53VpbXIi8SvWKlh4IxQdhIQu8Ju5YHF7xoAEGBUjJqjZ8EwjI+Jn7HaD+97fyPGyanq3iWPyyG5mRxJ/Yhjw36lwYDWExzu2eo9sYXYq8QEqRkzgXNarYkT8zesr97J8Rx4RocHMGDOAsBC9hYgbvn0F1r8JQcHw27nQuqvZicRH6J3EBM6REd2mEX+yLaeYpz7dCsBDV/SiS5Ia34kbdi2Fz6Y5ji95ArqMNDeP+BQVIyaomcCqvjTiJyqtNu55Zz1VVjsXdU/khsHtzI4k/iR/F7x3Mxh26H89DL3T7ETiY1SMNLEqWxVHy48Cuk0j/uOZz7axNaeYhOgw/vqbfqftRSVSr3/fCRXHoe0guPLv2upd6lAx0sRySh07r0ZYImgZ3tLkNCJnt2JnHrOX7wHgqav7khgb2H2jxMOyN8L+VRAcCr99HUL0/SN1qRhpYjWTV2NS9a9L8XmFZdXc++5GAK4b1I5LetXfEFPktNa85vjc8ypt9S6npWKkidVMXlVPGvFxhmHw4EebyCmqoGPraB6+sqfZkcTfVBTCpvccx+dOMDeL+DQVI01Mk1fFX3y04RD//SEbS3AQM8b0JypMG1OJmzYuhOoySOwJ7evv7C4CKkaa3OES7TEivu9AQRl/+mgzAJMv7ko/NcATdxkGrD1xiybzFk1alTNSMdLE1JdGfJ3NbjD13Q0UV1rJaN+SOy7sbHYk8Uf7VsLRrRAaBf3GmJ1GfJyKkSamvjTi615etos1e48REx7C36/pT4hFbxPSAM5RkT6/hYh4c7OIz9O7TBOyG/aapb2awCq+aNPBQv6+ZDsAj/78HNolRJmcSPxSSS5kfew4Pne8uVnEL6gYaUJ55XlU26uxBFlIjEo0O45ILeVVNu5ZuB6r3eDyPilcPVDLMKWB1r8J9mpokwmp/cxOI35AxUgTck5eTYpKIiRYKxPEtzy5aAu7j5aSHBfOX37ZR/vgSMPYbbD2dcexRkXERSpGmpAmr4qvWrr1CG+u3gfAM7/tR8voMJMTid/asQQK90NECzjnV2anET+hYqQJafKq+KK8kkruf/8HAG45ryPDu+oWojSCc+LqgBsgNNLcLOI3VIw0Ie0xIr7GMAwe+OAH8kqq6J4cy/2XdTc7kvizY3sdIyPg2FtExEUqRpqQdl8VX7PguwN8viWXMEswM67tT0SoxexI4s/WvQ4Y0OkiSND+NOI6FSNNyDkyomW94gt2Hy3hiU+yALj/su70TI0zOZH4NWslfP+m41gTV8VNKkaakHOPEY2MiNmqbXamLNxAebWNYZ0TuOW8jmZHEn+35T9QlgexadBttNlpxM+oGGkiRVVFlFSXAJASlWJyGmnunv9iBxsPFhIfGcqz1/QjOFjLeKWR1pyYuJpxE1i0dYG4R8VIE8kuccwXaRnekqhQ7Wop5lm7t4CXvtwJwJO/6kNqvFY8SCMdyYL9KyHIAgPHmZ1G/JCKkSZSs5JGt2jERMUV1Ux5dwN2A349sA1X9NX3o3jA2rmOzz0uhzjNiRP3qRhpIjV7jGjyqpjosf9kcaCgnLYtI3ns5+eYHUcCQWUJbHzHcZypiavSMCpGmoiW9YrZFm3K5v11BwkOgueu6U9sRKjZkSQQbHoXqoqhVWfoeIHZacRPqRhpItrwTMyUU1jBgx9uAuCOCzszqGMrkxNJQDAMWHPiFk3mLRCsXynSMPrOaSK6TSNmsdsN/vDeRo6XVdO3bTyTR3YzO5IEioNr4MgmCImA/mPNTiN+TMVIE9FtGjHLP1fu5ZudeUSEBvP3Mf0JtejHXjzEuZy399UQpdE2aTi9KzWBSlsleeV5gEZGpGltzSni6U+3AvB/V/Sic2KMyYkkYJQVwOYPHceauCqNpGKkCTh3Xo0MiSQ+PN7kNNJcVFTbmPzOBqqsdkb0SOL6we3MjiSBZP1bYKuE1H7QZqDZacTPqRhpAidPXg0K0k6X0jSeXbyNrTnFJESH8fTVffW9J55jt/+0t0jmeND3ljSSipEmoJ400tRW7Mxj9vI9APz1N31JjA03OZEElN1fwrE9EB4PfX5jdhoJACpGmsDhUnXrlaZzvKyKe9/dCMD1g9txcc9kkxNJwHGOivS7FsKizc0iAUHFSBPQHiPSVAzD4KEPfySnqIJOraN56IqeZkeSQFN4ELYtchxn3mJuFgkYKkaagJb1SlP5cP0h/rspm5DgIGZc25+oMHVPFQ9b9wYYdmh/PiT1MDuNBAgVI03AOTKi2zTiTQcKyvjTvzcDMHlkV/q2bWFuIAk8tmr4fp7j+FyNiojnqBjxMpvdxpGyIwCkxagYEe+w2Q2mvruBkkorme1bcseFXcyOJIFo63+hJAeik6DHVWankQCiYsTL8srzsNqtWIIstI5sbXYcCVAvL9vFmr3HiAkP4e9j+mMJ1lJL8YK1J3ZcHTgOQsLMzSIBRcWIlznniyRHJRMSrPv34nk/HDzO35dsB+Cxn59DeqsokxNJQMrbAXu+hqBgyPid2WkkwKgY8bKalTSavCpeUFZlZfI7G7DaDa7ok8qvB7YxO5IEKudy3q6XQot0c7NIwFEx4mXq1ive9OSiLezOKyU5Lpy//Kq3dlkV76gqgw3zHcfnqg+NeJ6KES9zFiMp0SkmJ5FAs3TrEd5avR+AZ3/bnxZRuocvXrL5X1BRCC3aQ+eLzU4jAUjFiJfVLOvVShrxoLySSu5//wcAJpzfkfO7anK0eNGaOY7PmTdDsH5tiOfpu8rLdJtGPM0wDP74/g/klVTRIyWWP1za3exIEsgOfQ+H14MlDAbcaHYaCVAqRrzIMAxNYBWPe/u7/XyxNZcwSzAzru1PRKjF7EgSyJzLeXv9AqI1AifeoWLEi4qqiiizlgHqSyOecaSogj9/sgWA+y/rTo+UOJMTSUArPwabPnAcZ2riqniPihEvct6iaRXRioiQCJPTSCBYtCmb8mob/drGc8t5Hc2OI4Fu4ztgLYekXtBuiNlpJICpGPEidesVT/t8i6O1wFX90gjWLqviTYbx094i544HLRsXL1Ix4kU1k1e1kkY8oLC8mm93FwBwSa9kk9NIwNu7HPK2Q1gM9B1jdhoJcCpGvCi7xFGMaGREPOGrbblY7QbdkmNonxBtdhwJdGtOTFztew2Ex5qbRQKeihEvOlyq2zTiOYuzHLdoRvbUqIh4WXEObP3EcayJq9IEVIx4Uc3IiJb1SiNVWm0s23YU0C0aaQLfzwO7FdIHQ0pvs9NIM6BixIucIyPa8Ewaa/XuAkoqrSTFhtOvbQuz40ggs1lh3euOY42KSBNRMeIlFdYKCiockw01gVUaa0lWDgAX90zWKhrxrh2fQdEhiGzl2OhMpAmoGPGSnFLHL4/IkEjiwrQxlTScYRh8npULwCjdohFvc05cHXADhGp/JGkaKka85ORbNGrrLo3x46EicooqiAqzMLRzgtlxJJAV7IZdXziOM282N4s0Kw0qRmbOnEnHjh2JiIggIyOD5cuXn/H8yspKHnroIdq3b094eDidO3dm7ty5DQrsLzR5VTzFeYvmZ10T1YdGvGvtPx2fO18MrTqZm0WalRB3n7Bw4UImT57MzJkzOe+883jllVcYPXo0WVlZtGvXrt7nXHPNNRw5coTXXnuNLl26kJubi9VqbXR4X6ZuveIpziW9WkUjXlVdAevfchyfq4mr0rTcLkaee+45xo8fz4QJEwCYMWMGn332GbNmzWL69Ol1zv/0009ZtmwZu3fvplWrVgB06NChcan9gLMY0ciINMaBgjK25hRjCQ5iRI8ks+NIIMv6N5QXQFxb6Hqp2WmkmXHrNk1VVRXr1q1j1KhRtR4fNWoUK1eurPc5H3/8MZmZmfz1r3+lTZs2dOvWjT/84Q+Ul5ef9u+prKykqKio1oe/UV8a8YQlJ0ZFMtu3pGV0mMlpJKCtPTFxNeN3YHH736kijeLWd1xeXh42m43k5NrDxcnJyeTk5NT7nN27d/PNN98QERHBhx9+SF5eHpMmTaKgoOC080amT5/OY4895k40n6O+NOIJzsZ4ukUjXpWzCQ58C8EhMHCc2WmkGWrQBNZTV4cYhnHaFSN2u52goCDmz5/PoEGDuPzyy3nuued4/fXXTzs6Mm3aNAoLC2s+Dhw40JCYprHZbRwpdfwS0ciINFRhWTXf7lFjPGkCzuW8Pa6EWH2vSdNza2SkdevWWCyWOqMgubm5dUZLnFJTU2nTpg3x8fE1j/Xs2RPDMDh48CBdu3at85zw8HDCw8PdieZTjpYfxWpYCQkKITEy0ew44qe+3JaLTY3xxNsqiuCHdx3HmrgqJnFrZCQsLIyMjAyWLFlS6/ElS5YwbNiwep9z3nnncfjwYUpKSmoe2759O8HBwbRt27YBkX2f8xZNcnQylmAtxZSGWaJVNNIUflgI1aXQuht0GG52Gmmm3L5NM3XqVObMmcPcuXPZsmULU6ZMYf/+/UycOBFw3GIZN+6ne45jx44lISGBm2++maysLL7++mvuu+8+brnlFiIjIz33SnyIJq9KY1VabXy1zbHr6iW9UkxOIwHLMGDtibl7mbeANmgUk7g9ZXrMmDHk5+fz+OOPk52dTe/evVm0aBHt27cHIDs7m/3799ecHxMTw5IlS7jrrrvIzMwkISGBa665hj//+c+eexU+RpNXpbFW7cqntMpGUmw4fdvEn/0JIg2xfzXkZkFIJPS7zuw00ow1aP3WpEmTmDRpUr1/9vrrr9d5rEePHnVu7QSymt1XNTIiDeRcRTOylxrjiRc5l/P2uRoiW5gaRZo39abxAmdfGhUj0hAnN8bTfBHxmtI8x0ZnAJmauCrmUjHiBepLI42x6VDhT43xOqkxnnjJ+jfBVgVpA6HNQLPTSDOnYsTDDMNQXxppFOcqmgu6qTGeeInd9lNTPC3nFR+gYsTDiqqKKLOWAZASrVUQ4j4t6RWv2/kFHN8HEfFwzq/NTiOiYsTTnMt6W0W0IiIkwuQ04m/UGE+ahHPiav/rISzK3CwiqBjxOOfkVd2ikYZwjoqc26ElLaLUGE+84Ph+2P6Z4zjzFnOziJygYsTDNHlVGuOnWzS6xSdesu51wICOP4PWddtxiJhBxYiHafKqNNTxsiq+23uiMV5PzRcRL7BWwffzHMdazis+RMWIhzmLEY2MiLucjfG6J8fSLkH38cULtv4HSo9CTAr0uMLsNCI1VIx4mPrSSENpFY143ZoTfWgGjgNLqLlZRE6iYsTD1JdGGqLSamPZtqOAihHxktytsO8bCAqGjJvMTiNSi4oRD6qwVlBQ4bjnr5ERcYezMV5yXDh91BhPvMHZnbfbaIhva24WkVOoGPEg56hIVEgUcWFxJqcRf+K8RTOypxrjiRdUlsDGBY5j7bgqPkjFiAc5l/WmxaQRFKRfKOIau92o1aVXxON+fB8qi6BlR+h0kdlpROpQMeJB6tYrDbHpUCFHiiqJDrMwrLMa44mHGQasObHjauYtEKy3ffE9+q70IE1elYaoaYzXPZHwEDXGEw87tA5yfgBLOAy4wew0IvVSMeJBNbuvamRE3KAlveJVzlGRc34FUa3MzSJyGipGPEi3acRd+/PL2HbE0Rjvou5qjCceVlYAm//lONbEVfFhKkY86OQJrCKuWLJFjfHEiza8DdYKSO4Dbc81O43IaakY8RCb3caRMscvFo2MiKuWZOUAaownXmC3/7S3yLm3gFb4iQ9TMeIhR8uPYjNshASHkBiVaHYc8QPHy6pYs/cYAKM0X0Q8bc8yKNgFYbHQ5xqz04ickYoRD3H2pEmOSiY4SJdVzm7pVkdjvB4psaS3UmM88bDVsxyf+10L4THmZhE5C/3W9BDn5FXNFxFXaRWNeM3RbbDjMyAIBk80O43IWakY8ZCcUse9f80XEVdUVNtYtl2N8cRLVs90fO4+Glp3MTeLiAtUjHiI8zaNRkbEFat251N2ojFe7zQ1xhMPKs2Dje84jof+3twsIi5SMeIh2mNE3KHGeOI1a15zLOdNGwDth5mdRsQlKkY8RLuviqvsdoPPNV9EvKG6AtbMdhwP/b2W84rfUDHiAYZhqC+NuOyHQ4XkFlcSEx7CUDXGE0/6YSGUHoW4ttDrF2anEXGZihEPKKwspNxaDkBKtDavkjNzbnR2QTc1xhMPMgxY9ZLjeMhEsISam0fEDSpGPMA5XyQhIoFwS7jJacTXfZ6VC+gWjXjYzs8hb5tjk7OB48xOI+IWFSMeoJ404qqTG+Nd2F079YoHrXzB8XngOIjQCi3xLypGPEAracRVi0/cohnUoZUa44nn5GxybP8eZHHcohHxMypGPECTV8VV2nVVvMI5V6TXL6BFO3OziDSAihEP0LJeccWx0irW7C0AVIyIBxVlw6b3Hcfa5Ez8lIoRD9BtGnHF0q252A3UGE8867tXwV4N7YZC2wyz04g0iIoRD9AEVnHF51sct2hGaVREPKWqFNbOdRxrVET8mIqRRiq3lnOs8hgAqTEaGZH6ndwYb6SKEfGU9fOh4ji07Ohoiifip1SMNJJz8mpMaAxxYXEmpxFftWqXozFeSlwEfdpo2aV4gN32U3feoXdCsDbQE/+lYqSRnLdotPOqnMliZ2O8XkkEqV+IeMK2RXBsD0S0gP5jzU4j0igqRhrJOXlV80XkdOx2o2a+yCW9VLSKh6x80fE58xYIizY3i0gjqRhpJC3rlbPZePA4R080xhvSqZXZcSQQHFwLB1ZDcCgMus3sNCKNpmKkkbThmZyNc1Tkgu5qjCcesurEqEif30Kc/iEk/k/FSCMdLtEeI3JmNbuu9tQqGvGAY/sg69+O46GTzM0i4iEqRhrJOTKiYkTqsy+/lO1HSrAEB3FR9ySz40gg+PYVMOzQ6UJI6WN2GhGPUDHSCFa7ldwyRzt43aaR+jhHRQZ3bEV8VKjJacTvVRTC9/Mcx0PvMjeLiAepGGmEo2VHsRk2QoJDaB3Z2uw44oMWqzGeeNK6N6CqGBJ7QJeLzU4j4jEqRhrBuaw3JSqF4CBdSqmtoLSKtWqMJ55iq3bcogHHJmfar0YCiH6DNoJz8qpu0Uh9vjzRGK9nahxtW6oxnjRS1r+h6CBEJ0Kfa8xOI+JRKkYaQZNX5Ux+WkWjiavSSIYBK19wHJ97K4RGmJtHxMNUjDSC9hiR06motvH1DkdjPO26Ko22byVkb4CQCDh3vNlpRDxOxUgjaPdVOZ2Vu/Ioq7KRGh9B7zZqoCiN5NzkrN91EK3J8hJ4VIw0gnMCa2qMihGpzXmLZmTPZDXGk8bJ2wnb/uc4HqJNziQwqRhpIMMwakZG0qJ1m0Z+4miM59h/RqtopNFWzwQM6HYZJHYzO42IV6gYaaDjlcepsFUAkBKtOQHyk5Mb4w1WYzxpjLIC2PC243jo783NIuJFKkYayHmLJjEykTBLmMlpxJc4b9GoMZ402prXwFoOKX2hw/lmpxHxGhUjDaTJq3I6zmJklG7RSGNYK+G7Vx3Hw+7SJmcS0FSMNFBNt15NXpWT7M0rZUduCSHBQVyoxnjSGJveg9JciE2Dc35ldhoRr1Ix0kA1e4xo8qqcpKYxXqdWxEeqMZ40kGHAqpccx4NvB4u+lySwqRhpoJrdVzUyIif5addV3aKRRti1FHKzICwGMn5ndhoRr2tQMTJz5kw6duxIREQEGRkZLF++3KXnrVixgpCQEPr379+Qv9an1Nym0ZwROaGgtIq1+xyN8UZqvog0hnOTswE3QmQLU6OINAW3i5GFCxcyefJkHnroIdavX8/w4cMZPXo0+/fvP+PzCgsLGTduHBdfHBhtr9WXRk61VI3xxBOObHaMjAQFw5CJZqcRaRJuFyPPPfcc48ePZ8KECfTs2ZMZM2aQnp7OrFmzzvi822+/nbFjxzJ06NAGh/UVZdVlHK88DqgvjfxkSVYOoI3OpJFWzXR87nkVtOxgahSRpuJWMVJVVcW6desYNWpUrcdHjRrFypUrT/u8f/7zn+zatYtHHnnEpb+nsrKSoqKiWh++JKfU8UsnNjSW2LBYk9OIL6iotvH19jxAS3qlEYqPwKZ3HcdD7zI3i0gTcqsYycvLw2azkZxc+802OTmZnJycep+zY8cOHnjgAebPn09ISIhLf8/06dOJj4+v+UhPT3cnptc5NzxLidHOq+KwYmce5dU20uIjOCdNjfGkgdbMBlsVtB0E6eeanUakyTRoAuupjb8Mw6i3GZjNZmPs2LE89thjdOvmek+FadOmUVhYWPNx4MCBhsT0GufkVS3rFaeaxni91BhPGqiqzLHjKsAwbf0uzYtrQxUntG7dGovFUmcUJDc3t85oCUBxcTFr165l/fr1/P73jh8uu92OYRiEhISwePFiRowYUed54eHhhIeHuxOtSTlv02jyqkDtxngjtaRXGmrj21BeAC3aQ48rzU4j0qTcGhkJCwsjIyODJUuW1Hp8yZIlDBs2rM75cXFxbNq0iQ0bNtR8TJw4ke7du7NhwwYGDx7cuPQmcd6m0eRVAdhw8Dh5JZXEhocwpFOC2XHEH9ntP01cHTIJgtXTSJoXt0ZGAKZOncqNN95IZmYmQ4cO5dVXX2X//v1MnOhYgjZt2jQOHTrEvHnzCA4Opnfv3rWen5SURERERJ3H/Yn60sjJTm6MFxaifQSlAbZ/CgW7ICIeBtxgdhqRJud2MTJmzBjy8/N5/PHHyc7Opnfv3ixatIj27dsDkJ2dfdY9R/ydc2REu68KnLTrqlbRSEM5NznLuBnCY8zNImKCIMMwDLNDnE1RURHx8fEUFhYSF2fuSoVqezWZb2ViN+ws/e1SEqMSTc0j5tqTV8pFz3xFSHAQ6x6+RP1oxH2HvofZF0FwCNzzA8S3MTuRiMe4+vtbY8puOlp2FLthJzQ4lIRIzQ9o7pwbnQ3plKBCRBrG2RCv99UqRKTZUjHippN70gQH6fI1d59nOVfRJJmcRPzS8QOw+UPH8dA7zc0iYiL9NnWTetKIkxrjSaN99woYNugwHFL7mZ1GxDQqRtxUMzKiyavN3hdbjmA3oJca40lDVBTBujccx8O09bs0bypG3OQcGdHuq6JVNNIo69+EyiJo3Q26XGJ2GhFTqRhxU81tGo2MNGsV1TaW73A0xlMxIm6zWWH1y47jIZMgWG/F0rzpJ8BNJ09glebrmx1qjCeNsOVjKNwPUa2h37VmpxExnYoRNxiGUdOXRrdpmrfPt6gxnjSQYfy0ydm5EyA00tw8Ij5AxYgbjlUeo8JWQRBBpESnmB1HTHJyYzzdohG3HfgWDq0DS7ijGBERFSPucPakSYxMJNSiDa6aq/UHfmqMN7ijNr4TN618wfG53xiI0Q7OIqBixC3OnjQpMRoVac6cq2gu7JGkxnjinvxdsPW/juMh2uRMxEnvpG5wTl7VfJHmzbkFvG7RiNu+fRkwHEt5k3qYnUbEZ6gYcYNz8qqW9TZfu4+WsOtoKSHBQVzYXUPs4oayAlj/luN42O/NzSLiY1SMuEEjI+JcRTOkUwJxEZo3JG5Y90+oLoPkPtDxArPTiPgUFSNuUF8a0a6r0iDWKvj2Vcfx0DtBy8FFalEx4gbnBFbdpmme8ksqWbfvGKDGeOKmHz+AkhyITYXeV5udRsTnqBhxUVl1GYWVhYBu0zRXX2zNxW7AOWlxtGmhjarERSdvcjboNggJMzePiA9SMeIi5y2a2LBYYsJiTE4jZtAtGmmQPcvgyI8QGgUZvzM7jYhPUjHiIvWkad7Kq2ws33EUgJE9VYyIG1aeGBUZcANEtTI3i4iPUjHiIufIiG7RNE8rduZRUW2nTYtINcYT1+VuhZ1LgCAYcofZaUR8looRF9WMjGjyarPkvEUzsmeSGuOJ61a/5Pjc4wpo1cncLCI+TMWIizQy0nwVllfz6WbnrqtqBSAuKsmFjQsdx8PuMjeLiI9TMeKimj1GNDLS7Ly8bBeF5dV0SYphSCfd8xcXrZkDtkpokwnpg81OI+LTVIy4SBNYm6fDx8uZ+80eAB64rAchFv3IiAuqyx3FCGiTMxEX6J3VBdX2ao6WO1ZSpMXoNk1z8uzi7VRa7Qzu2IqLeyaZHUf8xcZ3oCwf4ttBz5+bnUbE56kYcUFuWS52w05YcBitIjRM31xkHS7iX+sPAjDt8p6auCqusdth1YmJq0PuAEuIuXlE/ICKERecvJImOEiXrLl46tOtGAZc2TeV/uktzI4j/mLnEsjfAeFxjr1FROSs9JvVBc7JqynRWknRXCzfcZSvtx8l1BLEfZd2NzuO+JOVLzg+Z9wEEdqTRsQVKkZc4BwZ0bLe5sFuN5i+aCsANwxpT/uEaJMTid848B3sXQ5BFhg80ew0In5DxYgLckode0xoWW/z8O+Nh8jKLiI2PIS7RnQ1O474C8OAxQ87jvtfB/Ftzc0j4kdUjLhAIyPNR0W1jWc+2w7AHRd1plW0OqyKi7b+Fw6shpBIuOghs9OI+BUVIy6o2fBMe4wEvDdW7uXQ8XJS4yO45byOZscRf2Grhs8fcRwPvRPi9A8XEXeoGDkLwzC0+2ozcay0ihe/3AnAvaO6ExFqMTmR+I3v50H+TohKgPPuMTuNiN9RMXIWBRUFVNoqCSKIlCitpglkL325k+IKKz1SYvnVgDZmxxF/UVkMX013HF/wgFbQiDSAipGzcI6KJEYlEmoJNTmNeMuBgjLmrdoHODY4swRrgzNx0coXofSooytvxu/MTiPil1SMnIV60jQPzyzeRpXNzvldWvOzrq3NjiP+ojgHVj7vOB75KIRowrNIQ6gYOQvnyIhW0gSuTQcL+fcGR9H5wOge2vZdXPfVdKgug7bnqgeNSCOoGDmLk7eCl8BjGAZPLtoCwK8GtKF3m3iTE4nfOLrNMXEV4JIn1JlXpBFUjJyFRkYC21fbjrJqdz5hIcHcO6qb2XHEn3z+KBh26HEltB9qdhoRv6Zi5Cy0rDdw2ewG0//nGBW5eVgH2raMMjmR+I29K2DbIse27xc/YnYaEb+nYuQsNIE1cH2w7iDbj5QQHxnKpAu7mB1H/IVhwJIT275n3ASJGlETaSwVI2dQWl1KUVURAGkxuk0TSMqrbDy7ZBsAd43oQnyUlm2Li7I+gkPrIDTasa+IiDSaipEzyC5x3KKJC4sjOlSdWwPJ3BV7OFJUSduWkdw4tL3ZccRfWKvg88ccx+fdDbHJ5uYRCRAqRs7gcOmJBnkaFQko+SWVzPpqFwD3Xdqd8BBt+y4uWjsXju2BmGQY+nuz04gEDBUjZ+AcGUmJ1jbwgeT5L3ZQUmmld5s4ruqrQlNcVFEIy552HF84DcJjzM0jEkBUjJxBzciIlvUGjD15pcz/dj8AD47uSbC2fRdXfTMDygugdTcYcKPZaUQCioqRM6jZY0S3aQLG3z7bitVucFH3RIZ10bbv4qLCQ7B6puN45GNgCTE3j0iAUTFyBs7bNFrWGxi+33+MRZtyCA6CB0b3NDuO+JMvnwRrBbQbBt1Hm51GJOCoGDkD520aFSP+zzAMpp/Y9v03GW3pnhJrciLxG0c2w4b5juNR2vZdxBtUjJxGta2ao2VHAe2+GgiWZB1hzd5jRIQGM+USbVIlbljyJ8CAXr+EtplmpxEJSCpGTuNI2REMDMIt4SREJJgdRxrBarPz1KdbARh/fkdS4yNNTiR+Y9eXsPNzCA6Fkdr2XcRbVIycRk1PmuhUtZT3c++sOcDuo6W0ig7j9gs6mx1H/IXdfmJUBDh3PLTqZG4ekQCmYuQ0nD1ptMeIfyuptDLj8+0A3D2iC3ER2vZdXPTj+5DzA4THwc/uMzuNSEBTMXIa2n01MMz+ejd5JVV0SIhi7GBt+y4uqq6AL55wHJ8/GaK1DFzEm1SMnEZOaQ6glTT+LLeogtnLdwNw/2U9CAvRt7u4aM1sKNwPsWkw+A6z04gEPL07n4bzNo1GRvzXjC92UFZlo396C0b31u02cVFZAXz9N8fxiIcgLMrcPCLNgIqR0zh5Aqv4n525xSxccwCABy/vqUnI4rrlzzr60CT1gn7XmZ1GpFlQMVIPu2HX7qt+7ulPt2GzG1zSK5lBHVuZHUf8xbF98N2rjuNLHodgdXQWaQoqRupRUFFAlb2K4KBgkqOTzY4jbvpuTwFLso5gCQ7ij5f1MDuO+JOlfwZbFXS8ALqMNDuNSLOhYqQezlGRxMhEQoO1FNSfGIbBX05s+z7m3HS6JKnNu7jo8AbY9K7j+JLHte27SBNqUDEyc+ZMOnbsSEREBBkZGSxfvvy05/7rX//ikksuITExkbi4OIYOHcpnn33W4MBNQct6/deiTTlsPHCcqDALk0d2NTuO+AvDgCUPO477XANp/U2NI9LcuF2MLFy4kMmTJ/PQQw+xfv16hg8fzujRo9m/f3+953/99ddccsklLFq0iHXr1nHRRRdx1VVXsX79+kaH9xbnyIg2PPMvVVY7f/3Mse37bT/rRFJshMmJxG/s/AL2fA2WMBjxf2anEWl23C5GnnvuOcaPH8+ECRPo2bMnM2bMID09nVmzZtV7/owZM7j//vs599xz6dq1K08++SRdu3blP//5T6PDe0vNyEi0Rkb8ydvf7mNffhmtY8K5dbi27hYX2W0/bfs+6DZoqc3xRJqaW8VIVVUV69atY9SoUbUeHzVqFCtXrnTpv2G32ykuLqZVq9OvcKisrKSoqKjWR1NyLuvVbRr/UVRRzfNLdwIw5ZKuRIeHmJxI/MbGBZC7GSLiYfi9ZqcRaZbcKkby8vKw2WwkJ9deYZKcnExOTo5L/41nn32W0tJSrrnmmtOeM336dOLj42s+0tPT3YnZaFrW639eWbaLgtIqOiVGMyazab9fxI9VlcHSvziOh/8BorQMXMQMDZrAeuoGUoZhuLSp1IIFC3j00UdZuHAhSUlJpz1v2rRpFBYW1nwcOHCgITEbzHmbRsWIf8guLGfO8j0APHBZD0IsWiQmLvp2FhQfhvh2jls0ImIKt8ayW7dujcViqTMKkpubW2e05FQLFy5k/PjxvPfee4wceeb1++Hh4YSHh7sTzWNKqkoorioGdJvGXzy3eDuVVjvndmjJJb20L4y4qDQPlv/dcXzxwxCqCc8iZnHrn5BhYWFkZGSwZMmSWo8vWbKEYcOGnfZ5CxYs4He/+x1vv/02V1xxRcOSNhHnfJH48HiiQtWTwtdtzSni/e8PAtr2Xdz09d+gqhhS+kLv35idRqRZc3uW39SpU7nxxhvJzMxk6NChvPrqq+zfv5+JEycCjlsshw4dYt68eYCjEBk3bhz/+Mc/GDJkSM2oSmRkJPHx8R58KZ5RM3lVK2n8wlP/24phwBV9UhnQrqXZccRf5O+CNXMcx6OegGDd2hMxk9vFyJgxY8jPz+fxxx8nOzub3r17s2jRItq3dyyHy87OrrXnyCuvvILVauXOO+/kzjvvrHn8pptu4vXXX2/8K/AwZ7de7THi+1bszOOrbUcJCQ7ivku7mx1H/MnSJ8BudWz53ulCs9OINHsNWv84adIkJk2aVO+fnVpgfPXVVw35K0yj3Vf9g91uMP1/jm3fbxjSng6to01OJH7j4FrY/CEQBCMfMzuNiKDeNHXklDhuI2kljW/7zw+H+fFQETHhIdw1oovZccRfGAYsPrHte/+xkNLb3DwiAqgYqUMjI76v0mrjr59uA+COCzuTEGPOyivxQ9v+B/tXQkgEXPSg2WlE5AQVI6fQhme+b97KfRw6Xk5yXDi3nNfR7DjiL2xW+PwRx/GQOyC+rbl5RKSGipGTVNuqOVp+FFAx4quOl1XxwtIdANx7SXciwywmJxK/sf5NyNsOka3g/ClmpxGRk6gYOUlOWQ4GBhGWCFpFaFtoXzTzq10UVVjpnhzL1Rn6l624qLIEvpruOL7gj44+NCLiM1SMnMR5iyYlOkWbZ/mgAwVlvL5iLwAPXN4DS7D+H4mLVr0EJUegZQfIvMXsNCJyChUjJ9HkVd/23JLtVNnsDOucwIXdEs2OI/6i+Ais+Ifj+OJHICTM3DwiUoeKkZNo8qrv+vFQIR+uPwTAtNHa9l3csOwpqC6FtIFwzq/MTiMi9VAxchJ16/VNhvHTBme/6J9Gn7a63y8uOrod1r3hOB71BKiIFfFJKkZOUtOXRrdpfMqy7UdZsTOfMEswfxilbd/FDV88BoYNuo2GDuebnUZETkPFyEl0m8b32OwGT/1vKwDjhrYnvZU6KYuL9q2CrZ9AUDBcom3fRXyZipET7Ia9ZmQkNUbFiK/41/cH2ZpTTFxECL/Xtu/iKsOAJSe2fR84DhI1oibiy1SMnJBfnk+1vZrgoGCSopLMjiNARbWNZxdvB+D3I7rQIkqrIMRFWz6Gg2sgNAounGZ2GhE5CxUjJzhHRZKikggNDjU5jQC8uHQnOUUVtGkRybihHcyOI/7CWgWfP+o4HnYXxKaYGkdEzk7FyAk1e4xEa/KqL/jfpmxe/HInAA+M7kFEqLZ9Fxetex0KdkN0oqMYERGfp2LkhJN3XxVzZR0uYuq7GwG4+bwOXNVPBaK4qKLIsa8IwIUPQHisuXlExCUqRk44XKLdV31BXkklt85bS3m1jfO7tOahy3uaHUn8yYp/QFk+JHSBgTeZnUZEXKRi5ISc0hxAy3rNVGW1M+mt7zl0vJwOCVG8OHYAIRZ9i4qLig47etAAjHwULJr7JeIv9E5/gvrSmMswDB75+Ee+21tAbHgIc27K1OoZcc+XT4K1HNKHQI8rzU4jIm5QMXKCNjwz17xV+1jw3QGCguD56wbQJUn3+sUNR7Jgw3zHsbZ9F/E7KkaA4qpiiquLARUjZlixM4/HP8kC4IHLenBRD+3zIm44shk+vA0MO/T8OaQPMjuRiLgpxOwAvsC5x0iL8BZEhWq78aa0L7+USfO/x2Y3+PWANtz2s05mRxJ/UVkMXz0Fq2c5+s+ExzvmioiI31Exgm7RmKW4oprxb6ylsLyafuktePLXfQjS8LqcjWFA1kfw6YNQ7JjrRc+fw2XTIb6tqdFEpGFUjKDJq2aw2Q3ueWcDO3NLSI4L59UbM7SxmZxd/i5Y9AfYtdTxdcsOcPkz0PUSU2OJSOOoGEEjI2Z4ZvE2lm7NJSwkmFdvzCQ5LsLsSOLLqsvhm787PmxVYAmH86fA+ZMhNNLsdCLSSCpG+GnOiIqRpvHvDYeY9dUuAP72m770S29hbiDxbTuWOEZDju11fN35Yrj8b5DQ2dRYIuI5KkbQbZqmtPHAce5//wcA7riwM7/o38bkROKzCg/Cpw/Alv84vo5Nc8wL6fULLd0VCTAqRjjpNk2MRka86UhRBbe9uZZKq52LeyTxh1HdzY4kvshW7dhJddnTUF0GQRYYOgku+KN6zYgEqGZfjFTZqjhafhTQbRpvqqi2cdub6zhSVEmXpBhmXNsfS7D+dSun2PsN/PdeOLrV8XW7oXDFs5B8jrm5RMSrmn0x4uxJE2GJoGV4S5PTBCbDMHjwX5vYeOA48ZGhzBmXSWyE+obISUpyYfHD8MM7jq+jWjt2Uu13nW7JiDQDzb4YqZm8GpOqPS68ZPby3fxr/SEswUHMvH4gHVpHmx1JfIXdBmvnwhdPQGUhEASZN8OIhyGqldnpRKSJNPti5HDJicmr0Zq86g1fbs1l+v8cQ+5/urIX53VpbXIi8RmH1sEnUyF7g+Pr1P5w5XPQJsPMVCJigmZfjDhHRlKiU0xOEnh25hZz94L1GAZcNyidcUPbmx1JfEH5McdIyNq5gOHYxv3ihyHzFgjWxncizVGzL0ZqRka0rNejCsuqmfDGWoorrZzboSWP/by3boM1d4YBGxc45oaU5Tke63utY25IjJojijRnzb4YcU5g1Uoaz7Ha7Px+wffszS+jTYtIZt2QQViIGkQ3a0eyHKtk9q90fJ3Yw7GNe8fh5uYSEZ/Q7IsRbXjmeU8u2sryHXlEhlqYPS6T1jHhZkcSs1SWwLKnYNVMR2fd0CjHfiFDJkFImNnpRMRHNOtixG7YNTLiYe+uOcDcFXsAeO6afvRKizM5kZjCMGDLx/C/B37qrNvjSrjsKWiRbm42EfE5zboYySvPo9pejSXIQlKU7lk31tq9BTz00SYAJo/syug+KvCapfxdsOg+2PWF4+sW7R29ZLpdam4uEfFZzboYca6kSYpKIiS4WV+KRjt0vJyJb62j2mYwuncKd4/oanYkaWrVFSd11q0ES9iJzrpT1FlXRM6oWf8GrulJo1s0jVJWZeXWN9aSV1JFz9Q4nr2mH8Ha6r152bHEMRpyzHGLjs4jHBNU1VlXRFzQrIsR5+RVNchrOMMwuO+9H8jKLiIhOozZ4zKICmvW31bNS+FB+HSaY34IQGzqic66v9Q27iLismb9W0O7rzbeC0t38t9N2YRagnj5xgzatowyO5J4U3EOHF5/4mMD7F3+U2fdIXfAhQ+os66IuK1ZFyM1K2k0MtIgn/6Yw3NLtgPwxC96c24H9RIJKKV5JxUeJz6Ks+uelz7E0Vk3pXfTZxSRgNCsi5HHhj3GwZKDGhlpgC3ZRUx9dwMAvxvWgWsHtTM3kDROWYGjR8zJox6FB+o5McixYVnaAMdHm4GQNhCCtamdiDRcsy5GEiITSIhMMDuG38kvqWTCG2spq7JxfpfW/N8VPc2OJO6oKITsjbVHPI7trf/chK4/FR5pAyClD4THNGlcEQl8zboYEfdVWe3cMf97Dh0vp31CFC+OHUCIRf8q9lmVJZDzQ+3CI39n/ee27Fi78EjtCxHxTZtXRJolFSPiMsMweOTjzXy3p4CY8BDmjMukRZS29PYZVWWQs+mnoiN7AxzdBhh1z41vB2n9Tyo8+kGU5vyIiDlUjIjL3lq9jwXf7ScoCJ6/rj9dk7VqwjTVFXBkMxz+3jG/4/B6OLrV0f/lVHFtThQd/SH1xOfo1k0cWETk9FSMiEtW7szj0f9kAfDHy3owokeyyYmaEWsV5GbVvtWSmwV2a91zo5NOTCp1jnj0h1j9vxIR36ZiRM5qX34pk97+Hpvd4Jf907j9Z53MjhS4bFbHCMfJhceRH8FWVffcqITaczzSBjg2HdNmYyLiZ1SMyBkVV1Qz4Y21HC+rpl/beJ66ui9B+mXnGXYb5O2oXXjkbAJred1zI+JPmVzaH1q0U+EhIgFBxYiclt1uMGXhBnbklpAUG84rN2YSEWoxO5Z/stuhYHftwiN7I1SX1j03PM4xodQ5zyNtgGOliwoPEQlQKkbktJ5dso3Pt+QSFhLMq+MySYmPMDuSfzAMx74dpxYelUV1zw2NPqnwOPHRqpM2ERORZkXFiNTr3xsO8dKXuwB4+uo+9E9vYW4gX2UYjmZxp26bXnG87rkhEZDSt3bh0borBGu0SUSaNxUjUscPB49z//s/AHD7BZ341YC2JifyIUXZdQuPsry651nCILl37cIjsQdY9CMnInIqvTMKNrvBrqMl/HCwkE0Hj/PfTdlUWu2M6JHE/Zf2MDueeUpyf+rT4iw8SnLqnhccAsnnOCaVOguPpF4Qog3hRERcoWKkmbHbDfbkl7LpYKGj+Dh0nM2Hiyirqr1ZVpekGGZc2x9LcDOZNFmaD9nraxcfRYfqnhdkgaSetXcvTToHQjWfRkSkoVSMBDDDMNhfUHai6Cjkh4PH+fFQESWVdTfLigqz0Dstnj5t4+nTJp6RvZKJCQ/Qb4/yY3UbxR3fX8+JQZDYvfZy2pQ+EBbV1IlFRAJagP62aX4Mw+DQ8XLHiMehwhMjH8cpqqhbeISHBHNOWhx927agT5t4+raNp1NiTGCOglQU/VR4ZG9wfC7YXf+5CV1O6VDbVx1qRUSaQIOKkZkzZ/K3v/2N7OxszjnnHGbMmMHw4cNPe/6yZcuYOnUqmzdvJi0tjfvvv5+JEyc2OHRzZxgGOUUVbKoZ8XB8Liitu0tnmCWYnqmx9GkbT982LejTNp6uSTGB2Wm3qhSyT+1Qu6P+c1t2OGUTsX7qUCsiYhK3i5GFCxcyefJkZs6cyXnnnccrr7zC6NGjycrKol27dnXO37NnD5dffjm33norb731FitWrGDSpEkkJiZy9dVXe+RFBLrc4oqT5ng4PueVVNY5LyQ4iB6psfRp04K+J263dEuOJSwkAAuP6nLI+bF24ZG3DQx73XPj00/pUNtfHWpFRHxIkGEY9fQXP73BgwczcOBAZs2aVfNYz549+eUvf8n06dPrnP/HP/6Rjz/+mC1bttQ8NnHiRDZu3MiqVatc+juLioqIj4+nsLCQuLg4d+L6nfySSjY5b7Oc+JxTVFHnPEtwEF2TYhxFR9sW9G0TT/eU2MDcIdVa6ejPUlN4bHQ0iquvQ21sat1t02MSmzyyiIi4/vvbrZGRqqoq1q1bxwMPPFDr8VGjRrFy5cp6n7Nq1SpGjRpV67FLL72U1157jerqakJDQ+s8p7KyksrKn/7lX1hYCDhelCd9N+8h4o+s9uh/szGqrAZVdse/7Huf+HCKDAkmMiyE6HALUaEhRIZZsAQFQQ6Oj7VQheMjoFSVQt52MOrpUBvVGlL6QWpfx0dK37odau2Ah79vRETENc7f22cb93CrGMnLy8Nms5GcXPsNPzk5mZycevZfAHJycuo932q1kpeXR2pqap3nTJ8+nccee6zO4+np6e7ElYBXDOwBPjI5h4iInElxcTHx8aefl9egCayndm01DOOMnVzrO7++x52mTZvG1KlTa7622+0UFBSQkJAQsB1ji4qKSE9P58CBAwF/K8pVuib103WpS9ekfroudema1OXNa2IYBsXFxaSlpZ3xPLeKkdatW2OxWOqMguTm5tYZ/XBKSUmp9/yQkBASEhLqfU54eDjh4eG1HmvRooU7Uf1WXFycfkBOoWtSP12XunRN6qfrUpeuSV3euiZnGhFxcmuZRVhYGBkZGSxZsqTW40uWLGHYsGH1Pmfo0KF1zl+8eDGZmZn1zhcRERGR5sXtNZ9Tp05lzpw5zJ07ly1btjBlyhT2799fs2/ItGnTGDduXM35EydOZN++fUydOpUtW7Ywd+5cXnvtNf7whz947lWIiIiI33J7zsiYMWPIz8/n8ccfJzs7m969e7No0SLat28PQHZ2Nvv3/7S1dseOHVm0aBFTpkzhpZdeIi0tjeeff157jJwiPDycRx55pM7tqeZM16R+ui516ZrUT9elLl2Tunzhmri9z4iIiIiIJwXg1pwiIiLiT1SMiIiIiKlUjIiIiIipVIyIiIiIqVSMeMnMmTPp2LEjERERZGRksHz58jOev2zZMjIyMoiIiKBTp068/PLLtf589uzZDB8+nJYtW9KyZUtGjhzJd999582X4BWevi4ne+eddwgKCuKXv/ylh1N7lzeuyfHjx7nzzjtJTU0lIiKCnj17smjRIm+9BK/wxnWZMWMG3bt3JzIykvT0dKZMmUJFRd1GlL7KnWuSnZ3N2LFj6d69O8HBwUyePLne8z744AN69epFeHg4vXr14sMPP/RSeu/w9DVpju+1rn6vOHnlvdYQj3vnnXeM0NBQY/bs2UZWVpZxzz33GNHR0ca+ffvqPX/37t1GVFSUcc899xhZWVnG7NmzjdDQUOP999+vOWfs2LHGSy+9ZKxfv97YsmWLcfPNNxvx8fHGwYMHm+plNZo3rovT3r17jTZt2hjDhw83fvGLX3j5lXiON65JZWWlkZmZaVx++eXGN998Y+zdu9dYvny5sWHDhqZ6WY3mjevy1ltvGeHh4cb8+fONPXv2GJ999pmRmppqTJ48ualeVqO4e0327Nlj3H333cYbb7xh9O/f37jnnnvqnLNy5UrDYrEYTz75pLFlyxbjySefNEJCQozVq1d7+dV4hjeuSXN8r3Xlujh5671WxYgXDBo0yJg4cWKtx3r06GE88MAD9Z5///33Gz169Kj12O23324MGTLktH+H1Wo1YmNjjTfeeKPxgZuIt66L1Wo1zjvvPGPOnDnGTTfd5FfFiDeuyaxZs4xOnToZVVVVng/cRLxxXe68805jxIgRtc6ZOnWqcf7553sotXe5e01OdsEFF9T7C+aaa64xLrvsslqPXXrppca1117bqKxNxRvX5FTN4b32ZGe6Lt58r9VtGg+rqqpi3bp1jBo1qtbjo0aNYuXKlfU+Z9WqVXXOv/TSS1m7di3V1dX1PqesrIzq6mpatWrlmeBe5s3r8vjjj5OYmMj48eM9H9yLvHVNPv74Y4YOHcqdd95JcnIyvXv35sknn8Rms3nnhXiYt67L+eefz7p162qG3Hfv3s2iRYu44oorvPAqPKsh18QVp7tujflvNhVvXZNTNYf3Wld58722QV175fTy8vKw2Wx1GgcmJyfXaRjolJOTU+/5VquVvLw8UlNT6zzngQceoE2bNowcOdJz4b3IW9dlxYoVvPbaa2zYsMFb0b3GW9dk9+7dLF26lOuvv55FixaxY8cO7rzzTqxWK3/605+89no8xVvX5dprr+Xo0aOcf/75GIaB1Wrljjvu4IEHHvDaa/GUhlwTV5zuujXmv9lUvHVNTtUc3mtd4e33WhUjXhIUFFTra8Mw6jx2tvPrexzgr3/9KwsWLOCrr74iIiLCA2mbjievS3FxMTfccAOzZ8+mdevWng/bRDz9vWK320lKSuLVV1/FYrGQkZHB4cOH+dvf/uYXxYiTp6/LV199xV/+8hdmzpzJ4MGD2blzJ/fccw+pqak8/PDDHk7vHe5eE7P+m03Jm/mb03vtmTTFe62KEQ9r3bo1FoulTgWam5tbp1J1SklJqff8kJAQEhISaj3+zDPP8OSTT/L555/Tt29fz4b3Im9cl82bN7N3716uuuqqmj+32+0AhISEsG3bNjp37uzhV+I53vpeSU1NJTQ0FIvFUnNOz549ycnJoaqqirCwMA+/Es/y1nV5+OGHufHGG5kwYQIAffr0obS0lNtuu42HHnqI4GDfvWvdkGviitNdt8b8N5uKt66JU3N6rz2bXbt2ef291nd/+vxUWFgYGRkZLFmypNbjS5YsYdiwYfU+Z+jQoXXOX7x4MZmZmYSGhtY89re//Y0nnniCTz/9lMzMTM+H9yJvXJcePXqwadMmNmzYUPPx85//nIsuuogNGzaQnp7utdfjCd76XjnvvPPYuXNnzZsFwPbt20lNTfX5QgS8d13KysrqFBwWiwXDMZHfg6/A8xpyTVxxuuvWmP9mU/HWNYHm9157Nk3yXuuxqbBSw7ms6rXXXjOysrKMyZMnG9HR0cbevXsNwzCMBx54wLjxxhtrzncuS5wyZYqRlZVlvPbaa3WWJT799NNGWFiY8f777xvZ2dk1H8XFxU3++hrKG9flVP62msYb12T//v1GTEyM8fvf/97Ytm2b8cknnxhJSUnGn//85yZ/fQ3ljevyyCOPGLGxscaCBQuM3bt3G4sXLzY6d+5sXHPNNU3++hrC3WtiGIaxfv16Y/369UZGRoYxduxYY/369cbmzZtr/nzFihWGxWIxnnrqKWPLli3GU0895ZdLez15TZrje61hnP26nMrT77UqRrzkpZdeMtq3b2+EhYUZAwcONJYtW1bzZzfddJNxwQUX1Dr/q6++MgYMGGCEhYUZHTp0MGbNmlXrz9u3b28AdT4eeeSRJng1nuPp63IqfytGDMM712TlypXG4MGDjfDwcKNTp07GX/7yF8NqtXr7pXiUp69LdXW18eijjxqdO3c2IiIijPT0dGPSpEnGsWPHmuDVeIa716S+94z27dvXOue9994zunfvboSGhho9evQwPvjggyZ4JZ7j6WvSXN9rXfleOZmn32uDToQQERERMYXmjIiIiIipVIyIiIiIqVSMiIiIiKlUjIiIiIipVIyIiIiIqVSMiIiIiKlUjIiIiIipVIyIiIiIqVSMiIiIiKlUjIiIiIipVIyIiIiIqVSMiIiIiKn+HxFoc3OeYNXDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "values, bins = np.histogram(rnn_jsds, bins=np.arange(0, np.max(rnn_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(he_jsds, bins=np.arange(0, np.max(he_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "values, bins = np.histogram(min_jsds, bins=np.arange(0, np.max(min_jsds) + 0.01, 0.01))\n",
    "cdf = np.cumsum(values) / np.sum(values)\n",
    "plt.plot(bins[1:], cdf)\n",
    "\n",
    "plt.ylim(0, 1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [50, 90, 95, 99]:\n",
    "    print(i, np.percentile(rnn_jsds, i), np.percentile(he_jsds, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(gru, 'models/gru-0504.pth')\n",
    "# torch.save(s2h, 'models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizeDecoder(torch.nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dims, latent_dim):\n",
    "        super(SizeDecoder, self).__init__()\n",
    "        self.decoder = torch.nn.ModuleList()\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            self.decoder.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, out_features=h_dim,),\n",
    "                    nn.ReLU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "        self.output = nn.Linear(hidden_dims[-1], output_dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> List[Tensor]:\n",
    "        for module in self.decoder:\n",
    "            x = module(x)\n",
    "        result = self.output(x)\n",
    "        result = F.softmax(result, dim=1)\n",
    "        return result\n",
    "    \n",
    "decoder = torch.load('models/size-decoder-0425.pth')\n",
    "gru = torch.load('models/gru-0504.pth')\n",
    "s2h = torch.load('models/s2h-0504.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "def JSD(p, q):\n",
    "    p = list(p)\n",
    "    q = list(q)\n",
    "    pq_max_len = max(len(p), len(q))\n",
    "    p += [0.0] * (pq_max_len - len(p))\n",
    "    q += [0.0] * (pq_max_len - len(q))\n",
    "    assert (len(p) == len(q))\n",
    "    m = np.sum([p, q], axis=0) / 2\n",
    "    return 0.5 * entropy(p, m) + 0.5 * entropy(q, m)\n",
    "\n",
    "def sample_noisy_dataset(n, seed):\n",
    "    torch.manual_seed(seed)\n",
    "    dataset = []\n",
    "    for i in tqdm(range(n)):\n",
    "        latent_dim = 32\n",
    "        z = torch.randn((1, latent_dim)).to(device)\n",
    "        size = decoder(z)\n",
    "        size = size.squeeze().detach().to('cpu').numpy()\n",
    "        size[size < 1e-3] = 0\n",
    "        size /= size.sum()\n",
    "\n",
    "        dis = []\n",
    "        for j in range(1000):\n",
    "            loss = JSD(size, sizedata[j])\n",
    "            dis.append(loss)\n",
    "\n",
    "        pair = np.argmin(dis)\n",
    "        ran_index = np.random.randint(len(seq_set[pair]))\n",
    "        dataset.append([seq_set[pair][ran_index], size, target_set[pair][ran_index]])\n",
    "        \n",
    "    return dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
